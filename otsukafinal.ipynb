{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8931048,"sourceType":"datasetVersion","datasetId":5372674},{"sourceId":8932176,"sourceType":"datasetVersion","datasetId":5373505},{"sourceId":8932978,"sourceType":"datasetVersion","datasetId":5374080},{"sourceId":8933076,"sourceType":"datasetVersion","datasetId":5374160}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/Arun-Raghav-S/Advanced_RAG.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd Advanced_RAG","metadata":{"execution":{"iopub.status.busy":"2024-07-12T00:47:42.183096Z","iopub.execute_input":"2024-07-12T00:47:42.183362Z","iopub.status.idle":"2024-07-12T00:47:42.189274Z","shell.execute_reply.started":"2024-07-12T00:47:42.183337Z","shell.execute_reply":"2024-07-12T00:47:42.188080Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/working/Advanced_RAG\n","output_type":"stream"}]},{"cell_type":"code","source":"%pip install -r requirements.txt ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport nest_asyncio\nfrom dotenv import load_dotenv\nload_dotenv()\n\n\nnest_asyncio.apply()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T00:52:51.748150Z","iopub.execute_input":"2024-07-12T00:52:51.748452Z","iopub.status.idle":"2024-07-12T00:52:51.775800Z","shell.execute_reply.started":"2024-07-12T00:52:51.748424Z","shell.execute_reply":"2024-07-12T00:52:51.775143Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\na = user_secrets.get_secret(\"AZURE_OPENAI_API_KEY\")\nb = user_secrets.get_secret(\"AZURE_OPENAI_ENDPOINT\")\nc = user_secrets.get_secret(\"OPENAI_API_VERSION\")\nd = user_secrets.get_secret(\"PAT_KEY\")","metadata":{"execution":{"iopub.status.busy":"2024-07-12T00:52:51.776815Z","iopub.execute_input":"2024-07-12T00:52:51.777122Z","iopub.status.idle":"2024-07-12T00:52:54.402528Z","shell.execute_reply.started":"2024-07-12T00:52:51.777097Z","shell.execute_reply":"2024-07-12T00:52:54.401768Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from llama_index.llms.azure_openai import AzureOpenAI\nfrom llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\nfrom llama_index.core import SimpleDirectoryReader\n# Initialize an embedding model from Hugging Face using the \"BAAI/bge-small-en\" model.\nembedding_model = AzureOpenAIEmbedding(\n    api_key=a,\n    model=\"text-embedding-3-large\",\n    deployment_name=\"text-embedding3\",\n     azure_endpoint=b,\n    api_version=c\n    \n)\nllm = AzureOpenAI(\n    model=\"gpt-35-turbo-16k\",\n    deployment_name=\"GPT35-turboA\",\n    api_key=a,\n    azure_endpoint=b,\n    api_version=c,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-12T00:52:54.403526Z","iopub.execute_input":"2024-07-12T00:52:54.403777Z","iopub.status.idle":"2024-07-12T00:52:58.606087Z","shell.execute_reply.started":"2024-07-12T00:52:54.403755Z","shell.execute_reply":"2024-07-12T00:52:58.605107Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"source_docs=SimpleDirectoryReader('test_data').load_data()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T00:52:58.607211Z","iopub.execute_input":"2024-07-12T00:52:58.607682Z","iopub.status.idle":"2024-07-12T00:53:19.323122Z","shell.execute_reply.started":"2024-07-12T00:52:58.607654Z","shell.execute_reply":"2024-07-12T00:53:19.321952Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Extracting tables,text and images","metadata":{}},{"cell_type":"code","source":"!mkdir images","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tempfile\ndef clear_temp_files():\n    temp_dir = tempfile.gettempdir()\n    for file in os.listdir(temp_dir):\n        file_path = os.path.join(temp_dir, file)\n        try:\n            if os.path.isfile(file_path):\n                os.unlink(file_path)\n        except Exception as e:\n            print(f\"Error clearing temp files: {e}\")\n\nclear_temp_files()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!apt install -y poppler-utils","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from unstructured.partition.pdf import partition_pdf\nfrom unstructured.staging.base import elements_to_json\ndef process_pdfs_in_folder(folder_path, output_dir):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    for filename in os.listdir(folder_path):\n        if filename.endswith('.pdf'):\n            pdf_path = os.path.join(folder_path, filename)\n            print(f\"Processing {pdf_path}...\")\n\n            pdf_name = os.path.splitext(filename)[0]\n            pdf_output_dir = os.path.join(output_dir, pdf_name)\n\n            # Create subdirectories for the PDF's images and JSON\n            images_dir = os.path.join(pdf_output_dir, \"images\")\n            json_dir = os.path.join(pdf_output_dir, \"json\")\n\n            if not os.path.exists(images_dir):\n                os.makedirs(images_dir)\n            if not os.path.exists(json_dir):\n                os.makedirs(json_dir)\n\n            raw_pdf_elements = partition_pdf(\n                filename=pdf_path,\n                extract_images_in_pdf=True,\n                infer_table_structure=True,\n                chunking_strategy=\"by_title\",\n                max_characters=4000,\n                new_after_n_chars=3800,\n                combine_text_under_n_chars=2000,\n                extract_image_block_output_dir=images_dir,\n                extract_image_block_to_payload=False,\n                strategy=\"hi_res\"\n            )\n\n            json_filename = f\"{pdf_name}.json\"\n            json_path = os.path.join(json_dir, json_filename)\n\n            elements_to_json(raw_pdf_elements, filename=json_path)\n            print(f\"Saved data to {json_path}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"  source_folder = \"test_data\"  # Path to your source_docs folder\n  output_folder = \"extracted_jsons\"  # Path to the folder where JSON files will be saved\n\n  process_pdfs_in_folder(source_folder, output_folder)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nimport os\n\ndef create_zip_from_folder(folder_path, zip_filename):\n    # Create a zip file from the folder\n    shutil.make_archive(zip_filename, 'zip', folder_path)\n    print(f\"Created zip file: {zip_filename}.zip\")\n\nextracted_folder = \"chroma_db\"  # Path to the folder to be zipped\nzip_filename = \"chroma_db_archive\"  # Name of the resulting zip file (without extension)\n\ncreate_zip_from_folder(extracted_folder, zip_filename)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:28:54.485125Z","iopub.execute_input":"2024-07-11T18:28:54.485759Z","iopub.status.idle":"2024-07-11T18:28:55.554919Z","shell.execute_reply.started":"2024-07-11T18:28:54.485723Z","shell.execute_reply":"2024-07-11T18:28:55.553686Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Created zip file: chroma_db_archive.zip\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## Tables and Text","metadata":{}},{"cell_type":"markdown","source":"### Preprocessing Helpers","metadata":{}},{"cell_type":"markdown","source":"**Display Table and Text**","metadata":{}},{"cell_type":"code","source":"from io import StringIO\nimport pandas as pd\nimport json\nimport re\nfrom bs4 import BeautifulSoup\ndef clean_html_content(html_content):\n    \"\"\"Extract only the well-formed HTML table parts from the provided HTML content.\"\"\"\n    # This regex attempts to isolate <table>...</table> blocks\n    if '<table' not in html_content:\n    # Find all pieces of table rows or cells\n        pieces = re.findall(r'<tr.*?>.*?</tr>', html_content, flags=re.DOTALL)\n        if pieces:\n            # Reconstruct HTML with <table> tags properly placed\n            start = html_content.find(pieces[0])\n            end = html_content.rfind(pieces[-1]) + len(pieces[-1])\n            html_content = (html_content[:start] +\n                            '<table>' +\n                            html_content[start:end] +\n                            '</table>' +\n                            html_content[end:])\n    return ''.join(re.findall(r'<table.*?>.*?</table>', html_content, flags=re.DOTALL))\n\ndef preprocess_json_file(input_filepath):\n    with open(input_filepath, 'r') as file:\n        data = json.load(file)\n\n    preprocessed_elements = []\n    for entry in data:\n        if entry['type'] == 'CompositeElement':\n            preprocessed_elements.append({\n                'type': 'text',\n                'content': entry['text']\n            })\n        if entry['type'] == 'Table':\n            html_content = entry['metadata']['text_as_html']\n            html_content=str(BeautifulSoup(html_content, 'html.parser'))\n            cleaned_html=clean_html_content(html_content)\n            soup = BeautifulSoup(cleaned_html, 'html.parser')\n            table_html = str(soup)\n            df = pd.read_html(StringIO(table_html))[0]\n            # Check if the DataFrame has unique columns, if not, assign unique names\n            if isinstance(df.columns, pd.MultiIndex):\n                df.columns = ['_'.join(map(str, col)).strip() for col in df.columns.values]\n            elif df.columns.astype(str)[0].isdigit(): \n                df.columns = [f'Column_{i+1}' for i in range(len(df.columns))]\n            table_json = json.loads(df.to_json(orient='records'))\n            preprocessed_elements.append({\n                'type': 'table',\n                'content': table_json\n            })\n\n    return preprocessed_elements\ndef preprocess_all_json_files(base_folder_path):\n    preprocessed_data = {}\n    for pdf_name in os.listdir(base_folder_path):\n        pdf_folder_path = os.path.join(base_folder_path, pdf_name, 'json')\n        if os.path.isdir(pdf_folder_path):\n            for file in os.listdir(pdf_folder_path):\n                if file.endswith('.json'):\n                    input_filepath = os.path.join(pdf_folder_path, file)\n                    print(f\"Processing {input_filepath}...\")\n                    preprocessed_data[pdf_name] = preprocess_json_file(input_filepath)\n    return preprocessed_data\n\ndef combine_elements(preprocessed_data):\n    combined_data = {}\n    for pdf_name, elements in preprocessed_data.items():\n        combined_text = \"\"\n        for element in elements:\n            if element['type'] == 'text':\n                combined_text += element['content'] + \"\\n\\n\"\n            elif element['type'] == 'table':\n                combined_text += \"Table:\\n\" + element['content'] + \"\\n\\n\"\n        combined_data[pdf_name] = combined_text\n    return combined_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_folder = \"extracted_jsons\"  # Path to your base folder containing PDF directories\npreprocessed_data = preprocess_all_json_files(base_folder)\nwith open('processed_tableandtext.json', 'w', encoding='utf-8') as outfile:\n    json.dump(preprocessed_data, outfile, indent=4, ensure_ascii=False)\n# combined_data = combine_elements(preprocessed_data)\n\n# # Optionally, save combined data to a file for later use\n# with open('combined_data.json', 'w') as outfile:\n#     json.dump(combined_data, outfile, indent=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Table Text summaries","metadata":{}},{"cell_type":"code","source":"import json\nimport openai\n\ndef format_table(table_data):\n    # Create a string representation of the table\n    headers = table_data[0].keys()\n    header_line = \" | \".join(headers)\n    lines = [header_line, \"-\"*len(header_line)]\n    for item in table_data:\n        row = \" | \".join(str(item[h]) for h in headers)\n        lines.append(row)\n    return \"\\n\".join(lines)\n\ndef summarize_content(content, content_type):\n    # Define the prompt based on the content type\n    if content_type == \"text\":\n        prompt = \"Summarize the following text: \\n\\n\" + content\n    elif content_type == \"table\":\n        table_string = format_table(content)\n        prompt = \"Summarize the key information from this table: \\n\\n\" + table_string\n    else:\n        return \"No summary available.\"\n\n    # Call the OpenAI API to generate the summary\n    response = llm.complete(\n    prompt\n    )\n    return response.text\n\ndef generate_summaries(file_path):\n    # Load the JSON data\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n\n    summary_data = {}\n    # Iterate through each document\n    for pdf_name, contents in data.items():\n        print(\"Processing pdf :\",pdf_name)\n        print()\n        summary_data[pdf_name] = {}\n        # Summarize each content type\n        for content_dict in contents:  # Adjusted to iterate over the list\n            content_type = content_dict['type']\n            content = content_dict['content']\n            summary = summarize_content(content, content_type)\n            summary_data[pdf_name][content_type + \"_summary\"] = summary\n\n    return summary_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Usage\nsummaries = generate_summaries('processed_tableandtext.json')\nprint(summaries)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Summarising with  langchain","metadata":{}},{"cell_type":"code","source":"from langchain_openai import AzureChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema.output_parser import StrOutputParser\nmodel = AzureChatOpenAI(\n        deployment_name=\"GPT35-turboA\",\n        api_version=\"2024-02-01\",\n        temperature=0\n      )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.prompts import ChatPromptTemplate\nfrom langchain.schema.output_parser import StrOutputParser\n\nprompt_text = \"\"\"\n  You are responsible for concisely summarizing table or text chunk:\n  Keep the summary short and crisp and extract key features\n\n  {element}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(prompt_text)\nsummarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\ndef process_pdf_contents(pdf_name, contents, summarize_chain):\n    summary_results = {}\n    print(\"Processing pdf:\", pdf_name)\n\n    try:\n        tables = [c['content'] for c in contents if c['type'] == 'table']\n        texts = [c['content'] for c in contents if c['type'] == 'text']\n\n        # Process tables\n        if tables:\n            table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})\n            summary_results['table_summaries'] = table_summaries\n            print(\"Table Summaries:\")\n            print(table_summaries)\n\n        # Process texts\n        if texts:\n            text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n            summary_results['text_summaries'] = text_summaries\n            print('Text Summaries:')\n            print(text_summaries)\n\n    except Exception as e:\n        print(f\"Error processing {pdf_name}: {e}\")\n        time.sleep(10)  # Sleep to respect rate limits or handle transient issues\n\n    return summary_results\n\ndef load_and_summarize(file_path):\n    # Load existing data if available\n    if os.path.exists('intermediate_summary_results.json'):\n        with open('intermediate_summary_results.json', 'r') as infile:\n            all_results = json.load(infile)\n    else:\n        all_results = {}\n\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n\n    for pdf_name, contents in data.items():\n        if pdf_name not in all_results:\n            result = process_pdf_contents(pdf_name, contents, summarize_chain)\n            all_results[pdf_name] = result\n            # Save intermediate results to avoid losing progress\n            with open('intermediate_summary_results.json', 'w') as outfile:\n                json.dump(all_results, outfile, indent=4)\n\n    return all_results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res=load_and_summarize('processed_tableandtext.json')\nprint(res)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('summarized_data.json', 'w') as file:\n        json.dump(res, file, indent=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Processing Images","metadata":{}},{"cell_type":"markdown","source":"### Preprocessing Helpers","metadata":{}},{"cell_type":"markdown","source":"**Verifying images**","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport os\ndef verify_images(directory):\n    for filename in os.listdir(directory):\n        path = os.path.join(directory, filename)\n        try:\n            with Image.open(path) as img:\n                print(f\"{filename} is valid.\")\n        except IOError:\n            print(f\"Error opening {filename}; it may be corrupted or in an incorrect format.\")\n\n# Verify images before processing\nverify_images('images')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Display images**","metadata":{}},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport pandas as pd\n\n# Directory containing the extracted images\nimages_path = \"images\"\n\n# Function to display images\ndef plot_images(images_folder):\n    # Check if the directory exists\n    if not os.path.exists(images_folder):\n        print(f\"The directory {images_folder} does not exist.\")\n        return\n\n    # List all image files\n    image_files = [f for f in os.listdir(images_folder) if f.endswith(('.png', '.jpg', '.jpeg', '.ppm'))]\n    total_files = len(image_files)\n\n    if total_files == 0:\n        print(\"No images found in the directory.\")\n        return\n\n    print(f\"Found {total_files} images.\")\n    \n    # Plot each image\n    fig, axs = plt.subplots(1, total_files, figsize=(15, 5))\n    for ax, image_file in zip(axs, image_files):\n        image_path = os.path.join(images_folder, image_file)\n        image = Image.open(image_path)\n        ax.imshow(image)\n        ax.axis('off')\n        ax.set_title(image_file)\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Clear files**","metadata":{}},{"cell_type":"code","source":"def clear_file_if_not_empty(file_path):\n    try:\n        # Check if the file exists\n        with open(file_path, 'r+') as file:\n            contents = file.read()\n            # Check if the file is not empty\n            if contents:\n                # Move the cursor to the beginning of the file\n                file.seek(0)\n                # Clear the file\n                file.truncate()\n                print(\"File was not empty and has been cleared.\")\n            else:\n                print(\"File is already empty.\")\n    except FileNotFoundError:\n        print(f\"No file found at {file_path}.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n# Specify the path to your file\nfile_path = 'captions_output.txt'\n\n# Call the function to check and clear the file\nclear_file_if_not_empty(file_path)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Processing with MILVLG","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom PIL import Image\ntorch.set_default_device(\"cuda\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntorch.cuda.empty_cache()\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"MILVLG/imp-v1-3b\",\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(\"MILVLG/imp-v1-3b\", trust_remote_code=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_and_caption_image(image_path):\n    try:\n        # Ensure image is loaded correctly\n        image = Image.open(image_path).convert('RGB')\n        text = (\n            \"Please analyze the image thoroughly. The image may contain various forms of data representation such as charts, graphs, tables, etc. \"\n            \"For charts and graphs, identify and describe the type, axes, labels, data points, trends, and any significant peaks, troughs, or patterns. \"\n            \"Highlight any anomalies or outliers and discuss their possible implications. Extract and report all key numerical values. \"\n            \"For other types of images, describe all visible elements and their relationships in detail. Provide a clear and precise summary of the key features,\"\n            \"interpret the data where applicable, and make note of any notable observations or ambiguities.\" \n            \"It is crucial to extract all key numerical values if present in the image.\"\n\n        )\n        input_ids = tokenizer(text, return_tensors='pt').input_ids\n        image_tensor = model.image_preprocess(image)\n\n        \n\n        # Generate the answer\n        with torch.no_grad():\n            output_ids = model.generate(\n                input_ids,\n                max_new_tokens=100,\n                images=image_tensor,\n                use_cache=True\n            )[0]\n        \n        return [tokenizer.decode(output_ids[input_ids.shape[1]:], skip_special_tokens=True).strip()]\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {e}\")\n        return []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport json\ndef process_images_in_folder(base_folder_path):\n    structured_data = {}\n\n    for pdf_name in os.listdir(base_folder_path):\n        pdf_folder_path = os.path.join(base_folder_path, pdf_name, 'images')\n        if os.path.isdir(pdf_folder_path):\n            image_files = [f for f in os.listdir(pdf_folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n            image_captions = {}\n            print(f\"Processing pdf : {pdf_name} \\n\")\n            for filename in image_files:\n                image_path = os.path.join(pdf_folder_path, filename)\n                \n                captions = process_and_caption_image(image_path)\n                image_captions[filename] = captions\n\n            structured_data[pdf_name] = image_captions\n\n    return structured_data\n\nbase_folder = \"extracted_jsons\"  # Path to your base folder containing PDF directories\nquestion = \"Describe the image in detail and extract key features from it\"\nstructured_image_data = process_images_in_folder(base_folder)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Optionally, save structured image data to a JSON file for later use\nwith open('structured_image_data.json', 'w') as outfile:\n    json.dump(structured_image_data, outfile, indent=4)\n\nprint(f\"Image captions have been saved to structured_image_data.json\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"structured_image_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Retriever Langchain","metadata":{}},{"cell_type":"code","source":"import uuid\nimport json\n\nfrom langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\nfrom langchain.retrievers.multi_vector import MultiVectorRetriever\nfrom langchain.schema.document import Document\nfrom langchain.storage import InMemoryStore\nfrom langchain.vectorstores import Chroma\n\nembeddings_model = AzureOpenAIEmbeddings(\n        azure_deployment=\"text-embedding3\",\n        api_version=\"2024-02-01\"\n    )\n# Load the JSON file containing summaries\ndef load_data(file_path):\n    with open(file_path, 'r') as file:\n        return json.load(file)\n\n# Initialize the retriever\nid_key = \"doc_id\"\nretriever = MultiVectorRetriever(\n    vectorstore=Chroma(collection_name=\"summaries\", embedding_function=embeddings_model),\n    docstore=InMemoryStore(),\n    id_key=id_key,\n)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"retriever.vectorstore.clear()\nretriever.docstore.clear()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef store_summaries(original_data, summary_data):\n    for pdf_name, contents in original_data.items():\n        print(f\"Processing PDF: {pdf_name}\")\n        \n        # Extract original text and table contents\n        text_contents = [c['content'] for c in contents if c['type'] == 'text']\n        table_contents = [c['content'] for c in contents if c['type'] == 'table']\n#         print(text_contents[1])\n          \n\n        # Store and print text summaries along with their corresponding original texts\n        if 'text_summaries' in summary_data[pdf_name]:\n            text_ids = [str(uuid.uuid4()) for _ in summary_data[pdf_name]['text_summaries']]\n            print(\"text Ids:\",text_ids)\n            for i, summary in enumerate(summary_data[pdf_name]['text_summaries']):\n                \n                summary_texts = Document(page_content=summary, metadata={id_key: text_ids[i]})\n                retriever.vectorstore.add_documents([summary_texts])\n                retriever.docstore.mset([(text_ids[i], text_contents[i])])\n        \n        # Store and print table summaries along with their corresponding original tables\n        if 'table_summaries' in summary_data[pdf_name]:\n            table_ids = [str(uuid.uuid4()) for _ in summary_data[pdf_name]['table_summaries']]\n            for i, summary in enumerate(summary_data[pdf_name]['table_summaries']):                \n                summary_tables = Document(page_content=summary, metadata={id_key: table_ids[i]})\n                retriever.vectorstore.add_documents([summary_tables])\n                retriever.docstore.mset([(table_ids[i], table_contents[i])])\n\n# Proceed with data loading and summarizing\noriginal_data = load_data('processed_tableandtext.json')\nsummary_data = load_data('summarized_data.json')\nstore_summaries(original_data, summary_data)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport os\n\nimport base64\n\ndef image_to_base64(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode('utf-8')\n\n\ndef load_and_process_images(pdf_name, image_data, base_dir):\n    \"\"\"Load image files, convert them to base64, and collect summaries.\"\"\"\n    image_paths = [os.path.join(base_dir, pdf_name, 'images', image_name) for image_name in image_data.keys()]\n    image_summaries = [summary[0] for summary in image_data.values()]\n    image_contents = [image_to_base64(path) for path in image_paths]\n\n    # Print each image path, a snippet of its base64 content, and its summary for verification\n    for path, content, summary in zip(image_paths, image_contents, image_summaries):\n        print(f\"Image Path: {path}\")\n        print(f\"Base64 Snippet: {content[:60]}...\")  # Print only the first 60 characters of the base64 string\n        print(f\"Summary: {summary}\")\n        print(\"\\n-------------------\\n\")\n\n    return image_paths, image_summaries, image_contents\n\ndef store_image_data(image_paths, image_summaries, image_contents):\n    doc_ids = [str(uuid.uuid4()) for _ in image_paths]\n    summary_images = [\n        Document(page_content=summary, metadata={id_key: doc_id})\n        for doc_id, summary in zip(doc_ids, image_summaries)\n    ]\n    # Add summaries to the vector store\n    retriever.vectorstore.add_documents(summary_images)\n    # Store original images in the document store\n    retriever.docstore.mset(list(zip(doc_ids, image_contents)))\n\n# Example usage\nbase_dir = 'extracted_jsons'\nstructured_image_file = 'structured_image_data.json'\nimage_data_full = json.load(open(structured_image_file))\n\n# Process and store each PDF's images\nfor pdf_name, images in image_data_full.items():\n    image_paths, image_summaries, image_contents = load_and_process_images(pdf_name, images, base_dir)\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import base64\nfrom PIL import Image\nfrom IPython.display import HTML, display\nimport io\nfrom langchain.schema import Document, HumanMessage\n\ndef plt_img_base64(img_base64):\n    \"\"\"Display an image from a base64 encoded string.\"\"\"\n    display(HTML(f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'))\n\ndef is_image_data(b64data):\n    \"\"\"Check if the base64 data is an image by looking at the start of the data.\"\"\"\n    image_signatures = {\n        b\"\\xFF\\xD8\\xFF\": \"jpg\",\n        b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\": \"png\",\n        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n    }\n    try:\n        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n        for sig, format in image_signatures.items():\n            if header.startswith(sig):\n                return True\n        return False\n    except Exception:\n        return False\n\ndef split_image_text_types(docs):\n    \"\"\"Split base64-encoded images and texts.\"\"\"\n    b64_images = []\n    texts = []\n    for doc in docs:\n        # Check if the document is of type Document and extract page_content if so\n        if isinstance(doc, Document):\n            doc = doc.page_content\n\n        if is_image_data(doc):\n            b64_images.append(doc)\n        else:\n            texts.append(doc)\n    return {\"images\": b64_images, \"texts\": texts}\n\ndef img_prompt_func(data_dict):\n    \"\"\"Generate a text prompt incorporating image descriptions or placeholders.\"\"\"\n    messages = []\n\n    # Process images: Since GPT-3 cannot directly handle images, use descriptions or placeholders\n    if \"images\" in data_dict[\"context\"]:\n        for image in data_dict[\"context\"][\"images\"]:\n            description = f\"[Image: A detailed description or a URL pointing to the image data.]\"  # Placeholder text\n            messages.append(description)\n\n    # Process texts: Handle complex structures\n    if \"texts\" in data_dict[\"context\"]:\n        formatted_texts = []\n        for text_block in data_dict[\"context\"][\"texts\"]:\n            if isinstance(text_block, list):\n                for text in text_block:\n                    # Check if the item is a dictionary and format it\n                    if isinstance(text, dict):\n                        text_description = ' | '.join(f\"{key}: {value}\" for key, value in text.items() if value is not None)\n                        formatted_texts.append(text_description)\n                    elif isinstance(text, str):\n                        # Directly append the string if it's not a dictionary\n                        formatted_texts.append(text)\n            elif isinstance(text_block, str):\n                # Handle the case where text_block is directly a string\n                formatted_texts.append(text_block)\n\n        formatted_text_string = \"\\n\".join(formatted_texts)\n        messages.append(formatted_text_string)\n\n    # Combine all messages into a single string with the question\n    prompt = (\n        f\"You are a research analyst. You should provide precise answers to each question\\n\"\n        f\"Question: {data_dict['question']}\\n\\n\"\n        \"Details:\\n\"\n        + \"\\n\".join(messages)\n    )\n    return prompt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n\n# RAG pipeline\nchain = (\n    {\n        \"context\": retriever | RunnableLambda(split_image_text_types),\n        \"question\": RunnablePassthrough(),\n    }\n    | RunnableLambda(img_prompt_func)\n    | model\n    | StrOutputParser()\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query='Who are the authors of \"GNN_DEEPRL\"?'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chain.invoke(query)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"docs = retriever.get_relevant_documents(query)\nlen(docs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"docs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Retriever llama index","metadata":{}},{"cell_type":"code","source":"import json\nfrom tqdm import tqdm  # \nfrom llama_index.core import SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.core.node_parser import SimpleNodeParser\nfrom llama_index.vector_stores.chroma import ChromaVectorStore\n\n# Load the JSON data from files\nwith open('processed_tableandtext.json', 'r') as file:\n    table_and_text_data = json.load(file)\n\nwith open('summarized_data.json', 'r') as file:\n    summarized_data = json.load(file)\n\nwith open('structured_image_data.json', 'r') as file:\n    structured_image_data = json.load(file)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T00:59:30.683450Z","iopub.execute_input":"2024-07-12T00:59:30.684071Z","iopub.status.idle":"2024-07-12T00:59:30.703057Z","shell.execute_reply.started":"2024-07-12T00:59:30.684043Z","shell.execute_reply":"2024-07-12T00:59:30.702135Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def normalize_title(title):\n    return title.lower().strip().replace('.pdf', '')\n\n# Normalize the keys in the data dictionaries\nnormalized_table_and_text_data = {normalize_title(key): value for key, value in table_and_text_data.items()}\nnormalized_summarized_data = {normalize_title(key): value for key, value in summarized_data.items()}\nnormalized_structured_image_data = {normalize_title(key): value for key, value in structured_image_data.items()}","metadata":{"execution":{"iopub.status.busy":"2024-07-12T00:59:30.867889Z","iopub.execute_input":"2024-07-12T00:59:30.868479Z","iopub.status.idle":"2024-07-12T00:59:30.874280Z","shell.execute_reply.started":"2024-07-12T00:59:30.868450Z","shell.execute_reply":"2024-07-12T00:59:30.873287Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"source_docs = SimpleDirectoryReader('test_data').load_data()\n\n# Create a node parser with desired settings\nbaseline_parser = SimpleNodeParser.from_defaults(\n    chunk_overlap=200,\n    chunk_size=1024\n)\n\n# Extract nodes from the documents with progress monitoring\nprint(\"Parsing documents into nodes...\")\nbaseline_nodes = []\nfor doc in tqdm(source_docs, desc=\"Parsing documents\"):\n    nodes = baseline_parser.get_nodes_from_documents([doc])\n    for node in nodes:\n        # Initialize extra_info if it doesn't exist\n        if not hasattr(node, 'extra_info'):\n            node.extra_info = {'document_title': doc.metadata.get('title', 'Unknown')}\n    baseline_nodes.extend(nodes)\nprint(\"Parsing complete.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:14:25.065579Z","iopub.execute_input":"2024-07-11T18:14:25.066320Z","iopub.status.idle":"2024-07-11T18:14:47.101342Z","shell.execute_reply.started":"2024-07-11T18:14:25.066285Z","shell.execute_reply":"2024-07-11T18:14:47.100381Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pypdf/_utils.py:233: RuntimeWarning: coroutine 'Executor.wrap_callable_with_index.<locals>.wrapped_callable_async' was never awaited\n  m = regex.search(name + tok)\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\n","output_type":"stream"},{"name":"stdout","text":"Parsing documents into nodes...\n","output_type":"stream"},{"name":"stderr","text":"Parsing documents: 100%|██████████| 179/179 [00:00<00:00, 229.00it/s]","output_type":"stream"},{"name":"stdout","text":"Parsing complete.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"from tqdm import tqdm  # Import tqdm for progress monitoring\n\ndef normalize_title(title):\n    return title.lower().strip().replace('.pdf', '')\n\ndef extend_nodes_with_summaries(nodes, normalized_table_and_text_data, normalized_summarized_data, normalized_structured_image_data):\n    extended_nodes = []\n    for node in tqdm(nodes, desc=\"Extending nodes\"):\n        # Normalize title for comparison\n        title = normalize_title(node.metadata['file_name'])  # Assuming the title is in metadata\n        print(f\"Processing node for title: {title}\")\n\n        # Handling table content\n        if title in normalized_table_and_text_data:\n            table_content = [item['content'] for item in normalized_table_and_text_data[title] if item['type'] == 'table']\n            node.metadata['table_content'] = table_content\n            print(f\"Added table content for title: {title}\")\n        else:\n            print(f\"Title {title} not found in normalized_table_and_text_data\")\n\n        # Handling summarized data for tables and texts\n        if title in normalized_summarized_data:\n            table_summaries = normalized_summarized_data[title].get('table_summaries', [])\n            text_summaries = normalized_summarized_data[title].get('text_summaries', [])\n            node.metadata['table_summaries'] = table_summaries\n            node.metadata['text_summaries'] = text_summaries\n            if table_summaries:\n                print(f\"Added table summaries for title: {title}\")\n            if text_summaries:\n                print(f\"Added text summaries for title: {title}\")\n        else:\n            print(f\"Title {title} not found in normalized_summarized_data\")\n\n        # Handling summarized data for images\n        if title in normalized_structured_image_data:\n            image_summaries = normalized_structured_image_data[title]\n            node.metadata['image_summaries'] = image_summaries\n            print(f\"Added image summaries for title: {title}\")\n        else:\n            print(f\"Title {title} not found in normalized_structured_image_data\")\n\n        extended_nodes.append(node)\n    \n    return extended_nodes\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:14:51.275216Z","iopub.execute_input":"2024-07-11T18:14:51.275850Z","iopub.status.idle":"2024-07-11T18:14:51.286463Z","shell.execute_reply.started":"2024-07-11T18:14:51.275813Z","shell.execute_reply":"2024-07-11T18:14:51.285445Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"extended_baseline_nodes = extend_nodes_with_summaries(baseline_nodes, normalized_table_and_text_data, normalized_summarized_data, normalized_structured_image_data)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:14:53.635159Z","iopub.execute_input":"2024-07-11T18:14:53.635840Z","iopub.status.idle":"2024-07-11T18:14:53.657597Z","shell.execute_reply.started":"2024-07-11T18:14:53.635806Z","shell.execute_reply":"2024-07-11T18:14:53.656688Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"Extending nodes: 100%|██████████| 299/299 [00:00<00:00, 21105.28it/s]","output_type":"stream"},{"name":"stdout","text":"Processing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: fmri_vision_reconstruction\nAdded table content for title: fmri_vision_reconstruction\nAdded table summaries for title: fmri_vision_reconstruction\nAdded text summaries for title: fmri_vision_reconstruction\nAdded image summaries for title: fmri_vision_reconstruction\nProcessing node for title: fmri_vision_reconstruction\nAdded table content for title: fmri_vision_reconstruction\nAdded table summaries for title: fmri_vision_reconstruction\nAdded text summaries for title: fmri_vision_reconstruction\nAdded image summaries for title: fmri_vision_reconstruction\nProcessing node for title: fmri_vision_reconstruction\nAdded table content for title: fmri_vision_reconstruction\nAdded table summaries for title: fmri_vision_reconstruction\nAdded text summaries for title: fmri_vision_reconstruction\nAdded image summaries for title: fmri_vision_reconstruction\nProcessing node for title: fmri_vision_reconstruction\nAdded table content for title: fmri_vision_reconstruction\nAdded table summaries for title: fmri_vision_reconstruction\nAdded text summaries for title: fmri_vision_reconstruction\nAdded image summaries for title: fmri_vision_reconstruction\nProcessing node for title: fmri_vision_reconstruction\nAdded table content for title: fmri_vision_reconstruction\nAdded table summaries for title: fmri_vision_reconstruction\nAdded text summaries for title: fmri_vision_reconstruction\nAdded image summaries for title: fmri_vision_reconstruction\nProcessing node for title: fmri_vision_reconstruction\nAdded table content for title: fmri_vision_reconstruction\nAdded table summaries for title: fmri_vision_reconstruction\nAdded text summaries for title: fmri_vision_reconstruction\nAdded image summaries for title: fmri_vision_reconstruction\nProcessing node for title: fmri_vision_reconstruction\nAdded table content for title: fmri_vision_reconstruction\nAdded table summaries for title: fmri_vision_reconstruction\nAdded text summaries for title: fmri_vision_reconstruction\nAdded image summaries for title: fmri_vision_reconstruction\nProcessing node for title: fmri_vision_reconstruction\nAdded table content for title: fmri_vision_reconstruction\nAdded table summaries for title: fmri_vision_reconstruction\nAdded text summaries for title: fmri_vision_reconstruction\nAdded image summaries for title: fmri_vision_reconstruction\nProcessing node for title: fmri_vision_reconstruction\nAdded table content for title: fmri_vision_reconstruction\nAdded table summaries for title: fmri_vision_reconstruction\nAdded text summaries for title: fmri_vision_reconstruction\nAdded image summaries for title: fmri_vision_reconstruction\nProcessing node for title: fmri_vision_reconstruction\nAdded table content for title: fmri_vision_reconstruction\nAdded table summaries for title: fmri_vision_reconstruction\nAdded text summaries for title: fmri_vision_reconstruction\nAdded image summaries for title: fmri_vision_reconstruction\nProcessing node for title: fmri_vision_reconstruction\nAdded table content for title: fmri_vision_reconstruction\nAdded table summaries for title: fmri_vision_reconstruction\nAdded text summaries for title: fmri_vision_reconstruction\nAdded image summaries for title: fmri_vision_reconstruction\nProcessing node for title: fmri_vision_reconstruction\nAdded table content for title: fmri_vision_reconstruction\nAdded table summaries for title: fmri_vision_reconstruction\nAdded text summaries for title: fmri_vision_reconstruction\nAdded image summaries for title: fmri_vision_reconstruction\nProcessing node for title: fmri_vision_reconstruction\nAdded table content for title: fmri_vision_reconstruction\nAdded table summaries for title: fmri_vision_reconstruction\nAdded text summaries for title: fmri_vision_reconstruction\nAdded image summaries for title: fmri_vision_reconstruction\nProcessing node for title: fmri_vision_reconstruction\nAdded table content for title: fmri_vision_reconstruction\nAdded table summaries for title: fmri_vision_reconstruction\nAdded text summaries for title: fmri_vision_reconstruction\nAdded image summaries for title: fmri_vision_reconstruction\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"for node in extended_baseline_nodes[:3]:  # Check the first 3 nodes for demonstration\n    print(f\"Title: {node.metadata['file_name']}\")\n    print(f\"Table Content: {node.metadata['table_summaries']}\")\n    print(f\"Table Summaries: {node.metadata['table_summaries']}\")\n    print(f\"Text Summaries: {node.metadata['text_summaries']}\")\n    print(f\"Image Summaries: {node.metadata['image_summaries']}\")\n    print(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from llama_index.core import Settings\nSettings.llm=llm\nSettings.embed_model=embedding_model","metadata":{"execution":{"iopub.status.busy":"2024-07-12T03:02:48.263000Z","iopub.execute_input":"2024-07-12T03:02:48.263713Z","iopub.status.idle":"2024-07-12T03:02:48.268336Z","shell.execute_reply.started":"2024-07-12T03:02:48.263679Z","shell.execute_reply":"2024-07-12T03:02:48.267344Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"import time\nimport chromadb\nfrom llama_index.core import StorageContext\n\n# Retry mechanism for handling rate limits\n\n\ndef create_and_persist_index():\n    # Create the Chroma client and collection using a persistent client\n    print(\"Initializing Chroma persistent client and collection...\")\n    db = chromadb.PersistentClient(path=\"./chroma_db\")  # Define the storage path\n    chroma_collection = db.get_or_create_collection(\"baseline_indexNew\")\n    \n    print(f\"Collection '{chroma_collection.name}' ready for use.\")\n\n    # Create the Chroma vector store\n    print(\"Initializing ChromaVectorStore...\")\n    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n    print(\"ChromaVectorStore initialized with collection_name 'baseline_indexNew'\")\n\n    # Create the index\n    print(\"Creating VectorStoreIndex...\")\n    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n    baseline_index = VectorStoreIndex.from_documents(\n        documents=source_docs,\n        storage_context=storage_context,\n        embed_model=embedding_model\n    )\n    print(\"VectorStoreIndex created\")\n\n    # Persist the index\n    print(\"Persisting VectorStoreIndex...\")\n    baseline_index.storage_context.persist()\n    print(\"VectorStoreIndex persisted successfully.\")\n    return baseline_index\n\nbaseline_index = retry_with_backoff(create_and_persist_index)\nprint(\"Index creation and persistence complete.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:24:52.426440Z","iopub.execute_input":"2024-07-11T18:24:52.427128Z","iopub.status.idle":"2024-07-11T18:26:29.521795Z","shell.execute_reply.started":"2024-07-11T18:24:52.427094Z","shell.execute_reply":"2024-07-11T18:26:29.520772Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Creating and persisting index with retries...\nInitializing Chroma persistent client and collection...\nCollection 'baseline_indexNew' ready for use.\nInitializing ChromaVectorStore...\nChromaVectorStore initialized with collection_name 'baseline_indexNew'\nCreating VectorStoreIndex...\nVectorStoreIndex created\nPersisting VectorStoreIndex...\nVectorStoreIndex persisted successfully.\nIndex creation and persistence complete.\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Converting index to query engine...\")\nbaseline_query_engine = baseline_index.as_query_engine(similarity_top_k=3)\nprint(\"Query engine ready.\")\n\n# Function to retrieve data based on a query\ndef retrieve_data(query):\n    results = baseline_query_engine.query(query)\n    return results\n\n# Example query\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:31:22.103923Z","iopub.execute_input":"2024-07-11T18:31:22.104680Z","iopub.status.idle":"2024-07-11T18:31:22.110174Z","shell.execute_reply.started":"2024-07-11T18:31:22.104647Z","shell.execute_reply":"2024-07-11T18:31:22.109337Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Converting index to query engine...\nQuery engine ready.\n","output_type":"stream"}]},{"cell_type":"code","source":"query = 'what is multimodal condition in talking face generation?'\nretrieved_data = retrieve_data(query)\nprint(retrieved_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_output(results):\n    for result in results:\n        node = result.node\n        print(\"Document ID:\", node.id_)\n        print(\"File Name:\", node.metadata['file_name'])\n        print(\"File Type:\", node.metadata['file_type'])\n        print(\"Creation Date:\", node.metadata['creation_date'])\n        print(\"Page Label:\", node.metadata['page_label'])\n        print(\"File Path:\", node.metadata['file_path'])\n        print()\n        print(\"Text Extract:\")\n        print(node.text[:300])  # Displaying first 300 characters for brevity\n        print(\"-\" * 80)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=baseline_query_engine.retrieve(query)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Custom Retriever","metadata":{}},{"cell_type":"code","source":"import time\nfrom tqdm import tqdm\nimport chromadb\nfrom llama_index.core import Document, VectorStoreIndex, StorageContext\nfrom llama_index.core.node_parser import SimpleNodeParser\nfrom llama_index.core.schema import Node, NodeRelationship, RelatedNodeInfo\nfrom llama_index.vector_stores.chroma import ChromaVectorStore\nimport time\nimport chromadb\nimport multiprocessing\nfrom tqdm import tqdm\nimport logging\nfrom functools import partial\ndef normalize_title(title):\n    return title.lower().strip().replace('.pdf', '')\n\ndef process_document(doc, normalized_table_and_text_data, normalized_summarized_data, normalized_structured_image_data):\n    all_nodes = []\n    main_node = Node(\n        text=doc.text,\n        metadata={\n            \"file_name\": doc.metadata.get(\"file_name\"),\n            \"doc_id\": doc.id_,\n            \"is_document\": True\n        }\n    )\n    all_nodes.append(main_node)\n\n    title = normalize_title(doc.metadata.get(\"file_name\", \"\"))\n    print(\"PROCESSING FILE:\",title)\n\n    # Create nodes for table content\n    if title in normalized_table_and_text_data:\n        for item in normalized_table_and_text_data[title]:\n            if item['type'] == 'table':\n                table_node = Node(\n                    text=f\"Table Content: {item['content']}\",\n                    metadata={\n                        \"doc_id\": doc.id_,\n                        \"type\": \"table_content\"\n                    }\n                )\n                all_nodes.append(table_node)\n                main_node.relationships[NodeRelationship.CHILD] = main_node.relationships.get(NodeRelationship.CHILD, []) + [\n                    RelatedNodeInfo(node_id=table_node.id_)\n                ]\n\n    # Create nodes for table summaries\n    if title in normalized_summarized_data:\n        for summary in normalized_summarized_data[title].get('table_summaries', []):\n            table_summary_node = Node(\n                text=f\"Table Summary: {summary}\",\n                metadata={\n                    \"doc_id\": doc.id_,\n                    \"type\": \"table_summary\"\n                }\n            )\n            all_nodes.append(table_summary_node)\n            main_node.relationships[NodeRelationship.CHILD] = main_node.relationships.get(NodeRelationship.CHILD, []) + [\n                RelatedNodeInfo(node_id=table_summary_node.id_)\n            ]\n\n    # Create nodes for text summaries\n    if title in normalized_summarized_data:\n        for summary in normalized_summarized_data[title].get('text_summaries', []):\n            text_summary_node = Node(\n                text=f\"Text Summary: {summary}\",\n                metadata={\n                    \"doc_id\": doc.id_,\n                    \"type\": \"text_summary\"\n                }\n            )\n            all_nodes.append(text_summary_node)\n            main_node.relationships[NodeRelationship.CHILD] = main_node.relationships.get(NodeRelationship.CHILD, []) + [\n                RelatedNodeInfo(node_id=text_summary_node.id_)\n            ]\n\n    # Create nodes for image summaries\n    if title in normalized_structured_image_data:\n        for summary in normalized_structured_image_data[title]:\n            image_summary_node = Node(\n                text=f\"Image Summary: {summary}\",\n                metadata={\n                    \"doc_id\": doc.id_,\n                    \"type\": \"image_summary\"\n                }\n            )\n            all_nodes.append(image_summary_node)\n            main_node.relationships[NodeRelationship.CHILD] = main_node.relationships.get(NodeRelationship.CHILD, []) + [\n                RelatedNodeInfo(node_id=image_summary_node.id_)\n            ]\n\n    return all_nodes","metadata":{"execution":{"iopub.status.busy":"2024-07-12T02:00:05.152931Z","iopub.execute_input":"2024-07-12T02:00:05.153324Z","iopub.status.idle":"2024-07-12T02:00:05.170141Z","shell.execute_reply.started":"2024-07-12T02:00:05.153292Z","shell.execute_reply":"2024-07-12T02:00:05.169201Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def create_and_persist_index_for_document(doc, vector_store, retry_delay=60, max_retries=5):\n    for attempt in range(max_retries):\n        try:\n            nodes = process_document(doc, normalized_table_and_text_data, normalized_summarized_data, normalized_structured_image_data)\n            storage_context = StorageContext.from_defaults(vector_store=vector_store)\n            index = VectorStoreIndex(nodes, storage_context=storage_context)\n            return index\n        except Exception as e:\n            if \"rate limit\" in str(e).lower():\n                if attempt < max_retries - 1:\n                    logging.warning(f\"Rate limit error. Retrying in {retry_delay} seconds... (Attempt {attempt + 1}/{max_retries})\")\n                    time.sleep(retry_delay)\n                else:\n                    logging.error(f\"Max retries exceeded for document: {doc.metadata.get('file_name')}\")\n                    raise\n            else:\n                logging.error(f\"Unexpected error: {e}\")\n                raise","metadata":{"execution":{"iopub.status.busy":"2024-07-12T02:00:06.283230Z","iopub.execute_input":"2024-07-12T02:00:06.284072Z","iopub.status.idle":"2024-07-12T02:00:06.292278Z","shell.execute_reply.started":"2024-07-12T02:00:06.284037Z","shell.execute_reply":"2024-07-12T02:00:06.291058Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def add_document_to_index(doc, index, retry_delay=60, max_retries=5):\n    for attempt in range(max_retries):\n        try:\n            nodes = process_document(doc, normalized_table_and_text_data, normalized_summarized_data, normalized_structured_image_data)\n            index.insert_nodes(nodes)\n            return True\n        except Exception as e:\n            if \"rate limit\" in str(e).lower():\n                if attempt < max_retries - 1:\n                    logging.warning(f\"Rate limit error. Retrying in {retry_delay} seconds... (Attempt {attempt + 1}/{max_retries})\")\n                    time.sleep(retry_delay)\n                else:\n                    logging.error(f\"Max retries exceeded for document: {doc.metadata.get('file_name')}\")\n                    return False\n            else:\n                logging.error(f\"Unexpected error: {e}\")\n                return False\n\ndef create_full_index(source_docs):\n    logging.info(\"Creating Chroma client and collection\")\n    chroma_client = chromadb.EphemeralClient()\n    chroma_collection = chroma_client.get_or_create_collection(\"advanced_rag_index\")\n    \n    logging.info(\"Initializing vector store and storage context\")\n    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n\n    logging.info(\"Creating empty index\")\n    index = VectorStoreIndex([], storage_context=storage_context.property_graph_store,embed_model=embedding_model)\n    \n    for doc in tqdm(source_docs, desc=\"Processing documents\"):\n        success = add_document_to_index(doc, index)\n        if success:\n            logging.info(f\"Successfully processed document: {doc.metadata.get('file_name')}\")\n        else:\n            logging.error(f\"Failed to process document: {doc.metadata.get('file_name')}\")\n        time.sleep(5)  # Add a delay between documents to avoid rate limiting\n\n    logging.info(\"Persisting final index\")\n    index.storage_context.persist()\n    logging.info(\"Full index creation and persistence complete\")\n\n    return index\n","metadata":{"execution":{"iopub.status.busy":"2024-07-12T02:05:49.162797Z","iopub.execute_input":"2024-07-12T02:05:49.163165Z","iopub.status.idle":"2024-07-12T02:05:49.174266Z","shell.execute_reply.started":"2024-07-12T02:05:49.163137Z","shell.execute_reply":"2024-07-12T02:05:49.173339Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"index = create_full_index(source_docs)\nif index:\n    print(\"Full index created successfully\")\nelse:\n    print(\"Failed to create full index\")","metadata":{"execution":{"iopub.status.busy":"2024-07-12T02:05:51.280916Z","iopub.execute_input":"2024-07-12T02:05:51.281292Z","iopub.status.idle":"2024-07-12T03:02:00.893088Z","shell.execute_reply.started":"2024-07-12T02:05:51.281261Z","shell.execute_reply":"2024-07-12T03:02:00.892125Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"Processing documents:   0%|          | 0/179 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   1%|          | 1/179 [00:17<52:54, 17.83s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   1%|          | 2/179 [00:36<53:32, 18.15s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   2%|▏         | 3/179 [01:20<1:27:38, 29.88s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   2%|▏         | 4/179 [01:38<1:14:09, 25.43s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   3%|▎         | 5/179 [01:57<1:06:33, 22.95s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   3%|▎         | 6/179 [02:15<1:01:33, 21.35s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   4%|▍         | 7/179 [02:33<58:25, 20.38s/it]  ","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   4%|▍         | 8/179 [02:51<56:01, 19.66s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   5%|▌         | 9/179 [03:10<54:30, 19.24s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   6%|▌         | 10/179 [03:28<53:19, 18.93s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   6%|▌         | 11/179 [03:46<52:25, 18.72s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   7%|▋         | 12/179 [04:05<51:56, 18.66s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   7%|▋         | 13/179 [04:23<51:20, 18.56s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   8%|▊         | 14/179 [04:41<50:46, 18.47s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   8%|▊         | 15/179 [04:59<50:12, 18.37s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   9%|▉         | 16/179 [05:18<49:40, 18.29s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   9%|▉         | 17/179 [05:36<49:29, 18.33s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  10%|█         | 18/179 [05:54<49:18, 18.38s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  11%|█         | 19/179 [06:13<48:47, 18.30s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  11%|█         | 20/179 [06:31<48:44, 18.40s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  12%|█▏        | 21/179 [06:50<48:25, 18.39s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  12%|█▏        | 22/179 [07:08<48:13, 18.43s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  13%|█▎        | 23/179 [07:27<47:52, 18.41s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  13%|█▎        | 24/179 [07:45<47:32, 18.40s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  14%|█▍        | 25/179 [08:03<47:19, 18.44s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  15%|█▍        | 26/179 [08:22<47:01, 18.44s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  15%|█▌        | 27/179 [08:40<46:38, 18.41s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  16%|█▌        | 28/179 [08:58<46:13, 18.36s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: fmri_vision_reconstruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  16%|█▌        | 29/179 [09:08<39:02, 15.62s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: fmri_vision_reconstruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  17%|█▋        | 30/179 [09:17<33:51, 13.64s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: fmri_vision_reconstruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  17%|█▋        | 31/179 [09:26<30:21, 12.31s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: fmri_vision_reconstruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  18%|█▊        | 32/179 [09:35<27:51, 11.37s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: fmri_vision_reconstruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  18%|█▊        | 33/179 [09:44<26:05, 10.72s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: fmri_vision_reconstruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  19%|█▉        | 34/179 [09:53<24:46, 10.25s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: fmri_vision_reconstruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  20%|█▉        | 35/179 [10:02<23:45,  9.90s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: fmri_vision_reconstruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  20%|██        | 36/179 [10:12<23:03,  9.68s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: fmri_vision_reconstruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  21%|██        | 37/179 [10:21<22:25,  9.48s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: fmri_vision_reconstruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  21%|██        | 38/179 [10:30<22:04,  9.40s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: fmri_vision_reconstruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  22%|██▏       | 39/179 [10:39<21:50,  9.36s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: fmri_vision_reconstruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  22%|██▏       | 40/179 [10:48<21:32,  9.30s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  23%|██▎       | 41/179 [11:03<25:06, 10.92s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  23%|██▎       | 42/179 [11:18<27:41, 12.13s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  24%|██▍       | 43/179 [11:33<29:42, 13.11s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  25%|██▍       | 44/179 [11:49<30:55, 13.74s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  25%|██▌       | 45/179 [12:04<31:45, 14.22s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  26%|██▌       | 46/179 [12:19<32:07, 14.49s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  26%|██▋       | 47/179 [12:34<32:18, 14.69s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  27%|██▋       | 48/179 [12:49<32:22, 14.83s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  27%|██▋       | 49/179 [13:04<32:15, 14.89s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  28%|██▊       | 50/179 [13:19<32:05, 14.93s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  28%|██▊       | 51/179 [13:35<32:04, 15.04s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  29%|██▉       | 52/179 [13:50<31:47, 15.02s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  30%|██▉       | 53/179 [14:05<31:35, 15.04s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  30%|███       | 54/179 [14:20<31:25, 15.09s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  31%|███       | 55/179 [14:35<31:13, 15.11s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  31%|███▏      | 56/179 [14:50<31:05, 15.16s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  32%|███▏      | 57/179 [15:05<30:45, 15.12s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  32%|███▏      | 58/179 [15:20<30:26, 15.10s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  33%|███▎      | 59/179 [15:36<30:21, 15.18s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  34%|███▎      | 60/179 [15:51<30:07, 15.19s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  34%|███▍      | 61/179 [16:06<29:47, 15.15s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  35%|███▍      | 62/179 [16:21<29:27, 15.10s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  35%|███▌      | 63/179 [16:36<29:10, 15.09s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  36%|███▌      | 64/179 [16:51<28:54, 15.08s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  36%|███▋      | 65/179 [17:06<28:34, 15.04s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  37%|███▋      | 66/179 [17:21<28:25, 15.09s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  37%|███▋      | 67/179 [17:37<28:13, 15.12s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  38%|███▊      | 68/179 [18:00<32:19, 17.47s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  39%|███▊      | 69/179 [18:22<35:02, 19.11s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  39%|███▉      | 70/179 [18:46<36:58, 20.36s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  40%|███▉      | 71/179 [19:09<37:59, 21.11s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  40%|████      | 72/179 [19:31<38:34, 21.63s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  41%|████      | 73/179 [19:58<41:00, 23.22s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  41%|████▏     | 74/179 [20:23<41:14, 23.56s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  42%|████▏     | 75/179 [20:46<40:38, 23.45s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  42%|████▏     | 76/179 [21:09<40:13, 23.43s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  43%|████▎     | 77/179 [21:33<39:46, 23.40s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  44%|████▎     | 78/179 [21:56<39:21, 23.38s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  44%|████▍     | 79/179 [22:19<38:51, 23.32s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  45%|████▍     | 80/179 [22:42<38:19, 23.22s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  45%|████▌     | 81/179 [23:05<37:59, 23.26s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  46%|████▌     | 82/179 [23:28<37:25, 23.15s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  46%|████▋     | 83/179 [23:51<36:56, 23.08s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  47%|████▋     | 84/179 [24:14<36:34, 23.10s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  47%|████▋     | 85/179 [24:37<36:07, 23.06s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  48%|████▊     | 86/179 [25:01<35:50, 23.13s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  49%|████▊     | 87/179 [25:24<35:24, 23.09s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  49%|████▉     | 88/179 [25:47<34:55, 23.03s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  50%|████▉     | 89/179 [26:10<34:38, 23.10s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  50%|█████     | 90/179 [26:33<34:21, 23.17s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  51%|█████     | 91/179 [26:56<33:58, 23.17s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  51%|█████▏    | 92/179 [27:20<33:42, 23.24s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  52%|█████▏    | 93/179 [27:43<33:18, 23.23s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  53%|█████▎    | 94/179 [28:06<32:49, 23.17s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  53%|█████▎    | 95/179 [28:29<32:27, 23.19s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  54%|█████▎    | 96/179 [28:53<32:09, 23.24s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  54%|█████▍    | 97/179 [29:16<31:43, 23.22s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  55%|█████▍    | 98/179 [29:39<31:17, 23.18s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  55%|█████▌    | 99/179 [30:02<30:57, 23.22s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  56%|█████▌    | 100/179 [30:29<32:08, 24.41s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  56%|█████▋    | 101/179 [30:52<31:14, 24.03s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  57%|█████▋    | 102/179 [31:15<30:24, 23.70s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  58%|█████▊    | 103/179 [31:39<30:04, 23.75s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  58%|█████▊    | 104/179 [32:02<29:25, 23.55s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  59%|█████▊    | 105/179 [32:26<28:56, 23.46s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  59%|█████▉    | 106/179 [32:51<29:15, 24.05s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  60%|█████▉    | 107/179 [33:14<28:30, 23.76s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  60%|██████    | 108/179 [33:37<27:55, 23.60s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  61%|██████    | 109/179 [34:01<27:42, 23.74s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  61%|██████▏   | 110/179 [34:25<27:09, 23.61s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  62%|██████▏   | 111/179 [34:48<26:38, 23.51s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  63%|██████▎   | 112/179 [35:11<26:09, 23.43s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  63%|██████▎   | 113/179 [35:35<25:44, 23.40s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  64%|██████▎   | 114/179 [35:58<25:15, 23.31s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  64%|██████▍   | 115/179 [36:21<24:48, 23.27s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  65%|██████▍   | 116/179 [36:44<24:25, 23.26s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  65%|██████▌   | 117/179 [37:07<24:01, 23.24s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  66%|██████▌   | 118/179 [37:31<23:50, 23.45s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  66%|██████▋   | 119/179 [37:54<23:16, 23.28s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  67%|██████▋   | 120/179 [38:17<22:46, 23.16s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  68%|██████▊   | 121/179 [38:40<22:13, 23.00s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  68%|██████▊   | 122/179 [39:03<22:04, 23.23s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  69%|██████▊   | 123/179 [39:26<21:35, 23.14s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  69%|██████▉   | 124/179 [39:49<21:13, 23.15s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  70%|██████▉   | 125/179 [40:13<20:51, 23.18s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  70%|███████   | 126/179 [40:36<20:27, 23.16s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  71%|███████   | 127/179 [40:48<17:15, 19.91s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  72%|███████▏  | 128/179 [41:00<14:59, 17.63s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  72%|███████▏  | 129/179 [41:13<13:18, 15.97s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  73%|███████▎  | 130/179 [41:25<12:06, 14.82s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  73%|███████▎  | 131/179 [41:37<11:14, 14.06s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  74%|███████▎  | 132/179 [41:49<10:37, 13.56s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  74%|███████▍  | 133/179 [42:02<10:05, 13.16s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  75%|███████▍  | 134/179 [42:14<09:39, 12.89s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  75%|███████▌  | 135/179 [42:26<09:20, 12.74s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  76%|███████▌  | 136/179 [42:39<09:01, 12.60s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  77%|███████▋  | 137/179 [42:51<08:45, 12.50s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  77%|███████▋  | 138/179 [43:03<08:30, 12.46s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  78%|███████▊  | 139/179 [43:16<08:23, 12.60s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  78%|███████▊  | 140/179 [43:28<08:07, 12.49s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  79%|███████▉  | 141/179 [43:40<07:50, 12.38s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  79%|███████▉  | 142/179 [43:52<07:34, 12.27s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  80%|███████▉  | 143/179 [44:05<07:22, 12.29s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  80%|████████  | 144/179 [44:17<07:07, 12.22s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  81%|████████  | 145/179 [44:29<06:54, 12.19s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  82%|████████▏ | 146/179 [44:41<06:40, 12.14s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  82%|████████▏ | 147/179 [44:53<06:28, 12.13s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  83%|████████▎ | 148/179 [45:05<06:15, 12.11s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  83%|████████▎ | 149/179 [45:17<06:04, 12.15s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  84%|████████▍ | 150/179 [45:29<05:51, 12.11s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  84%|████████▍ | 151/179 [45:42<05:40, 12.17s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  85%|████████▍ | 152/179 [45:50<04:54, 10.90s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  85%|████████▌ | 153/179 [45:58<04:21, 10.05s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  86%|████████▌ | 154/179 [46:06<03:57,  9.50s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  87%|████████▋ | 155/179 [46:14<03:38,  9.10s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  87%|████████▋ | 156/179 [46:22<03:23,  8.83s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  88%|████████▊ | 157/179 [46:31<03:10,  8.65s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  88%|████████▊ | 158/179 [46:39<02:58,  8.50s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  89%|████████▉ | 159/179 [46:51<03:09,  9.49s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  89%|████████▉ | 160/179 [46:58<02:51,  9.03s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  90%|████████▉ | 161/179 [47:06<02:36,  8.72s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  91%|█████████ | 162/179 [47:14<02:24,  8.50s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  91%|█████████ | 163/179 [47:23<02:14,  8.40s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  92%|█████████▏| 164/179 [47:31<02:05,  8.34s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  92%|█████████▏| 165/179 [47:39<01:56,  8.30s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  93%|█████████▎| 166/179 [47:52<02:05,  9.66s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  93%|█████████▎| 167/179 [48:00<01:50,  9.17s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  94%|█████████▍| 168/179 [48:08<01:37,  8.82s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  94%|█████████▍| 169/179 [48:16<01:25,  8.58s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  95%|█████████▍| 170/179 [48:24<01:16,  8.46s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  96%|█████████▌| 171/179 [48:32<01:06,  8.37s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  96%|█████████▌| 172/179 [48:40<00:57,  8.25s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  97%|█████████▋| 173/179 [48:49<00:49,  8.27s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  97%|█████████▋| 174/179 [48:59<00:45,  9.03s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  98%|█████████▊| 175/179 [49:08<00:35,  8.79s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  98%|█████████▊| 176/179 [49:16<00:25,  8.63s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  99%|█████████▉| 177/179 [49:24<00:16,  8.49s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  99%|█████████▉| 178/179 [49:32<00:08,  8.41s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents: 100%|██████████| 179/179 [49:40<00:00, 16.65s/it]\n","output_type":"stream"},{"name":"stdout","text":"Full index created successfully\n","output_type":"stream"}]},{"cell_type":"code","source":"print(index)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T03:02:00.894821Z","iopub.execute_input":"2024-07-12T03:02:00.895129Z","iopub.status.idle":"2024-07-12T03:02:00.899743Z","shell.execute_reply.started":"2024-07-12T03:02:00.895103Z","shell.execute_reply":"2024-07-12T03:02:00.898899Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"<llama_index.core.indices.vector_store.base.VectorStoreIndex object at 0x7cf7d3e59e70>\n","output_type":"stream"}]},{"cell_type":"code","source":"qe = index.as_query_engine(similarity_top_k=3)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T03:57:14.663196Z","iopub.execute_input":"2024-07-12T03:57:14.663902Z","iopub.status.idle":"2024-07-12T03:57:14.669001Z","shell.execute_reply.started":"2024-07-12T03:57:14.663871Z","shell.execute_reply":"2024-07-12T03:57:14.668016Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"y=qe.query(\"what is multimodal condition in talking face generation?\")","metadata":{"execution":{"iopub.status.busy":"2024-07-12T04:02:10.153025Z","iopub.execute_input":"2024-07-12T04:02:10.153798Z","iopub.status.idle":"2024-07-12T04:02:16.366001Z","shell.execute_reply.started":"2024-07-12T04:02:10.153766Z","shell.execute_reply":"2024-07-12T04:02:16.365029Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"execution":{"iopub.status.busy":"2024-07-12T04:02:29.083870Z","iopub.execute_input":"2024-07-12T04:02:29.084466Z","iopub.status.idle":"2024-07-12T04:02:29.091750Z","shell.execute_reply.started":"2024-07-12T04:02:29.084434Z","shell.execute_reply":"2024-07-12T04:02:29.090737Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"Response(response='Multimodal condition in talking face generation involves introducing additional modal information, such as text, image, and audio-emotional modalities, to guide facial pose and expression in generated videos. This approach aims to complement emotional content in textual information and enhance the vividness of the generated videos.', source_nodes=[NodeWithScore(node=TextNode(id_='e191431b-e323-4795-b7cd-c66750890fff', embedding=None, metadata={'file_name': 'Deepfake.pdf', 'doc_id': '10121698-98b4-4107-8cb3-7b0b7e8285c7', 'is_document': True}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.CHILD: '5'>: [RelatedNodeInfo(node_id='b8b8a401-7a3b-4c13-8083-c573b1b3ee65', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='c1927463-7e56-44bb-87f1-fa43298afd26', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='2ad9aefb-a183-45a3-bbf4-1f4fe8e4ecf8', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='bcc173b1-8102-4a1d-87e9-9ea403fd585b', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='bf670220-3983-4054-911d-559f99f5d640', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='ea26ab5d-ce6f-4a07-82ee-4fcefecbcb8c', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='14ef9c0b-1448-4e80-83d3-01970a857a5e', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='9fb22a24-203c-4d50-84e4-7b0c497a90ad', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='6e62adf0-6cf8-4fc7-b89d-ab7bf5d3b683', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='e995131e-5cdf-44cf-a2d1-39d579a8bf9e', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='d5b10257-a494-4ede-b462-2d276b59a74b', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='fd523a64-8c6d-4927-9985-63d8123fdb9f', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='c277c751-5f44-4c25-968a-c106d700ca18', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='aba1f1ec-649f-44ed-bbd8-c52fd4082764', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='39bacbb7-415e-43f5-aa39-2c3dd8fc8c83', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='85ebf18d-297a-4fe7-bbab-e953a406165b', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='15ee6e4d-227f-4e1d-abef-d405c64a06b4', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='9099cbef-4446-4ebf-9ccd-687f90188b84', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='1257b0d9-3364-4274-b69f-63f62c02080e', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='112caa9e-8b69-4ea9-a2a2-884c5f416629', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='4f7a3f0e-d3e0-4c99-b92c-250a61c14af8', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='2433c0f0-a70b-4a85-ae46-719e2c3bdcab', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='db6090bd-7910-42a4-89be-bb7fd86e7988', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='f7a0d7f0-3ff1-420b-a6c4-e96a9fa148c0', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='94886b23-2295-4489-b64a-1d061d47d72e', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='5f1b5419-effa-4032-b3cf-76ff412a1c9f', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='e333caf9-f995-4c24-935a-908feb138634', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='b831520f-c8fc-4eb2-94f1-cd0a3d725e3f', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='3ff0a048-9b3a-4cf2-831b-73478261ebf6', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='928b923e-eb27-41cf-af0a-106dfccfa742', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='72cd35d4-042f-4cd2-a078-59bd572ad6a0', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='8abcbdbe-452f-407c-8782-30afb469e0c7', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='a712fb63-034f-448a-a6e1-a3da19d01430', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='8e572d6c-7ba9-4fc5-8a6b-bc31fd7a0c22', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='825d0710-7162-4195-965c-d4061ec6773a', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='728a27e0-ddcd-4295-86b6-1055402d1dde', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='fcbbdc19-dc19-4630-9647-80001f5e40ee', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='7c3c4b2a-4e04-497a-8fc6-bd8943201505', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='6961f746-d65e-4e10-aff6-d50830031f58', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='bf74705f-fbef-4b6d-a06c-5b801aa1a878', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='05ef08af-99fa-4c97-a8a1-c98c1b5e41dd', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='d2c04758-fc21-473e-8057-3272608dcc9d', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='4a612660-fe8b-4083-acbe-e055713d5306', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='ada32c38-a7bd-40bd-864a-eaed99cc59c6', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='f5bc99af-3dd1-47bc-a59d-efe1a224ec74', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='cbbefac7-6a9e-49aa-b73a-45da8e6968c1', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='789bab2c-20a1-46e4-b259-4cd77bbc813d', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='0affa310-af6f-47df-ae67-7927d0a9b5f4', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='707911d3-c6b9-4655-a4c4-bd867f1452a5', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='94a72f5b-5b23-4821-b18c-d410a2e4e13c', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='d05220ae-d869-4509-bf52-e5795d056c16', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='39ecedf1-1735-4274-b89e-09c4a3efefd6', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='be785dbd-f6b0-4d9a-82c3-690e6b9bfc65', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='4044aab4-a9df-4ddb-9024-7133c8133da8', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='29f5913a-7a05-4d8d-9a99-01eb26a43e95', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='7d80d7ff-ff8e-4d79-a282-11bfc2ca0923', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='94f18b4d-670a-4c08-9326-fa0c9d3a0f91', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='c00bf39c-df11-49a4-b0dc-17a18865a8a0', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='9e7f3c4d-65e1-4f51-84cb-7db348e01c19', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='42f930cc-46b4-458e-bfb2-24e586f5cfd4', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='8357a684-9aaa-40ec-b5d3-18a66581f971', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='1f2e47e5-36c7-4d1b-a382-a521aa122d23', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='da605ba1-a04c-4095-8883-e4b8b03fed24', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='5aa2f93d-0a8c-4c32-bb6e-b5c89254388b', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='1a3f60eb-9add-4b4b-b341-43bc54844dd6', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='bbc04526-83ed-437f-b382-feced38b18f2', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='08cb407e-f4cd-4758-90d9-ece88618fb2e', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='a98c9702-8f93-49a9-b98a-11facc764391', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='faf61417-cfe6-4fdb-ba80-a46a605723a8', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='40e41644-f6b8-466e-90ca-cc96402f750f', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='1e42a8d7-0c23-4e0e-9110-b3643e1d1248', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='b0314e57-f148-4079-a0ad-2095d844760c', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='af665a31-4a8f-4658-9047-bdf995ea5a82', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='459ceeda-6ae3-4fd8-80f2-17b309916b98', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='fd5b31e7-61e1-47d1-b1b0-c9b666adc16e', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='e6fc624b-10c5-49da-aeca-92c6396c04a6', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='76f61557-a731-424f-a912-ebf1cbe0623f', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='46422b87-38d6-4a1a-b136-86336b87fd07', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='5512c293-2cfc-4767-855a-fadb879b3108', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='48beaa88-3246-44e4-9131-36175f86b59c', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='880a66eb-4a99-42d9-ae5b-c80c58c79075', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='4f0f9e99-21ef-4bf9-9cd5-88308cb15b12', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='214bfaaa-dc1a-4bdb-a363-937ceaac08b9', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='6e967c1f-c445-484f-b922-0a1d3f3bfce1', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='c1ee190b-4bf5-47bf-b425-e179e534c867', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='c5db5fb5-99b4-4327-b292-2ff2f2498059', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='6b6686f5-b860-4a63-aec2-05a924107bd9', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='17e2a07d-b9d8-4b19-9b08-ace60c57b008', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='108be156-d991-453f-a4a0-2dcc079a9a10', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='fd92ce14-365d-4439-a906-2495ca5cce2a', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='0e963688-80c8-4b34-b537-5579881b040b', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='792e1307-93f1-448c-8eaf-d6304f7c87f8', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='848f4cc7-acb4-4503-a975-8bdbcec91702', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='a72af28b-eda0-4a38-b975-253fe6657992', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='2a9c917b-5951-4298-830c-d1a09c79ee91', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='62d59b21-737b-4206-870c-5043a4783435', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='5de77258-1044-4b73-9a43-618ea1ec2039', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='98e4b437-d69a-4c76-bb3b-5811d1b49573', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='75e0dd2f-b2a9-4431-afed-915003ae2db9', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='7d5aa803-47b3-4ce8-97f0-9f495c168998', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='c5adf6a2-c17a-448b-a8a7-32e248755e0f', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='45d3c27f-3cdb-4966-b6fb-1098f7eb0c0f', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='ab4480f7-c87a-4bed-8c3c-7e0f1a915bef', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='45f49f81-6a29-4f7a-8062-aa502c7d9657', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='9d68ac0d-e49e-4a6b-9efa-0070a72a75a0', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='51c51a36-6b41-4ff3-9262-d44e328404e8', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='54c5f667-7344-4367-8ba8-1450ab592984', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='4afd3648-1eb6-4dc1-b0f4-9773b6003ae3', node_type=None, metadata={}, hash=None)]}, text='10 Gan Pei, Jiangning Zhang, et al.\\nmator addresses the regression of deformations related\\nto 3D poses and expressions. The eye gaze estimator\\ncontrols eye movement in videos, providing finer details.\\nMetaPortrait [324] achieves accurate distortion field\\nprediction through dense facial keypoint matching and\\naccelerates model training based on meta-learning prin-\\nciples, delivering excellent results on limited datasets.\\n•Feature Decoupling. The latent feature decoupling\\nand driving methods [16 –18,155,300] aims to disentangle\\nfacial features in the latent space of the driving video, re-\\nplacing or mapping the corresponding latent information\\nto achieve high-fidelity facial reproduction under specific\\nconditions. HyperReenact [17] uses attribute decoupling,\\nemploying a hyper-network to refine source identity fea-\\ntures and modify facial poses. StyleMask [18] separates\\nfacial pose and expression from the identity information\\nof the source image by learning masks and blending\\ncorresponding channels in the pre-trained style space S\\nof StyleGAN2. HiDe-NeRF [155] employs a deformable\\nneural radiance field to represent a 3D scene, with a\\nlightweight deformation module explicitly decoupling\\nfacial pose and expression attributes.\\n•Self-supervised Learning. Self-supervised learning\\nemploys supervisory signals inferred from the intrinsic\\nstructure of the data, reducing the reliance on exter-\\nnal data labels [207,253,321,328]. Oorloff et al. [207]\\nemploys self-supervised methods to train an encoder,\\ndisentangling identity and facial attribute information\\nof portrait images within the pre-defined latent space\\nitself of a pre-trained StyleGAN2. Zhang et al. [328]\\nutilizes 3DMM to provide geometric guidance, employs\\npre-computed optical flow to guide motion field esti-\\nmation, and relies on pre-computed occlusion maps to\\nguide the perception and repair of occluded areas.\\n3.1.3 Talking Face Generation\\nIn this section, we review current methods from three\\nperspectives:audio/textdriven,multimodalconditioned,\\ndiffusion-based, and 3D-model Technologies. We also\\nsummarize them in Table 4.\\n•Audio/Text Driven. Methods aim to map and guide\\nlip and facial movements in generated videos by under-\\nstanding the semantic information from the driving\\nsource [230,259,332]. Early methods [32,58] perform\\npoorly in terms of generalization and training complex-\\nity. After training, the models struggled to generalize to\\nnew individuals, requiring extensive conversational data\\nfor training new characters. Researchers [33,217] pro-\\npose their solutions from various perspectives. However,\\nMost of these methods prioritize generating lip move-\\nments aligned with semantic information, overlooking\\nessential aspects like identity and style, such as headpose changes and movement control, which are crucial\\nin natural videos. To address this, MakeItTalk [357]\\ndecouples input audio information by predicting facial\\nlandmarks based on audio and obtaining semantic de-\\ntails on facial expressions and poses from audio sig-\\nnals. SadTalker [337] extracts 3D motion coefficients\\nfor constructing a 3DMM from audio and uses this to\\nmodulate a new 3D perceptual facial rendering for gen-\\nerating head poses in talking videos. Additionally, some\\nmethods [63,145,262,279] propose their improvement\\nmethods, and these will not be detailed one by one. In\\naddition, the emotional expression varies for different\\ntextsduringaconversation,andvividemotionsareanes-\\nsential part of real talking face videos [64,233]. Recently,\\nsome methods [82,250,322] extend their previous ap-\\nproaches by incorporating matching between the driving\\ninformation and corresponding emotions. EMMN [250]\\nestablishes an organic relationship between emotions\\nand lip movements by extracting emotion embeddings\\nfrom the audio signal, synthesizing overall facial expres-\\nsionsintalkingfacesratherthanfocusingsolelyonaudio\\nfor facial expression synthesis. AMIGO [322] employs\\na sequence-to-sequence cross-modal emotion landmark\\ngeneration network to generate vivid landmarks aided\\nby audio information, ensuring that lips and emotions\\nin the output image sequence are synchronized with the\\ninput audio. However, existing methods still lack effec-\\ntive control over the intensity of emotions. In addition,\\nTalkCLIP [178] introduces style parameters, expanding\\nthe style categories for text-guided talking video genera-\\ntion. Zhong et al. [348] propose a two-stage framework,\\nincorporating appearance priors during the generation\\nprocess to enhance the model’s ability to preserve at-\\ntributes of the target face. DR2 [326] explores practical\\nstrategies for reducing the training workload.\\n•Multimodal Conditioned. To generate more real-\\nistic talking videos, some methods [159,261,284,351]\\nintroduce additional modal information on top of audio-\\ndriven methods to guide facial pose and expression.\\nGC-AVT [159] generates realistic talking videos by inde-\\npendently controlling head pose, audio information, and\\nfacial expressions. This approach introduces an expres-\\nsion source video, providing emotional information dur-\\ning the speech and the pose source video. However, the\\nvideo quality falls below expectations, and it struggles\\nto handle complex background changes. Xu et al. [284]\\nintegrate text, image, and audio-emotional modalities\\ninto a unified space to complement emotional content\\nin textual information. Multimodal approaches have sig-\\nnificantly enhanced the vividness of generated videos,\\nbut there is still room for exploration of organically\\ncombining information driven by different sources and\\nmodalities.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6025535344308235), NodeWithScore(node=TextNode(id_='5aa2f93d-0a8c-4c32-bb6e-b5c89254388b', embedding=None, metadata={'doc_id': '10121698-98b4-4107-8cb3-7b0b7e8285c7', 'type': 'text_summary'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Text Summary: In this section, the current methods for talking face generation are reviewed from three perspectives: audio/text driven, multimodal conditioned, and diffusion-based, as well as 3D-model Technologies. The methods aim to map and guide lip and facial movements in generated videos by understanding semantic information from the driving source. Some methods prioritize generating lip movements aligned with semantic information, while others incorporate matching between the driving information and corresponding emotions. Multimodal approaches have significantly enhanced the vividness of generated videos, but there is still room for exploration of organically combining information driven by different sources and modalities.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5918122777481373), NodeWithScore(node=TextNode(id_='fa6c2ebd-9cfb-4812-b229-cecd380a4e36', embedding=None, metadata={'doc_id': '2f5335ad-5219-43a1-bb15-5281541842dc', 'type': 'text_summary'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Text Summary: In this section, the current methods for talking face generation are reviewed from three perspectives: audio/text driven, multimodal conditioned, and diffusion-based, as well as 3D-model Technologies. The methods aim to map and guide lip and facial movements in generated videos by understanding semantic information from the driving source. Some methods prioritize generating lip movements aligned with semantic information, while others incorporate matching between the driving information and corresponding emotions. Multimodal approaches have significantly enhanced the vividness of generated videos, but there is still room for exploration of organically combining information driven by different sources and modalities.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.591019749333408)], metadata={'e191431b-e323-4795-b7cd-c66750890fff': {'file_name': 'Deepfake.pdf', 'doc_id': '10121698-98b4-4107-8cb3-7b0b7e8285c7', 'is_document': True}, '5aa2f93d-0a8c-4c32-bb6e-b5c89254388b': {'doc_id': '10121698-98b4-4107-8cb3-7b0b7e8285c7', 'type': 'text_summary'}, 'fa6c2ebd-9cfb-4812-b229-cecd380a4e36': {'doc_id': '2f5335ad-5219-43a1-bb15-5281541842dc', 'type': 'text_summary'}})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Ragas","metadata":{}},{"cell_type":"code","source":"from ragas.testset.generator import TestsetGenerator\nimport random\n\n# Initialize a TestsetGenerator using its default settings.\n# TestsetGenerator is used for generating test datasets, typically for model evaluation or testing.\n# The 'from_default' method sets up the generator with default configurations.\ntestsetgenerator = TestsetGenerator.from_llama_index(\n    generator_llm=llm,\n    critic_llm=llm,\n    embeddings=embedding_model,\n)\n\n# Specify the sample size for the source documents.\n# This determines how many documents will be randomly selected from the source documents.\nsample_size = 6\n\n# Define the number of questions to be included in the test set.\n# This will set how many test cases or questions the test set will contain.\nnum_questions = 15\n\n# Generate a test dataset from a random sample of source documents.\n# 'random.sample' is used to randomly select a subset of documents from the source.\n# The test set is then generated based on these documents.\n# Parameters:\n#   random.sample(source_docs, sample_size): A randomly selected subset of source documents.\n#   test_size: The number of questions or test cases to generate in the test set.\ntestset = testsetgenerator.generate_with_llamaindex_docs(\n    random.sample(source_docs, sample_size),  # Randomly selected documents\n    test_size=num_questions,               # Number of questions in the test set \n)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:02:21.022914Z","iopub.execute_input":"2024-07-11T18:02:21.023264Z","iopub.status.idle":"2024-07-11T18:09:30.014929Z","shell.execute_reply.started":"2024-07-11T18:02:21.023236Z","shell.execute_reply":"2024-07-11T18:09:30.013959Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"embedding nodes:   0%|          | 0/14 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2574dab99ba4e7d8cc06fadc435b8ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60b51db203af45d6a1871553dfd1806b"}},"metadata":{}}]},{"cell_type":"code","source":"import re\n\ntest_df = testset.to_pandas()\n# Define the regex pattern to match any character that is NOT a letter, a number, '.', ',', or '?'\npattern = r\"[^a-zA-Z0-9.,? ]\"\n\n# Define a function to replace special characters in a string\ndef remove_special_chars(s):\n    return re.sub(pattern, '', str(s))\n\n# Apply the function to each cell in the DataFrame\ntest_df = test_df.applymap(remove_special_chars)\n\n\ntest_questions = test_df['question'].values.tolist()\ntest_answers = [[item] for item in test_df['ground_truth'].values.tolist()]\n\ntest_df","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:09:58.039281Z","iopub.execute_input":"2024-07-11T18:09:58.039690Z","iopub.status.idle":"2024-07-11T18:09:58.157553Z","shell.execute_reply.started":"2024-07-11T18:09:58.039658Z","shell.execute_reply":"2024-07-11T18:09:58.156324Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_532/2846673957.py:12: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  test_df = test_df.applymap(remove_special_chars)\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                             question  \\\n0   How does the modified code correctly calculate...   \n1   How does the modified code correctly handle th...   \n2   What are the average scores of the proprietary...   \n3   How does the text normalization process impact...   \n4   What is the approach used to train the chat mo...   \n5   How can the punchline be revealed upon button ...   \n6   How does the use of multilingual and multitask...   \n7   What changes were made in the modified code to...   \n8   Whats the purpose of the queue in the code and...   \n9   What are the average scores of the proprietary...   \n10  Whats the purpose of the queue in the code and...   \n11  What is the return value and calculation proce...   \n\n                                             contexts  \\\n0   Qwen14BChat RLHF nnnnnndefmaxDepthself, root  ...   \n1   Qwen14BChat RLHF nnnnnndefmaxDepthself, root  ...   \n2   Table 13 Results on MMLU . All are tested with...   \n3   Robust Speech Recognition via LargeScale Weak ...   \n4   QWEN TECHNICAL REPORTnJinze Bai, Shuai Bai, Yu...   \n5   DOCTYPE htmlnhtmlnheadntitleMy Joke Websitetit...   \n6   Robust Speech Recognition via LargeScale Weak ...   \n7   Qwen14BChat RLHF nnnnnndefmaxDepthself, root  ...   \n8   def maxDepth  s e l f , r o o t  TreeNode   i ...   \n9   Table 13 Results on MMLU . All are tested with...   \n10  def maxDepth  s e l f , r o o t  TreeNode   i ...   \n11  def maxDepth  s e l f , r o o t  TreeNode   i ...   \n\n                                         ground_truth evolution_type  \\\n0   The modified code correctly calculates the max...         simple   \n1   In this code, when each node is extracted, we ...         simple   \n2   The answer to given question is not present in...         simple   \n3   The text normalization process impacts the per...         simple   \n4   The approach used to train the chat models in ...         simple   \n5   The answer to given question is not present in...         simple   \n6   Multilingual and multitask models benefit more...         simple   \n7   In the modified code, when each node is extrac...         simple   \n8   Your code is correct, the answer is correct. I...      reasoning   \n9   The answer to given question is not present in...      reasoning   \n10  Your code is correct, the answer is correct. I...      reasoning   \n11  Your code is correct, the answer is correct. I...      reasoning   \n\n                                             metadata episode_done  \n0   pagelabel 56, filename Qwen.pdf, filepath kagg...         True  \n1   pagelabel 56, filename Qwen.pdf, filepath kagg...         True  \n2   pagelabel 37, filename Qwen.pdf, filepath kagg...         True  \n3   pagelabel 12, filename Wisper.pdf, filepath ka...         True  \n4   pagelabel 1, filename Qwen.pdf, filepath kaggl...         True  \n5   pagelabel 16, filename VisionInstruction.pdf, ...         True  \n6   pagelabel 12, filename Wisper.pdf, filepath ka...         True  \n7   pagelabel 56, filename Qwen.pdf, filepath kagg...         True  \n8   pagelabel 55, filename Qwen.pdf, filepath kagg...         True  \n9   pagelabel 37, filename Qwen.pdf, filepath kagg...         True  \n10  pagelabel 55, filename Qwen.pdf, filepath kagg...         True  \n11  pagelabel 55, filename Qwen.pdf, filepath kagg...         True  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>contexts</th>\n      <th>ground_truth</th>\n      <th>evolution_type</th>\n      <th>metadata</th>\n      <th>episode_done</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>How does the modified code correctly calculate...</td>\n      <td>Qwen14BChat RLHF nnnnnndefmaxDepthself, root  ...</td>\n      <td>The modified code correctly calculates the max...</td>\n      <td>simple</td>\n      <td>pagelabel 56, filename Qwen.pdf, filepath kagg...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>How does the modified code correctly handle th...</td>\n      <td>Qwen14BChat RLHF nnnnnndefmaxDepthself, root  ...</td>\n      <td>In this code, when each node is extracted, we ...</td>\n      <td>simple</td>\n      <td>pagelabel 56, filename Qwen.pdf, filepath kagg...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>What are the average scores of the proprietary...</td>\n      <td>Table 13 Results on MMLU . All are tested with...</td>\n      <td>The answer to given question is not present in...</td>\n      <td>simple</td>\n      <td>pagelabel 37, filename Qwen.pdf, filepath kagg...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>How does the text normalization process impact...</td>\n      <td>Robust Speech Recognition via LargeScale Weak ...</td>\n      <td>The text normalization process impacts the per...</td>\n      <td>simple</td>\n      <td>pagelabel 12, filename Wisper.pdf, filepath ka...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>What is the approach used to train the chat mo...</td>\n      <td>QWEN TECHNICAL REPORTnJinze Bai, Shuai Bai, Yu...</td>\n      <td>The approach used to train the chat models in ...</td>\n      <td>simple</td>\n      <td>pagelabel 1, filename Qwen.pdf, filepath kaggl...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>How can the punchline be revealed upon button ...</td>\n      <td>DOCTYPE htmlnhtmlnheadntitleMy Joke Websitetit...</td>\n      <td>The answer to given question is not present in...</td>\n      <td>simple</td>\n      <td>pagelabel 16, filename VisionInstruction.pdf, ...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>How does the use of multilingual and multitask...</td>\n      <td>Robust Speech Recognition via LargeScale Weak ...</td>\n      <td>Multilingual and multitask models benefit more...</td>\n      <td>simple</td>\n      <td>pagelabel 12, filename Wisper.pdf, filepath ka...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>What changes were made in the modified code to...</td>\n      <td>Qwen14BChat RLHF nnnnnndefmaxDepthself, root  ...</td>\n      <td>In the modified code, when each node is extrac...</td>\n      <td>simple</td>\n      <td>pagelabel 56, filename Qwen.pdf, filepath kagg...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Whats the purpose of the queue in the code and...</td>\n      <td>def maxDepth  s e l f , r o o t  TreeNode   i ...</td>\n      <td>Your code is correct, the answer is correct. I...</td>\n      <td>reasoning</td>\n      <td>pagelabel 55, filename Qwen.pdf, filepath kagg...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>What are the average scores of the proprietary...</td>\n      <td>Table 13 Results on MMLU . All are tested with...</td>\n      <td>The answer to given question is not present in...</td>\n      <td>reasoning</td>\n      <td>pagelabel 37, filename Qwen.pdf, filepath kagg...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Whats the purpose of the queue in the code and...</td>\n      <td>def maxDepth  s e l f , r o o t  TreeNode   i ...</td>\n      <td>Your code is correct, the answer is correct. I...</td>\n      <td>reasoning</td>\n      <td>pagelabel 55, filename Qwen.pdf, filepath kagg...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>What is the return value and calculation proce...</td>\n      <td>def maxDepth  s e l f , r o o t  TreeNode   i ...</td>\n      <td>Your code is correct, the answer is correct. I...</td>\n      <td>reasoning</td>\n      <td>pagelabel 55, filename Qwen.pdf, filepath kagg...</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_df.to_csv(\"test_dataset.csv\", index=False, encoding='utf-8')","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:10:47.085213Z","iopub.execute_input":"2024-07-11T18:10:47.085582Z","iopub.status.idle":"2024-07-11T18:10:47.171839Z","shell.execute_reply.started":"2024-07-11T18:10:47.085548Z","shell.execute_reply":"2024-07-11T18:10:47.171053Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import asyncio\nfrom ragas.metrics import (\n    faithfulness,\n    answer_relevancy,\n    context_precision,\n    context_recall,\n    answer_similarity,\n    answer_correctness\n)\nfrom tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\nfrom ragas.integrations.llama_index import evaluate\nimport pandas as pd\nimport time\nimport httpx \n\n# List of evaluation metrics functions to be used.\nmetrics = [\n    faithfulness,           # Evaluates faithfulness of the response to the source material.\n    answer_relevancy,       # Assesses relevance of the response to the query.\n    context_precision,      # Measures precision of the context in the response.\n    context_recall,         # Measures recall of the context in the response.\n    answer_correctness,     # Checks correctness of the answer.\n    answer_similarity,      # Evaluates similarity of the answer to a reference answer.\n]\n\n# A list to collect individual result DataFrames.\nresults_list = []","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:31:31.143403Z","iopub.execute_input":"2024-07-11T18:31:31.144138Z","iopub.status.idle":"2024-07-11T18:31:31.150627Z","shell.execute_reply.started":"2024-07-11T18:31:31.144105Z","shell.execute_reply":"2024-07-11T18:31:31.149656Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"@retry(stop=stop_after_attempt(5), wait=wait_exponential(min=1, max=60), retry=retry_if_exception_type(httpx.HTTPStatusError))\ndef safe_evaluate(query_engine, metrics, dataset, llm, embeddings):\n    return evaluate(query_engine=query_engine, metrics=metrics, dataset=dataset, llm=llm, embeddings=embeddings,raise_exceptions=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:31:41.633507Z","iopub.execute_input":"2024-07-11T18:31:41.634367Z","iopub.status.idle":"2024-07-11T18:31:41.639757Z","shell.execute_reply.started":"2024-07-11T18:31:41.634334Z","shell.execute_reply":"2024-07-11T18:31:41.638760Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def evaluate_and_append(query_engine, technique):\n    # Evaluate the query engine.\n    result = safe_evaluate(query_engine=query_engine, metrics=metrics, dataset=test_df, llm=llm, embeddings=embedding_model)\n    # Add a 'technique' column to the result DataFrame.\n    result['technique'] = technique\n\n    # Add the result DataFrame to the results list.\n    results_list.append(result)\n\n    # Sleep to handle rate limits.\n    # time.sleep(60)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:31:44.624712Z","iopub.execute_input":"2024-07-11T18:31:44.625312Z","iopub.status.idle":"2024-07-11T18:31:44.630579Z","shell.execute_reply.started":"2024-07-11T18:31:44.625282Z","shell.execute_reply":"2024-07-11T18:31:44.629599Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"evaluate_and_append(baseline_query_engine, 'chunks_with_overlap')","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:31:45.364388Z","iopub.execute_input":"2024-07-11T18:31:45.365243Z","iopub.status.idle":"2024-07-11T18:34:02.974788Z","shell.execute_reply.started":"2024-07-11T18:31:45.365210Z","shell.execute_reply":"2024-07-11T18:34:02.974041Z"},"trusted":true},"execution_count":41,"outputs":[{"output_type":"display_data","data":{"text/plain":"Running Query Engine:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3d25c826e9f4c58bd611439c2ba07f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/72 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"169e842385254fda8072f178e52c8e8d"}},"metadata":{}}]},{"cell_type":"code","source":"# Convert each Result object's items to a dictionary and collect them in a list\ndict_list = [dict(result.items()) for result in results_list]\n\n# Convert the list of dictionaries to a DataFrame\nresults_df = pd.DataFrame(dict_list)\n\nresults_df","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:34:02.976032Z","iopub.execute_input":"2024-07-11T18:34:02.976305Z","iopub.status.idle":"2024-07-11T18:34:02.990396Z","shell.execute_reply.started":"2024-07-11T18:34:02.976279Z","shell.execute_reply":"2024-07-11T18:34:02.989488Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"   faithfulness  answer_relevancy  context_precision  context_recall  \\\n0      0.949074          0.821416               0.75        0.741667   \n\n   answer_correctness  answer_similarity            technique  \n0            0.559331           0.625103  chunks_with_overlap  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>faithfulness</th>\n      <th>answer_relevancy</th>\n      <th>context_precision</th>\n      <th>context_recall</th>\n      <th>answer_correctness</th>\n      <th>answer_similarity</th>\n      <th>technique</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.949074</td>\n      <td>0.821416</td>\n      <td>0.75</td>\n      <td>0.741667</td>\n      <td>0.559331</td>\n      <td>0.625103</td>\n      <td>chunks_with_overlap</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}