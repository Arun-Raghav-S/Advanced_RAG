{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8931048,"sourceType":"datasetVersion","datasetId":5372674},{"sourceId":8932176,"sourceType":"datasetVersion","datasetId":5373505},{"sourceId":8932978,"sourceType":"datasetVersion","datasetId":5374080},{"sourceId":8933076,"sourceType":"datasetVersion","datasetId":5374160}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nsecret_value = UserSecretsClient().get_secret(\"PAT_KEY\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://{secret_value}@github.com/Arun-Raghav-S/Advanced_RAG.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd Advanced_RAG","metadata":{"execution":{"iopub.status.busy":"2024-07-12T00:47:42.183096Z","iopub.execute_input":"2024-07-12T00:47:42.183362Z","iopub.status.idle":"2024-07-12T00:47:42.189274Z","shell.execute_reply.started":"2024-07-12T00:47:42.183337Z","shell.execute_reply":"2024-07-12T00:47:42.188080Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/working/Advanced_RAG\n","output_type":"stream"}]},{"cell_type":"code","source":"%pip install -r requirements.txt ","metadata":{"execution":{"iopub.status.busy":"2024-07-12T00:47:49.943948Z","iopub.execute_input":"2024-07-12T00:47:49.944815Z","iopub.status.idle":"2024-07-12T00:52:22.586631Z","shell.execute_reply.started":"2024-07-12T00:47:49.944783Z","shell.execute_reply":"2024-07-12T00:52:22.585369Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting llama-index==0.10.51 (from -r requirements.txt (line 1))\n  Downloading llama_index-0.10.51-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: pypdf<5.0.0,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (4.2.0)\nCollecting sentence-transformers==2.2.2 (from -r requirements.txt (line 3))\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (4.9.0)\nRequirement already satisfied: nest_asyncio==1.5.8 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (1.5.8)\nCollecting ragas (from -r requirements.txt (line 6))\n  Downloading ragas-0.1.10-py3-none-any.whl.metadata (5.2 kB)\nCollecting pyarrow==14.0.1 (from -r requirements.txt (line 7))\n  Downloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\nCollecting torch==2.3.0 (from -r requirements.txt (line 8))\n  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nCollecting requests==2.31.0 (from -r requirements.txt (line 9))\n  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\nCollecting llama-index-llms-azure-openai (from -r requirements.txt (line 10))\n  Downloading llama_index_llms_azure_openai-0.1.8-py3-none-any.whl.metadata (735 bytes)\nCollecting llama-index-embeddings-azure-openai (from -r requirements.txt (line 11))\n  Downloading llama_index_embeddings_azure_openai-0.1.10-py3-none-any.whl.metadata (753 bytes)\nCollecting llama-index-core (from -r requirements.txt (line 12))\n  Downloading llama_index_core-0.10.54.post1-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: python-dotenv in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (1.0.0)\nCollecting rouge (from -r requirements.txt (line 14))\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\nCollecting bert-score (from -r requirements.txt (line 15))\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (3.7.5)\nRequirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (0.12.2)\nCollecting pdfplumber (from -r requirements.txt (line 18))\n  Downloading pdfplumber-0.11.2-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pytesseract in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (0.3.10)\nCollecting chromadb (from -r requirements.txt (line 22))\n  Downloading chromadb-0.5.4-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 23)) (4.41.2)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 24)) (9.5.0)\nCollecting llm-lens (from -r requirements.txt (line 25))\n  Downloading llm_lens-0.0.0.3-py3-none-any.whl.metadata (968 bytes)\nCollecting transformers (from -r requirements.txt (line 23))\n  Downloading transformers-4.39.2-py3-none-any.whl.metadata (134 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 27)) (0.30.1)\nCollecting einops (from -r requirements.txt (line 28))\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nCollecting poppler-utils (from -r requirements.txt (line 29))\n  Downloading poppler_utils-0.1.0-py3-none-any.whl.metadata (883 bytes)\nCollecting tesseract (from -r requirements.txt (line 30))\n  Downloading tesseract-0.1.3.tar.gz (45.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting unstructured[all-docs] (from -r requirements.txt (line 21))\n  Downloading unstructured-0.14.10-py3-none-any.whl.metadata (29 kB)\nCollecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index==0.10.51->-r requirements.txt (line 1))\n  Downloading llama_index_agent_openai-0.2.8-py3-none-any.whl.metadata (729 bytes)\nCollecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index==0.10.51->-r requirements.txt (line 1))\n  Downloading llama_index_cli-0.1.12-py3-none-any.whl.metadata (1.5 kB)\nCollecting llama-index-core (from -r requirements.txt (line 12))\n  Downloading llama_index_core-0.10.51-py3-none-any.whl.metadata (2.4 kB)\nCollecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index==0.10.51->-r requirements.txt (line 1))\n  Downloading llama_index_embeddings_openai-0.1.10-py3-none-any.whl.metadata (604 bytes)\nCollecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama-index==0.10.51->-r requirements.txt (line 1))\n  Downloading llama_index_indices_managed_llama_cloud-0.2.5-py3-none-any.whl.metadata (3.8 kB)\nCollecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index==0.10.51->-r requirements.txt (line 1))\n  Downloading llama_index_legacy-0.9.48-py3-none-any.whl.metadata (8.5 kB)\nCollecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama-index==0.10.51->-r requirements.txt (line 1))\n  Downloading llama_index_llms_openai-0.1.25-py3-none-any.whl.metadata (610 bytes)\nCollecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index==0.10.51->-r requirements.txt (line 1))\n  Downloading llama_index_multi_modal_llms_openai-0.1.7-py3-none-any.whl.metadata (728 bytes)\nCollecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index==0.10.51->-r requirements.txt (line 1))\n  Downloading llama_index_program_openai-0.1.6-py3-none-any.whl.metadata (715 bytes)\nCollecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index==0.10.51->-r requirements.txt (line 1))\n  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\nCollecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index==0.10.51->-r requirements.txt (line 1))\n  Downloading llama_index_readers_file-0.1.30-py3-none-any.whl.metadata (5.4 kB)\nCollecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama-index==0.10.51->-r requirements.txt (line 1))\n  Downloading llama_index_readers_llama_parse-0.1.6-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 3)) (4.66.4)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 3)) (0.16.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 3)) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 3)) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 3)) (1.11.4)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 3)) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 3)) (0.2.0)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 3)) (0.23.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->-r requirements.txt (line 8)) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->-r requirements.txt (line 8)) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->-r requirements.txt (line 8)) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->-r requirements.txt (line 8)) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->-r requirements.txt (line 8)) (2024.3.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0->-r requirements.txt (line 8))\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0->-r requirements.txt (line 8))\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0->-r requirements.txt (line 8))\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0->-r requirements.txt (line 8))\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0->-r requirements.txt (line 8))\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0->-r requirements.txt (line 8))\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0->-r requirements.txt (line 8))\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0->-r requirements.txt (line 8))\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0->-r requirements.txt (line 8))\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0->-r requirements.txt (line 8))\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0->-r requirements.txt (line 8))\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.3.0 (from torch==2.3.0->-r requirements.txt (line 8))\n  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests==2.31.0->-r requirements.txt (line 9)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests==2.31.0->-r requirements.txt (line 9)) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests==2.31.0->-r requirements.txt (line 9)) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests==2.31.0->-r requirements.txt (line 9)) (2024.2.2)\nRequirement already satisfied: PyYAML>=6.0.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core->-r requirements.txt (line 12)) (6.0.1)\nRequirement already satisfied: SQLAlchemy>=1.4.49 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core->-r requirements.txt (line 12)) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/conda/lib/python3.10/site-packages (from llama-index-core->-r requirements.txt (line 12)) (3.9.1)\nRequirement already satisfied: dataclasses-json in /opt/conda/lib/python3.10/site-packages (from llama-index-core->-r requirements.txt (line 12)) (0.6.6)\nRequirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core->-r requirements.txt (line 12)) (1.2.14)\nCollecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core->-r requirements.txt (line 12))\n  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: httpx in /opt/conda/lib/python3.10/site-packages (from llama-index-core->-r requirements.txt (line 12)) (0.27.0)\nCollecting llama-cloud<0.0.7,>=0.0.6 (from llama-index-core->-r requirements.txt (line 12))\n  Downloading llama_cloud-0.0.6-py3-none-any.whl.metadata (750 bytes)\nCollecting nltk (from sentence-transformers==2.2.2->-r requirements.txt (line 3))\n  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\nCollecting openai>=1.1.0 (from llama-index-core->-r requirements.txt (line 12))\n  Downloading openai-1.35.13-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from llama-index-core->-r requirements.txt (line 12)) (2.2.1)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core->-r requirements.txt (line 12)) (8.2.3)\nCollecting tiktoken>=0.3.3 (from llama-index-core->-r requirements.txt (line 12))\n  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core->-r requirements.txt (line 12)) (0.9.0)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from llama-index-core->-r requirements.txt (line 12)) (1.14.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 23)) (21.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 23)) (2023.12.25)\nCollecting tokenizers<0.19,>=0.14 (from transformers->-r requirements.txt (line 23))\n  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 23)) (0.4.3)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->-r requirements.txt (line 8))\n  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from ragas->-r requirements.txt (line 6)) (2.19.2)\nCollecting langchain (from ragas->-r requirements.txt (line 6))\n  Downloading langchain-0.2.7-py3-none-any.whl.metadata (6.9 kB)\nCollecting langchain-core (from ragas->-r requirements.txt (line 6))\n  Downloading langchain_core-0.2.16-py3-none-any.whl.metadata (6.0 kB)\nCollecting langchain-community (from ragas->-r requirements.txt (line 6))\n  Downloading langchain_community-0.2.7-py3-none-any.whl.metadata (2.5 kB)\nCollecting langchain-openai (from ragas->-r requirements.txt (line 6))\n  Downloading langchain_openai-0.1.15-py3-none-any.whl.metadata (2.5 kB)\nCollecting pysbd>=0.3.4 (from ragas->-r requirements.txt (line 6))\n  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: appdirs in /opt/conda/lib/python3.10/site-packages (from ragas->-r requirements.txt (line 6)) (1.4.4)\nCollecting azure-identity<2.0.0,>=1.15.0 (from llama-index-llms-azure-openai->-r requirements.txt (line 10))\n  Downloading azure_identity-1.17.1-py3-none-any.whl.metadata (79 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.4/79.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge->-r requirements.txt (line 14)) (1.16.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 16)) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 16)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 16)) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 16)) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 16)) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 16)) (2.9.0.post0)\nCollecting pdfminer.six==20231228 (from pdfplumber->-r requirements.txt (line 18))\n  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\nCollecting pypdfium2>=4.18.0 (from pdfplumber->-r requirements.txt (line 18))\n  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: cryptography>=36.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber->-r requirements.txt (line 18)) (41.0.7)\nCollecting chardet (from unstructured[all-docs]->-r requirements.txt (line 21))\n  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\nCollecting filetype (from unstructured[all-docs]->-r requirements.txt (line 21))\n  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\nCollecting python-magic (from unstructured[all-docs]->-r requirements.txt (line 21))\n  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from unstructured[all-docs]->-r requirements.txt (line 21)) (5.2.2)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from unstructured[all-docs]->-r requirements.txt (line 21)) (0.9.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from unstructured[all-docs]->-r requirements.txt (line 21)) (4.12.2)\nRequirement already satisfied: emoji in /opt/conda/lib/python3.10/site-packages (from unstructured[all-docs]->-r requirements.txt (line 21)) (2.12.1)\nCollecting python-iso639 (from unstructured[all-docs]->-r requirements.txt (line 21))\n  Downloading python_iso639-2024.4.27-py3-none-any.whl.metadata (13 kB)\nCollecting langdetect (from unstructured[all-docs]->-r requirements.txt (line 21))\n  Downloading langdetect-1.0.9.tar.gz (981 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting rapidfuzz (from unstructured[all-docs]->-r requirements.txt (line 21))\n  Downloading rapidfuzz-3.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nRequirement already satisfied: backoff in /opt/conda/lib/python3.10/site-packages (from unstructured[all-docs]->-r requirements.txt (line 21)) (2.2.1)\nCollecting unstructured-client (from unstructured[all-docs]->-r requirements.txt (line 21))\n  Downloading unstructured_client-0.24.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from unstructured[all-docs]->-r requirements.txt (line 21)) (5.9.3)\nCollecting pillow-heif (from unstructured[all-docs]->-r requirements.txt (line 21))\n  Downloading pillow_heif-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\nCollecting pypandoc (from unstructured[all-docs]->-r requirements.txt (line 21))\n  Downloading pypandoc-1.13-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: pdf2image in /opt/conda/lib/python3.10/site-packages (from unstructured[all-docs]->-r requirements.txt (line 21)) (1.17.0)\nCollecting unstructured.pytesseract>=0.3.12 (from unstructured[all-docs]->-r requirements.txt (line 21))\n  Downloading unstructured.pytesseract-0.3.12-py3-none-any.whl.metadata (11 kB)\nCollecting python-oxmsg (from unstructured[all-docs]->-r requirements.txt (line 21))\n  Downloading python_oxmsg-0.0.1-py3-none-any.whl.metadata (5.0 kB)\nCollecting pikepdf (from unstructured[all-docs]->-r requirements.txt (line 21))\n  Downloading pikepdf-9.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\nCollecting effdet (from unstructured[all-docs]->-r requirements.txt (line 21))\n  Downloading effdet-0.4.1-py3-none-any.whl.metadata (33 kB)\nRequirement already satisfied: google-cloud-vision in /opt/conda/lib/python3.10/site-packages (from unstructured[all-docs]->-r requirements.txt (line 21)) (2.8.0)\nRequirement already satisfied: openpyxl in /opt/conda/lib/python3.10/site-packages (from unstructured[all-docs]->-r requirements.txt (line 21)) (3.1.3)\nCollecting unstructured-inference==0.7.36 (from unstructured[all-docs]->-r requirements.txt (line 21))\n  Downloading unstructured_inference-0.7.36-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: onnx in /opt/conda/lib/python3.10/site-packages (from unstructured[all-docs]->-r requirements.txt (line 21)) (1.16.1)\nRequirement already satisfied: markdown in /opt/conda/lib/python3.10/site-packages (from unstructured[all-docs]->-r requirements.txt (line 21)) (3.5.2)\nCollecting python-docx>=1.1.2 (from unstructured[all-docs]->-r requirements.txt (line 21))\n  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\nCollecting python-pptx<=0.6.23 (from unstructured[all-docs]->-r requirements.txt (line 21))\n  Downloading python_pptx-0.6.23-py3-none-any.whl.metadata (18 kB)\nCollecting xlrd (from unstructured[all-docs]->-r requirements.txt (line 21))\n  Downloading xlrd-2.0.1-py2.py3-none-any.whl.metadata (3.4 kB)\nCollecting layoutparser (from unstructured-inference==0.7.36->unstructured[all-docs]->-r requirements.txt (line 21))\n  Downloading layoutparser-0.3.4-py3-none-any.whl.metadata (7.7 kB)\nCollecting python-multipart (from unstructured-inference==0.7.36->unstructured[all-docs]->-r requirements.txt (line 21))\n  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: opencv-python!=4.7.0.68 in /opt/conda/lib/python3.10/site-packages (from unstructured-inference==0.7.36->unstructured[all-docs]->-r requirements.txt (line 21)) (4.10.0.82)\nCollecting onnxruntime>=1.17.0 (from unstructured-inference==0.7.36->unstructured[all-docs]->-r requirements.txt (line 21))\n  Downloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\nRequirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (from unstructured-inference==0.7.36->unstructured[all-docs]->-r requirements.txt (line 21)) (1.0.3)\nCollecting build>=1.0.3 (from chromadb->-r requirements.txt (line 22))\n  Downloading build-1.2.1-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: pydantic>=1.9 in /opt/conda/lib/python3.10/site-packages (from chromadb->-r requirements.txt (line 22)) (2.5.3)\nCollecting chroma-hnswlib==0.7.5 (from chromadb->-r requirements.txt (line 22))\n  Downloading chroma_hnswlib-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\nRequirement already satisfied: fastapi>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from chromadb->-r requirements.txt (line 22)) (0.108.0)\nRequirement already satisfied: uvicorn>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 22)) (0.25.0)\nCollecting posthog>=2.4.0 (from chromadb->-r requirements.txt (line 22))\n  Downloading posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb->-r requirements.txt (line 22)) (1.22.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb->-r requirements.txt (line 22)) (1.22.0)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb->-r requirements.txt (line 22))\n  Downloading opentelemetry_instrumentation_fastapi-0.46b0-py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb->-r requirements.txt (line 22)) (1.22.0)\nCollecting pypika>=0.48.9 (from chromadb->-r requirements.txt (line 22))\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.10/site-packages (from chromadb->-r requirements.txt (line 22)) (7.4.0)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb->-r requirements.txt (line 22)) (6.1.1)\nRequirement already satisfied: grpcio>=1.58.0 in /opt/conda/lib/python3.10/site-packages (from chromadb->-r requirements.txt (line 22)) (1.59.3)\nCollecting bcrypt>=4.0.1 (from chromadb->-r requirements.txt (line 22))\n  Downloading bcrypt-4.1.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\nRequirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chromadb->-r requirements.txt (line 22)) (0.9.0)\nCollecting kubernetes>=28.1.0 (from chromadb->-r requirements.txt (line 22))\n  Downloading kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting mmh3>=4.0.1 (from chromadb->-r requirements.txt (line 22))\n  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nCollecting orjson>=3.9.12 (from chromadb->-r requirements.txt (line 22))\n  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting evaluate (from llm-lens->-r requirements.txt (line 25))\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nCollecting bitsandbytes (from llm-lens->-r requirements.txt (line 25))\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nCollecting ftfy (from llm-lens->-r requirements.txt (line 25))\n  Downloading ftfy-6.2.0-py3-none-any.whl.metadata (7.3 kB)\nCollecting omegaconf (from llm-lens->-r requirements.txt (line 25))\n  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\nCollecting open-clip-torch (from llm-lens->-r requirements.txt (line 25))\n  Downloading open_clip_torch-2.26.1-py3-none-any.whl.metadata (31 kB)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (from llm-lens->-r requirements.txt (line 25)) (0.17.0)\nCollecting fairscale (from llm-lens->-r requirements.txt (line 25))\n  Downloading fairscale-0.4.13.tar.gz (266 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: Click>=7.0 in /opt/conda/lib/python3.10/site-packages (from poppler-utils->-r requirements.txt (line 29)) (8.1.7)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core->-r requirements.txt (line 12)) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core->-r requirements.txt (line 12)) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core->-r requirements.txt (line 12)) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core->-r requirements.txt (line 12)) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core->-r requirements.txt (line 12)) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core->-r requirements.txt (line 12)) (4.0.3)\nCollecting azure-core>=1.23.0 (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai->-r requirements.txt (line 10))\n  Downloading azure_core-1.30.2-py3-none-any.whl.metadata (37 kB)\nCollecting msal>=1.24.0 (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai->-r requirements.txt (line 10))\n  Downloading msal-1.29.0-py3-none-any.whl.metadata (11 kB)\nCollecting msal-extensions>=0.3.0 (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai->-r requirements.txt (line 10))\n  Downloading msal_extensions-1.2.0-py3-none-any.whl.metadata (7.6 kB)\nCollecting pyproject_hooks (from build>=1.0.3->chromadb->-r requirements.txt (line 22))\n  Downloading pyproject_hooks-1.1.0-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb->-r requirements.txt (line 22)) (2.0.1)\nRequirement already satisfied: starlette<0.33.0,>=0.29.0 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb->-r requirements.txt (line 22)) (0.32.0.post1)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core->-r requirements.txt (line 12)) (4.2.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core->-r requirements.txt (line 12)) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->llama-index-core->-r requirements.txt (line 12)) (1.3.0)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core->-r requirements.txt (line 12)) (0.14.0)\nRequirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 22)) (2.26.1)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 22)) (1.7.0)\nRequirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 22)) (1.3.1)\nRequirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 22)) (3.2.2)\nINFO: pip is looking at multiple versions of llama-index-indices-managed-llama-cloud to determine which version is compatible with other requirements. This could take a while.\nCollecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama-index==0.10.51->-r requirements.txt (line 1))\n  Downloading llama_index_indices_managed_llama_cloud-0.2.4-py3-none-any.whl.metadata (3.8 kB)\nCollecting beautifulsoup4 (from unstructured[all-docs]->-r requirements.txt (line 21))\n  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\nCollecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.51->-r requirements.txt (line 1))\n  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->unstructured[all-docs]->-r requirements.txt (line 21)) (2.5)\nCollecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index==0.10.51->-r requirements.txt (line 1))\n  Downloading llama_parse-0.4.6-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers==2.2.2->-r requirements.txt (line 3)) (1.4.2)\nCollecting coloredlogs (from onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs]->-r requirements.txt (line 21))\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs]->-r requirements.txt (line 21)) (23.5.26)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs]->-r requirements.txt (line 21)) (3.20.3)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-core->-r requirements.txt (line 12)) (1.9.0)\nRequirement already satisfied: importlib-metadata<7.0,>=6.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 22)) (6.11.0)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 22)) (1.62.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.22.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 22)) (1.22.0)\nRequirement already satisfied: opentelemetry-proto==1.22.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 22)) (1.22.0)\nCollecting opentelemetry-instrumentation-asgi==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 22))\n  Downloading opentelemetry_instrumentation_asgi-0.46b0-py3-none-any.whl.metadata (1.9 kB)\nCollecting opentelemetry-instrumentation==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 22))\n  Downloading opentelemetry_instrumentation-0.46b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 22))\n  Downloading opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-util-http==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 22))\n  Downloading opentelemetry_util_http-0.46b0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: setuptools>=16.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 22)) (69.0.3)\nCollecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 22))\n  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\nCollecting opentelemetry-api>=1.2.0 (from chromadb->-r requirements.txt (line 22))\n  Downloading opentelemetry_api-1.25.0-py3-none-any.whl.metadata (1.4 kB)\nINFO: pip is looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb->-r requirements.txt (line 22))\n  Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl.metadata (2.0 kB)\nCollecting opentelemetry-instrumentation-asgi==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 22))\n  Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl.metadata (1.9 kB)\nCollecting opentelemetry-instrumentation==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 22))\n  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 22))\n  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl.metadata (2.2 kB)\nCollecting opentelemetry-util-http==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 22))\n  Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb->-r requirements.txt (line 22))\n  Downloading opentelemetry_instrumentation_fastapi-0.44b0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-instrumentation-asgi==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 22))\n  Downloading opentelemetry_instrumentation_asgi-0.44b0-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 22))\n  Downloading opentelemetry_instrumentation-0.44b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 22))\n  Downloading opentelemetry_semantic_conventions-0.44b0-py3-none-any.whl.metadata (2.2 kB)\nCollecting opentelemetry-util-http==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 22))\n  Downloading opentelemetry_util_http-0.44b0-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb->-r requirements.txt (line 22))\n  Downloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-instrumentation-asgi==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 22))\n  Downloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 22))\n  Downloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.43b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 22)) (0.43b0)\nCollecting opentelemetry-util-http==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 22))\n  Downloading opentelemetry_util_http-0.43b0-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-core->-r requirements.txt (line 12)) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-core->-r requirements.txt (line 12)) (2023.4)\nCollecting monotonic>=1.5 (from posthog>=2.4.0->chromadb->-r requirements.txt (line 22))\n  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9->chromadb->-r requirements.txt (line 22)) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9->chromadb->-r requirements.txt (line 22)) (2.14.6)\nCollecting XlsxWriter>=0.5.7 (from python-pptx<=0.6.23->unstructured[all-docs]->-r requirements.txt (line 21))\n  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core->-r requirements.txt (line 12)) (3.0.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core->-r requirements.txt (line 12)) (1.0.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 22)) (0.6.1)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 22)) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 22)) (0.21.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 22)) (12.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json->llama-index-core->-r requirements.txt (line 12)) (3.21.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->ragas->-r requirements.txt (line 6)) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->ragas->-r requirements.txt (line 6)) (0.3.8)\nINFO: pip is looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\nCollecting datasets (from ragas->-r requirements.txt (line 6))\n  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n  Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->ragas->-r requirements.txt (line 6)) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->ragas->-r requirements.txt (line 6)) (0.70.16)\nCollecting pycocotools>=2.0.2 (from effdet->unstructured[all-docs]->-r requirements.txt (line 21))\n  Downloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nCollecting antlr4-python3-runtime==4.9.* (from omegaconf->llm-lens->-r requirements.txt (line 25))\n  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->llm-lens->-r requirements.txt (line 25)) (0.2.13)\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision->unstructured[all-docs]->-r requirements.txt (line 21)) (2.11.1)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-vision->unstructured[all-docs]->-r requirements.txt (line 21)) (1.23.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.3.0->-r requirements.txt (line 8)) (2.1.3)\nCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain->ragas->-r requirements.txt (line 6))\n  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain->ragas->-r requirements.txt (line 6))\n  Downloading langsmith-0.1.85-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core->ragas->-r requirements.txt (line 6)) (1.33)\nCollecting packaging>=20.0 (from transformers->-r requirements.txt (line 23))\n  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: et-xmlfile in /opt/conda/lib/python3.10/site-packages (from openpyxl->unstructured[all-docs]->-r requirements.txt (line 21)) (1.1.0)\nCollecting pillow (from -r requirements.txt (line 24))\n  Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\nRequirement already satisfied: olefile in /opt/conda/lib/python3.10/site-packages (from python-oxmsg->unstructured[all-docs]->-r requirements.txt (line 21)) (0.47)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==2.2.2->-r requirements.txt (line 3)) (3.2.0)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.3.0->-r requirements.txt (line 8)) (1.3.0)\nRequirement already satisfied: deepdiff>=6.0 in /opt/conda/lib/python3.10/site-packages (from unstructured-client->unstructured[all-docs]->-r requirements.txt (line 21)) (7.0.1)\nCollecting jsonpath-python>=1.0.6 (from unstructured-client->unstructured[all-docs]->-r requirements.txt (line 21))\n  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\nINFO: pip is looking at multiple versions of unstructured-client to determine which version is compatible with other requirements. This could take a while.\nCollecting unstructured-client (from unstructured[all-docs]->-r requirements.txt (line 21))\n  Downloading unstructured_client-0.23.9-py3-none-any.whl.metadata (12 kB)\n  Downloading unstructured_client-0.23.8-py3-none-any.whl.metadata (12 kB)\n  Downloading unstructured_client-0.23.7-py3-none-any.whl.metadata (12 kB)\n  Downloading unstructured_client-0.23.5-py3-none-any.whl.metadata (12 kB)\n  Downloading unstructured_client-0.23.3-py3-none-any.whl.metadata (12 kB)\n  Downloading unstructured_client-0.23.2-py3-none-any.whl.metadata (12 kB)\n  Downloading unstructured_client-0.23.1-py3-none-any.whl.metadata (11 kB)\nINFO: pip is still looking at multiple versions of unstructured-client to determine which version is compatible with other requirements. This could take a while.\n  Downloading unstructured_client-0.23.0-py3-none-any.whl.metadata (11 kB)\n  Downloading unstructured_client-0.22.0-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb->llm-lens->-r requirements.txt (line 25)) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->llm-lens->-r requirements.txt (line 25)) (3.1.41)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb->llm-lens->-r requirements.txt (line 25)) (3.11.0)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->llm-lens->-r requirements.txt (line 25)) (2.3.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb->llm-lens->-r requirements.txt (line 25)) (1.3.3)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx->llama-index-core->-r requirements.txt (line 12)) (1.2.0)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber->-r requirements.txt (line 18)) (1.16.0)\nRequirement already satisfied: ordered-set<4.2.0,>=4.1.0 in /opt/conda/lib/python3.10/site-packages (from deepdiff>=6.0->unstructured-client->unstructured[all-docs]->-r requirements.txt (line 21)) (4.1.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->llm-lens->-r requirements.txt (line 25)) (4.0.11)\nRequirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision->unstructured[all-docs]->-r requirements.txt (line 21)) (1.48.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 22)) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 22)) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 22)) (4.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 22)) (3.17.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas->-r requirements.txt (line 6)) (2.4)\nRequirement already satisfied: PyJWT<3,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.24.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai->-r requirements.txt (line 10)) (2.8.0)\nCollecting portalocker<3,>=1.4 (from msal-extensions>=0.3.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai->-r requirements.txt (line 10))\n  Downloading portalocker-2.10.0-py3-none-any.whl.metadata (8.5 kB)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[all-docs]->-r requirements.txt (line 21))\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nCollecting iopath (from layoutparser->unstructured-inference==0.7.36->unstructured[all-docs]->-r requirements.txt (line 21))\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber->-r requirements.txt (line 18)) (2.21)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->llm-lens->-r requirements.txt (line 25)) (5.0.1)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 22)) (0.5.1)\nDownloading llama_index-0.10.51-py3-none-any.whl (6.8 kB)\nDownloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading llama_index_core-0.10.51-py3-none-any.whl (15.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.39.2-py3-none-any.whl (8.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ragas-0.1.10-py3-none-any.whl (91 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.5/91.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading llama_index_llms_azure_openai-0.1.8-py3-none-any.whl (4.9 kB)\nDownloading llama_index_embeddings_azure_openai-0.1.10-py3-none-any.whl (3.3 kB)\nDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pdfplumber-0.11.2-py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading unstructured_inference-0.7.36-py3-none-any.whl (56 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chromadb-0.5.4-py3-none-any.whl (581 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.4/581.4 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chroma_hnswlib-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading llm_lens-0.0.0.3-py3-none-any.whl (6.4 kB)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading poppler_utils-0.1.0-py3-none-any.whl (9.2 kB)\nDownloading azure_identity-1.17.1-py3-none-any.whl (173 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bcrypt-4.1.3-cp39-abi3-manylinux_2_28_x86_64.whl (283 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading build-1.2.1-py3-none-any.whl (21 kB)\nDownloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\nDownloading kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading llama_cloud-0.0.6-py3-none-any.whl (130 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading llama_index_agent_openai-0.2.8-py3-none-any.whl (13 kB)\nDownloading llama_index_cli-0.1.12-py3-none-any.whl (26 kB)\nDownloading llama_index_embeddings_openai-0.1.10-py3-none-any.whl (6.2 kB)\nDownloading llama_index_indices_managed_llama_cloud-0.2.4-py3-none-any.whl (9.2 kB)\nDownloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading llama_index_llms_openai-0.1.25-py3-none-any.whl (11 kB)\nDownloading llama_index_multi_modal_llms_openai-0.1.7-py3-none-any.whl (5.9 kB)\nDownloading llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\nDownloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\nDownloading llama_index_readers_file-0.1.30-py3-none-any.whl (38 kB)\nDownloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading llama_index_readers_llama_parse-0.1.6-py3-none-any.whl (2.5 kB)\nDownloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading openai-1.35.13-py3-none-any.whl (328 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.5/328.5 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl (11 kB)\nDownloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl (14 kB)\nDownloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl (28 kB)\nDownloading opentelemetry_util_http-0.43b0-py3-none-any.whl (6.9 kB)\nDownloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_pptx-0.6.23-py3-none-any.whl (471 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading unstructured.pytesseract-0.3.12-py3-none-any.whl (14 kB)\nDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading chardet-5.2.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading datasets-2.19.1-py3-none-any.whl (542 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading effdet-0.4.1-py3-none-any.whl (112 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\nDownloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain-0.2.7-py3-none-any.whl (983 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.6/983.6 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.2.16-py3-none-any.whl (362 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.4/362.4 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-24.1-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_community-0.2.7-py3-none-any.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_openai-0.1.15-py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading open_clip_torch-2.26.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pikepdf-9.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading pillow_heif-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pypandoc-1.13-py3-none-any.whl (21 kB)\nDownloading python_iso639-2024.4.27-py3-none-any.whl (274 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\nDownloading python_oxmsg-0.0.1-py3-none-any.whl (31 kB)\nDownloading rapidfuzz-3.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading unstructured-0.14.10-py3-none-any.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading unstructured_client-0.22.0-py3-none-any.whl (28 kB)\nDownloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading azure_core-1.30.2-py3-none-any.whl (194 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.3/194.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\nDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\nDownloading langsmith-0.1.85-py3-none-any.whl (127 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading llama_parse-0.4.6-py3-none-any.whl (9.1 kB)\nDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nDownloading msal-1.29.0-py3-none-any.whl (110 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.9/110.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading msal_extensions-1.2.0-py3-none-any.whl (19 kB)\nDownloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.8/427.8 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\nDownloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pyproject_hooks-1.1.0-py3-none-any.whl (9.2 kB)\nDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\nDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\nDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-2.10.0-py3-none-any.whl (18 kB)\nBuilding wheels for collected packages: sentence-transformers, tesseract, pypika, antlr4-python3-runtime, fairscale, langdetect, iopath\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=a30a48382594ea8bb8e62de16ec2be67c984115f9f5ce28150230c1d244c1872\n  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n  Building wheel for tesseract (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for tesseract: filename=tesseract-0.1.3-py3-none-any.whl size=45562552 sha256=643437220ccbf6bf6073de3ad92ceb3a630943d8d763400d244b373753b85448\n  Stored in directory: /root/.cache/pip/wheels/71/c9/aa/698c579693e83fdda9ad6d6f0d8f61ed986e27925ef576f109\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=3b022005569ca1d4af7fde2a3a70fc0920a9a50dcbd7c4b24460488cc07be7a3\n  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=dd8eeb1c60d8c26546209356fc08b50063a743d278980ac0eabc0c496528949f\n  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n  Building wheel for fairscale (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332104 sha256=444fb4e6ac3bd0a5871204904fba1d43fdd13d7c19473d0a0a348dd0cc8ffdce\n  Stored in directory: /root/.cache/pip/wheels/78/a4/c0/fb0a7ef03cff161611c3fa40c6cf898f76e58ec421b88e8cb3\n  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=8fcf3aa4640888622799bcdd9c4a70828108f2156bbfe8db2ce14f4eadb9327b\n  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n  Building wheel for iopath (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=ad4f1a1fae2c240eaabe6abe0816e4b182024f8a3345dda6a3876fb8e9d66d11\n  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\nSuccessfully built sentence-transformers tesseract pypika antlr4-python3-runtime fairscale langdetect iopath\nInstalling collected packages: tesseract, striprtf, pypika, monotonic, mmh3, filetype, dirtyjson, antlr4-python3-runtime, XlsxWriter, xlrd, triton, rouge, requests, rapidfuzz, python-oxmsg, python-multipart, python-magic, python-iso639, python-docx, pysbd, pyproject_hooks, pypdfium2, pypandoc, pyarrow, portalocker, poppler-utils, pillow, packaging, orjson, opentelemetry-util-http, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nltk, langdetect, jsonpath-python, humanfriendly, ftfy, einops, chroma-hnswlib, chardet, beautifulsoup4, bcrypt, asgiref, unstructured.pytesseract, tiktoken, python-pptx, posthog, pillow-heif, pikepdf, nvidia-cusparse-cu12, nvidia-cudnn-cu12, iopath, coloredlogs, build, azure-core, tokenizers, pycocotools, pdfminer.six, opentelemetry-instrumentation, openai, onnxruntime, nvidia-cusolver-cu12, llama-cloud, langsmith, kubernetes, unstructured-client, transformers, torch, pdfplumber, opentelemetry-instrumentation-asgi, msal, llama-index-legacy, llama-index-core, langchain-core, datasets, unstructured, opentelemetry-instrumentation-fastapi, msal-extensions, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, layoutparser, langchain-text-splitters, langchain-openai, fairscale, evaluate, bitsandbytes, bert-score, sentence-transformers, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, langchain, chromadb, azure-identity, unstructured-inference, open-clip-torch, llama-index-program-openai, llama-index-llms-azure-openai, langchain-community, effdet, ragas, llm-lens, llama-index-question-gen-openai, llama-index-embeddings-azure-openai, llama-index\n  Attempting uninstall: requests\n    Found existing installation: requests 2.32.3\n    Uninstalling requests-2.32.3:\n      Successfully uninstalled requests-2.32.3\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 14.0.2\n    Uninstalling pyarrow-14.0.2:\n      Successfully uninstalled pyarrow-14.0.2\n  Attempting uninstall: pillow\n    Found existing installation: Pillow 9.5.0\n    Uninstalling Pillow-9.5.0:\n      Successfully uninstalled Pillow-9.5.0\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.9.10\n    Uninstalling orjson-3.9.10:\n      Successfully uninstalled orjson-3.9.10\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n  Attempting uninstall: beautifulsoup4\n    Found existing installation: beautifulsoup4 4.12.2\n    Uninstalling beautifulsoup4-4.12.2:\n      Successfully uninstalled beautifulsoup4-4.12.2\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: kubernetes\n    Found existing installation: kubernetes 26.1.0\n    Uninstalling kubernetes-26.1.0:\n      Successfully uninstalled kubernetes-26.1.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.41.2\n    Uninstalling transformers-4.41.2:\n      Successfully uninstalled transformers-4.41.2\n  Attempting uninstall: torch\n    Found existing installation: torch 2.1.2\n    Uninstalling torch-2.1.2:\n      Successfully uninstalled torch-2.1.2\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.19.2\n    Uninstalling datasets-2.19.2:\n      Successfully uninstalled datasets-2.19.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\nkeras-nlp 0.12.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.1 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.1 which is incompatible.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires kubernetes<27,>=8.0.0, but you have kubernetes 30.1.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed XlsxWriter-3.2.0 antlr4-python3-runtime-4.9.3 asgiref-3.8.1 azure-core-1.30.2 azure-identity-1.17.1 bcrypt-4.1.3 beautifulsoup4-4.12.3 bert-score-0.3.13 bitsandbytes-0.43.1 build-1.2.1 chardet-5.2.0 chroma-hnswlib-0.7.5 chromadb-0.5.4 coloredlogs-15.0.1 datasets-2.19.1 dirtyjson-1.0.8 effdet-0.4.1 einops-0.8.0 evaluate-0.4.2 fairscale-0.4.13 filetype-1.2.0 ftfy-6.2.0 humanfriendly-10.0 iopath-0.1.10 jsonpath-python-1.0.6 kubernetes-30.1.0 langchain-0.2.7 langchain-community-0.2.7 langchain-core-0.2.16 langchain-openai-0.1.15 langchain-text-splitters-0.2.2 langdetect-1.0.9 langsmith-0.1.85 layoutparser-0.3.4 llama-cloud-0.0.6 llama-index-0.10.51 llama-index-agent-openai-0.2.8 llama-index-cli-0.1.12 llama-index-core-0.10.51 llama-index-embeddings-azure-openai-0.1.10 llama-index-embeddings-openai-0.1.10 llama-index-indices-managed-llama-cloud-0.2.4 llama-index-legacy-0.9.48 llama-index-llms-azure-openai-0.1.8 llama-index-llms-openai-0.1.25 llama-index-multi-modal-llms-openai-0.1.7 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.30 llama-index-readers-llama-parse-0.1.6 llama-parse-0.4.6 llm-lens-0.0.0.3 mmh3-4.1.0 monotonic-1.6 msal-1.29.0 msal-extensions-1.2.0 nltk-3.8.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 omegaconf-2.3.0 onnxruntime-1.18.1 open-clip-torch-2.26.1 openai-1.35.13 opentelemetry-instrumentation-0.43b0 opentelemetry-instrumentation-asgi-0.43b0 opentelemetry-instrumentation-fastapi-0.43b0 opentelemetry-util-http-0.43b0 orjson-3.10.6 packaging-24.1 pdfminer.six-20231228 pdfplumber-0.11.2 pikepdf-9.0.0 pillow-10.3.0 pillow-heif-0.17.0 poppler-utils-0.1.0 portalocker-2.10.0 posthog-3.5.0 pyarrow-14.0.1 pycocotools-2.0.8 pypandoc-1.13 pypdfium2-4.30.0 pypika-0.48.9 pyproject_hooks-1.1.0 pysbd-0.3.4 python-docx-1.1.2 python-iso639-2024.4.27 python-magic-0.4.27 python-multipart-0.0.9 python-oxmsg-0.0.1 python-pptx-0.6.23 ragas-0.1.10 rapidfuzz-3.9.4 requests-2.31.0 rouge-1.0.1 sentence-transformers-2.2.2 striprtf-0.0.26 tesseract-0.1.3 tiktoken-0.7.0 tokenizers-0.15.2 torch-2.3.0 transformers-4.39.2 triton-2.3.0 unstructured-0.14.10 unstructured-client-0.22.0 unstructured-inference-0.7.36 unstructured.pytesseract-0.3.12 xlrd-2.0.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"%pip install llama-index-vector-stores-chroma","metadata":{"execution":{"iopub.status.busy":"2024-07-12T00:52:22.589264Z","iopub.execute_input":"2024-07-12T00:52:22.590161Z","iopub.status.idle":"2024-07-12T00:52:37.613551Z","shell.execute_reply.started":"2024-07-12T00:52:22.590117Z","shell.execute_reply":"2024-07-12T00:52:37.612396Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting llama-index-vector-stores-chroma\n  Downloading llama_index_vector_stores_chroma-0.1.10-py3-none-any.whl.metadata (705 bytes)\nRequirement already satisfied: chromadb<0.6.0,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-vector-stores-chroma) (0.5.4)\nRequirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-vector-stores-chroma) (0.10.51)\nRequirement already satisfied: build>=1.0.3 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.2.1)\nRequirement already satisfied: pydantic>=1.9 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (2.5.3)\nRequirement already satisfied: chroma-hnswlib==0.7.5 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.7.5)\nRequirement already satisfied: fastapi>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.108.0)\nRequirement already satisfied: uvicorn>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.25.0)\nRequirement already satisfied: numpy<2.0.0,>=1.22.5 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.26.4)\nRequirement already satisfied: posthog>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (3.5.0)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (4.9.0)\nRequirement already satisfied: onnxruntime>=1.14.1 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.18.1)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.22.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.22.0)\nRequirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.43b0)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.22.0)\nRequirement already satisfied: tokenizers>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.15.2)\nRequirement already satisfied: pypika>=0.48.9 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.48.9)\nRequirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (4.66.4)\nRequirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (7.4.0)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (6.1.1)\nRequirement already satisfied: grpcio>=1.58.0 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.59.3)\nRequirement already satisfied: bcrypt>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (4.1.3)\nRequirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.9.0)\nRequirement already satisfied: kubernetes>=28.1.0 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (30.1.0)\nRequirement already satisfied: tenacity>=8.2.3 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (8.2.3)\nRequirement already satisfied: PyYAML>=6.0.0 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (6.0.1)\nRequirement already satisfied: mmh3>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (4.1.0)\nRequirement already satisfied: orjson>=3.9.12 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (3.10.6)\nRequirement already satisfied: httpx>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.27.0)\nRequirement already satisfied: SQLAlchemy>=1.4.49 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (3.9.1)\nRequirement already satisfied: dataclasses-json in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (0.6.6)\nRequirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.2.14)\nRequirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.0.8)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (2024.3.1)\nRequirement already satisfied: llama-cloud<0.0.7,>=0.0.6 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (0.0.6)\nRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.5.8)\nRequirement already satisfied: networkx>=3.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (3.2.1)\nRequirement already satisfied: nltk<4.0.0,>=3.8.1 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (3.8.1)\nRequirement already satisfied: openai>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.35.13)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (2.2.1)\nRequirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (10.3.0)\nRequirement already satisfied: requests>=2.31.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (2.31.0)\nRequirement already satisfied: tiktoken>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (0.7.0)\nRequirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (0.9.0)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.14.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (4.0.3)\nRequirement already satisfied: packaging>=19.1 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (24.1)\nRequirement already satisfied: pyproject_hooks in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.1.0)\nRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (2.0.1)\nRequirement already satisfied: starlette<0.33.0,>=0.29.0 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.32.0.post1)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (4.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (2024.2.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.0.5)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (3.6)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.3.0)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.14.0)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (2.9.0.post0)\nRequirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (2.26.1)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.7.0)\nRequirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.3.1)\nRequirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (3.2.2)\nRequirement already satisfied: urllib3>=1.24.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.26.18)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (2023.12.25)\nRequirement already satisfied: coloredlogs in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (15.0.1)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (23.5.26)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (3.20.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.12.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.9.0)\nRequirement already satisfied: importlib-metadata<7.0,>=6.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (6.11.0)\nRequirement already satisfied: backoff<3.0.0,>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (2.2.1)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.62.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.22.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.22.0)\nRequirement already satisfied: opentelemetry-proto==1.22.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.22.0)\nRequirement already satisfied: opentelemetry-instrumentation-asgi==0.43b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.43b0)\nRequirement already satisfied: opentelemetry-instrumentation==0.43b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.43b0)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.43b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.43b0)\nRequirement already satisfied: opentelemetry-util-http==0.43b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.43b0)\nRequirement already satisfied: setuptools>=16.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (69.0.3)\nRequirement already satisfied: asgiref~=3.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-asgi==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (3.8.1)\nRequirement already satisfied: monotonic>=1.5 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.6)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (3.3.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (3.0.3)\nRequirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.23.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (1.0.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.0.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.21.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (12.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (3.21.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-chroma) (2023.4)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.2.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (4.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (3.13.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (3.17.0)\nRequirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (10.0)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (1.3.0)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->llama-index-vector-stores-chroma) (0.5.1)\nDownloading llama_index_vector_stores_chroma-0.1.10-py3-none-any.whl (5.0 kB)\nInstalling collected packages: llama-index-vector-stores-chroma\nSuccessfully installed llama-index-vector-stores-chroma-0.1.10\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install chromadb","metadata":{"execution":{"iopub.status.busy":"2024-07-12T00:52:37.615063Z","iopub.execute_input":"2024-07-12T00:52:37.615378Z","iopub.status.idle":"2024-07-12T00:52:51.745849Z","shell.execute_reply.started":"2024-07-12T00:52:37.615349Z","shell.execute_reply":"2024-07-12T00:52:51.744751Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: chromadb in /opt/conda/lib/python3.10/site-packages (0.5.4)\nRequirement already satisfied: build>=1.0.3 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.2.1)\nRequirement already satisfied: pydantic>=1.9 in /opt/conda/lib/python3.10/site-packages (from chromadb) (2.5.3)\nRequirement already satisfied: chroma-hnswlib==0.7.5 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.7.5)\nRequirement already satisfied: fastapi>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.108.0)\nRequirement already satisfied: uvicorn>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.25.0)\nRequirement already satisfied: numpy<2.0.0,>=1.22.5 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.26.4)\nRequirement already satisfied: posthog>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (3.5.0)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.9.0)\nRequirement already satisfied: onnxruntime>=1.14.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.18.1)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.22.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.22.0)\nRequirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.43b0)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.22.0)\nRequirement already satisfied: tokenizers>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.15.2)\nRequirement already satisfied: pypika>=0.48.9 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.48.9)\nRequirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.66.4)\nRequirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (7.4.0)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb) (6.1.1)\nRequirement already satisfied: grpcio>=1.58.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.59.3)\nRequirement already satisfied: bcrypt>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.1.3)\nRequirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.9.0)\nRequirement already satisfied: kubernetes>=28.1.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (30.1.0)\nRequirement already satisfied: tenacity>=8.2.3 in /opt/conda/lib/python3.10/site-packages (from chromadb) (8.2.3)\nRequirement already satisfied: PyYAML>=6.0.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (6.0.1)\nRequirement already satisfied: mmh3>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.1.0)\nRequirement already satisfied: orjson>=3.9.12 in /opt/conda/lib/python3.10/site-packages (from chromadb) (3.10.6)\nRequirement already satisfied: httpx>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.27.0)\nRequirement already satisfied: packaging>=19.1 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (24.1)\nRequirement already satisfied: pyproject_hooks in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (1.1.0)\nRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (2.0.1)\nRequirement already satisfied: starlette<0.33.0,>=0.29.0 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.32.0.post1)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (4.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (2024.2.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.0.5)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (3.6)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.3.0)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\nRequirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.26.1)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.31.0)\nRequirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\nRequirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\nRequirement already satisfied: urllib3>=1.24.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.26.18)\nRequirement already satisfied: coloredlogs in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12.1)\nRequirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\nRequirement already satisfied: importlib-metadata<7.0,>=6.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (6.11.0)\nRequirement already satisfied: backoff<3.0.0,>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (2.2.1)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.62.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.22.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\nRequirement already satisfied: opentelemetry-proto==1.22.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\nRequirement already satisfied: opentelemetry-instrumentation-asgi==0.43b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\nRequirement already satisfied: opentelemetry-instrumentation==0.43b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.43b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\nRequirement already satisfied: opentelemetry-util-http==0.43b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\nRequirement already satisfied: setuptools>=16.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (69.0.3)\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\nRequirement already satisfied: asgiref~=3.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-asgi==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\nRequirement already satisfied: monotonic>=1.5 in /opt/conda/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.6)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (2.14.6)\nRequirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb) (0.23.2)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.3.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.3.2)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.0)\nRequirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport nest_asyncio\nfrom dotenv import load_dotenv\nload_dotenv()\n\n\nnest_asyncio.apply()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T00:52:51.748150Z","iopub.execute_input":"2024-07-12T00:52:51.748452Z","iopub.status.idle":"2024-07-12T00:52:51.775800Z","shell.execute_reply.started":"2024-07-12T00:52:51.748424Z","shell.execute_reply":"2024-07-12T00:52:51.775143Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\na = user_secrets.get_secret(\"AZURE_OPENAI_API_KEY\")\nb = user_secrets.get_secret(\"AZURE_OPENAI_ENDPOINT\")\nc = user_secrets.get_secret(\"OPENAI_API_VERSION\")\nd = user_secrets.get_secret(\"PAT_KEY\")","metadata":{"execution":{"iopub.status.busy":"2024-07-12T00:52:51.776815Z","iopub.execute_input":"2024-07-12T00:52:51.777122Z","iopub.status.idle":"2024-07-12T00:52:54.402528Z","shell.execute_reply.started":"2024-07-12T00:52:51.777097Z","shell.execute_reply":"2024-07-12T00:52:54.401768Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from llama_index.llms.azure_openai import AzureOpenAI\nfrom llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\nfrom llama_index.core import SimpleDirectoryReader\n# Initialize an embedding model from Hugging Face using the \"BAAI/bge-small-en\" model.\nembedding_model = AzureOpenAIEmbedding(\n    api_key=a,\n    model=\"text-embedding-3-large\",\n    deployment_name=\"text-embedding3\",\n     azure_endpoint=b,\n    api_version=c\n    \n)\nllm = AzureOpenAI(\n    model=\"gpt-35-turbo-16k\",\n    deployment_name=\"GPT35-turboA\",\n    api_key=a,\n    azure_endpoint=b,\n    api_version=c,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-12T00:52:54.403526Z","iopub.execute_input":"2024-07-12T00:52:54.403777Z","iopub.status.idle":"2024-07-12T00:52:58.606087Z","shell.execute_reply.started":"2024-07-12T00:52:54.403755Z","shell.execute_reply":"2024-07-12T00:52:58.605107Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"source_docs=SimpleDirectoryReader('test_data').load_data()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T00:52:58.607211Z","iopub.execute_input":"2024-07-12T00:52:58.607682Z","iopub.status.idle":"2024-07-12T00:53:19.323122Z","shell.execute_reply.started":"2024-07-12T00:52:58.607654Z","shell.execute_reply":"2024-07-12T00:53:19.321952Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"!mkdir images","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import shutil\n# import os\n\n# # Specify the path of the folder you want to delete\n# folder_path = 'extracted_jsons'\n\n# # Check if the folder exists\n# if os.path.exists(folder_path):\n#     # Delete the folder\n#     shutil.rmtree(folder_path)\n#     print(f\"Folder '{folder_path}' has been deleted.\")\n# else:\n#     print(f\"Folder '{folder_path}' does not exist.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\ndef delete_file(file_path):\n    try:\n        os.remove(file_path)\n        print(f\"Deleted file: {file_path}\")\n    except FileNotFoundError:\n        print(f\"File not found: {file_path}\")\n    except PermissionError:\n        print(f\"Permission denied: {file_path}\")\n    except Exception as e:\n        print(f\"Error deleting file: {file_path}, Error: {e}\")\n\nfile_to_delete = \"captions_output.txt\"  # Replace with the path to the file you want to delete\ndelete_file(file_to_delete)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tempfile\ndef clear_temp_files():\n    temp_dir = tempfile.gettempdir()\n    for file in os.listdir(temp_dir):\n        file_path = os.path.join(temp_dir, file)\n        try:\n            if os.path.isfile(file_path):\n                os.unlink(file_path)\n        except Exception as e:\n            print(f\"Error clearing temp files: {e}\")\n\nclear_temp_files()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!apt install -y poppler-utils","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from unstructured.partition.pdf import partition_pdf\n# from unstructured.staging.base import elements_to_json\n# import json \n\n# raw_pdf_elements = partition_pdf(\n#     filename=\"test_data/GNN_DEEPRL.pdf\",\n#     extract_images_in_pdf=True,\n#     infer_table_structure=True,\n#     chunking_strategy=\"by_title\",\n#     max_characters=4000,\n#     new_after_n_chars=3800,\n#     combine_text_under_n_chars=2000,\n#     extract_image_block_output_dir=\"images\",\n#     extract_image_block_to_payload=False,\n#     strategy = \"hi_res\"\n# )\n\n# # Store results in json\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# elements_to_json(raw_pdf_elements, \n#         filename=f\"./Extracted_pdf_elements.json\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n\n# def delete_pdf(folder_path, pdf_name):\n#     # Construct the full path to the PDF\n#     pdf_path = os.path.join(folder_path, pdf_name)\n\n#     # Check if the file exists\n#     if os.path.isfile(pdf_path):\n#         # Delete the file\n#         os.remove(pdf_path)\n#         print(f\"Deleted {pdf_name} from {folder_path}.\")\n#     else:\n#         print(f\"The file {pdf_name} does not exist in {folder_path}.\")\n\n# # Example usage:\n# delete_pdf('test_data', '/kaggle/working/Advanced_RAG/test_data/DRL.pdf')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from unstructured.partition.pdf import partition_pdf\nfrom unstructured.staging.base import elements_to_json\ndef process_pdfs_in_folder(folder_path, output_dir):\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    for filename in os.listdir(folder_path):\n        if filename.endswith('.pdf'):\n            pdf_path = os.path.join(folder_path, filename)\n            print(f\"Processing {pdf_path}...\")\n\n            pdf_name = os.path.splitext(filename)[0]\n            pdf_output_dir = os.path.join(output_dir, pdf_name)\n\n            # Create subdirectories for the PDF's images and JSON\n            images_dir = os.path.join(pdf_output_dir, \"images\")\n            json_dir = os.path.join(pdf_output_dir, \"json\")\n\n            if not os.path.exists(images_dir):\n                os.makedirs(images_dir)\n            if not os.path.exists(json_dir):\n                os.makedirs(json_dir)\n\n            raw_pdf_elements = partition_pdf(\n                filename=pdf_path,\n                extract_images_in_pdf=True,\n                infer_table_structure=True,\n                chunking_strategy=\"by_title\",\n                max_characters=4000,\n                new_after_n_chars=3800,\n                combine_text_under_n_chars=2000,\n                extract_image_block_output_dir=images_dir,\n                extract_image_block_to_payload=False,\n                strategy=\"hi_res\"\n            )\n\n            json_filename = f\"{pdf_name}.json\"\n            json_path = os.path.join(json_dir, json_filename)\n\n            elements_to_json(raw_pdf_elements, filename=json_path)\n            print(f\"Saved data to {json_path}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"  source_folder = \"test_data\"  # Path to your source_docs folder\n  output_folder = \"extracted_jsons\"  # Path to the folder where JSON files will be saved\n\n  process_pdfs_in_folder(source_folder, output_folder)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nimport os\n\ndef create_zip_from_folder(folder_path, zip_filename):\n    # Create a zip file from the folder\n    shutil.make_archive(zip_filename, 'zip', folder_path)\n    print(f\"Created zip file: {zip_filename}.zip\")\n\nextracted_folder = \"chroma_db\"  # Path to the folder to be zipped\nzip_filename = \"chroma_db_archive\"  # Name of the resulting zip file (without extension)\n\ncreate_zip_from_folder(extracted_folder, zip_filename)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:28:54.485125Z","iopub.execute_input":"2024-07-11T18:28:54.485759Z","iopub.status.idle":"2024-07-11T18:28:55.554919Z","shell.execute_reply.started":"2024-07-11T18:28:54.485723Z","shell.execute_reply":"2024-07-11T18:28:55.553686Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Created zip file: chroma_db_archive.zip\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## Tables and Text","metadata":{}},{"cell_type":"markdown","source":"### Preprocessing Helpers","metadata":{}},{"cell_type":"markdown","source":"**Display Table and Text**","metadata":{}},{"cell_type":"code","source":"from io import StringIO\nimport pandas as pd\nimport json\nimport re\nfrom bs4 import BeautifulSoup\ndef clean_html_content(html_content):\n    \"\"\"Extract only the well-formed HTML table parts from the provided HTML content.\"\"\"\n    # This regex attempts to isolate <table>...</table> blocks\n    if '<table' not in html_content:\n    # Find all pieces of table rows or cells\n        pieces = re.findall(r'<tr.*?>.*?</tr>', html_content, flags=re.DOTALL)\n        if pieces:\n            # Reconstruct HTML with <table> tags properly placed\n            start = html_content.find(pieces[0])\n            end = html_content.rfind(pieces[-1]) + len(pieces[-1])\n            html_content = (html_content[:start] +\n                            '<table>' +\n                            html_content[start:end] +\n                            '</table>' +\n                            html_content[end:])\n    return ''.join(re.findall(r'<table.*?>.*?</table>', html_content, flags=re.DOTALL))\n\ndef preprocess_json_file(input_filepath):\n    with open(input_filepath, 'r') as file:\n        data = json.load(file)\n\n    preprocessed_elements = []\n    for entry in data:\n        if entry['type'] == 'CompositeElement':\n            preprocessed_elements.append({\n                'type': 'text',\n                'content': entry['text']\n            })\n        if entry['type'] == 'Table':\n            html_content = entry['metadata']['text_as_html']\n            html_content=str(BeautifulSoup(html_content, 'html.parser'))\n            cleaned_html=clean_html_content(html_content)\n            soup = BeautifulSoup(cleaned_html, 'html.parser')\n            table_html = str(soup)\n            df = pd.read_html(StringIO(table_html))[0]\n            # Check if the DataFrame has unique columns, if not, assign unique names\n            if isinstance(df.columns, pd.MultiIndex):\n                df.columns = ['_'.join(map(str, col)).strip() for col in df.columns.values]\n            elif df.columns.astype(str)[0].isdigit(): \n                df.columns = [f'Column_{i+1}' for i in range(len(df.columns))]\n            table_json = json.loads(df.to_json(orient='records'))\n            preprocessed_elements.append({\n                'type': 'table',\n                'content': table_json\n            })\n\n    return preprocessed_elements\ndef preprocess_all_json_files(base_folder_path):\n    preprocessed_data = {}\n    for pdf_name in os.listdir(base_folder_path):\n        pdf_folder_path = os.path.join(base_folder_path, pdf_name, 'json')\n        if os.path.isdir(pdf_folder_path):\n            for file in os.listdir(pdf_folder_path):\n                if file.endswith('.json'):\n                    input_filepath = os.path.join(pdf_folder_path, file)\n                    print(f\"Processing {input_filepath}...\")\n                    preprocessed_data[pdf_name] = preprocess_json_file(input_filepath)\n    return preprocessed_data\n\ndef combine_elements(preprocessed_data):\n    combined_data = {}\n    for pdf_name, elements in preprocessed_data.items():\n        combined_text = \"\"\n        for element in elements:\n            if element['type'] == 'text':\n                combined_text += element['content'] + \"\\n\\n\"\n            elif element['type'] == 'table':\n                combined_text += \"Table:\\n\" + element['content'] + \"\\n\\n\"\n        combined_data[pdf_name] = combined_text\n    return combined_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_folder = \"extracted_jsons\"  # Path to your base folder containing PDF directories\npreprocessed_data = preprocess_all_json_files(base_folder)\nwith open('processed_tableandtext.json', 'w', encoding='utf-8') as outfile:\n    json.dump(preprocessed_data, outfile, indent=4, ensure_ascii=False)\n# combined_data = combine_elements(preprocessed_data)\n\n# # Optionally, save combined data to a file for later use\n# with open('combined_data.json', 'w') as outfile:\n#     json.dump(combined_data, outfile, indent=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Table Text summaries","metadata":{}},{"cell_type":"code","source":"import json\nimport openai\n\ndef format_table(table_data):\n    # Create a string representation of the table\n    headers = table_data[0].keys()\n    header_line = \" | \".join(headers)\n    lines = [header_line, \"-\"*len(header_line)]\n    for item in table_data:\n        row = \" | \".join(str(item[h]) for h in headers)\n        lines.append(row)\n    return \"\\n\".join(lines)\n\ndef summarize_content(content, content_type):\n    # Define the prompt based on the content type\n    if content_type == \"text\":\n        prompt = \"Summarize the following text: \\n\\n\" + content\n    elif content_type == \"table\":\n        table_string = format_table(content)\n        prompt = \"Summarize the key information from this table: \\n\\n\" + table_string\n    else:\n        return \"No summary available.\"\n\n    # Call the OpenAI API to generate the summary\n    response = llm.complete(\n    prompt\n    )\n    return response.text\n\ndef generate_summaries(file_path):\n    # Load the JSON data\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n\n    summary_data = {}\n    # Iterate through each document\n    for pdf_name, contents in data.items():\n        print(\"Processing pdf :\",pdf_name)\n        print()\n        summary_data[pdf_name] = {}\n        # Summarize each content type\n        for content_dict in contents:  # Adjusted to iterate over the list\n            content_type = content_dict['type']\n            content = content_dict['content']\n            summary = summarize_content(content, content_type)\n            summary_data[pdf_name][content_type + \"_summary\"] = summary\n\n    return summary_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Usage\nsummaries = generate_summaries('processed_tableandtext.json')\nprint(summaries)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Summarising with  langchain","metadata":{}},{"cell_type":"code","source":"from langchain_openai import AzureChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema.output_parser import StrOutputParser\nmodel = AzureChatOpenAI(\n        deployment_name=\"GPT35-turboA\",\n        api_version=\"2024-02-01\",\n        temperature=0\n      )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.prompts import ChatPromptTemplate\nfrom langchain.schema.output_parser import StrOutputParser\n\nprompt_text = \"\"\"\n  You are responsible for concisely summarizing table or text chunk:\n  Keep the summary short and crisp and extract key features\n\n  {element}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(prompt_text)\nsummarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\ndef process_pdf_contents(pdf_name, contents, summarize_chain):\n    summary_results = {}\n    print(\"Processing pdf:\", pdf_name)\n\n    try:\n        tables = [c['content'] for c in contents if c['type'] == 'table']\n        texts = [c['content'] for c in contents if c['type'] == 'text']\n\n        # Process tables\n        if tables:\n            table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})\n            summary_results['table_summaries'] = table_summaries\n            print(\"Table Summaries:\")\n            print(table_summaries)\n\n        # Process texts\n        if texts:\n            text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n            summary_results['text_summaries'] = text_summaries\n            print('Text Summaries:')\n            print(text_summaries)\n\n    except Exception as e:\n        print(f\"Error processing {pdf_name}: {e}\")\n        time.sleep(10)  # Sleep to respect rate limits or handle transient issues\n\n    return summary_results\n\ndef load_and_summarize(file_path):\n    # Load existing data if available\n    if os.path.exists('intermediate_summary_results.json'):\n        with open('intermediate_summary_results.json', 'r') as infile:\n            all_results = json.load(infile)\n    else:\n        all_results = {}\n\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n\n    for pdf_name, contents in data.items():\n        if pdf_name not in all_results:\n            result = process_pdf_contents(pdf_name, contents, summarize_chain)\n            all_results[pdf_name] = result\n            # Save intermediate results to avoid losing progress\n            with open('intermediate_summary_results.json', 'w') as outfile:\n                json.dump(all_results, outfile, indent=4)\n\n    return all_results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res=load_and_summarize('processed_tableandtext.json')\nprint(res)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('summarized_data.json', 'w') as file:\n        json.dump(res, file, indent=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom bs4 import BeautifulSoup\nfrom io import StringIO\n\n# Example HTML content as a string\nhtml_content = \"\"\"\n<table><tbody><tr><td>Bronchoalveolar lavage</td></tr><tr><td>Bronchoalveolar lavage fluid</td></tr><tr><td>Forced oscillatory technique</td></tr><tr><td>Functional residual capacity</td></tr><tr><td>Peak expiratory flow</td></tr><tr><td>Peak inspiratory flow</td></tr><tr><td>Respiratory inductive plethysmography</td></tr><tr><td>Pulmonary resistance</td></tr><tr><td>Respiratory system resistance</td></tr><tr><td>Expiratory time</td></tr><tr><td>Inspiratory time</td></tr><tr><td>Minute ventilation</td></tr><tr><td>Tidal volume</td></tr><tr><td>Respiratory system reactance</td></tr></tbody></table>\n\"\"\"\n\nsoup = BeautifulSoup(html_content, 'html.parser')\ntable_html = str(soup)\ndf = pd.read_html(StringIO(table_html))[0]\n\n# Manually assign a header if you know the content type\ndf.columns = ['Medical Procedures']\n\n# Convert DataFrame to JSON\ntable_json = df.to_json(orient='records', indent=4)\nprint(table_json)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Processing Images","metadata":{}},{"cell_type":"markdown","source":"### Preprocessing Helpers","metadata":{}},{"cell_type":"markdown","source":"**Verifying images**","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport os\ndef verify_images(directory):\n    for filename in os.listdir(directory):\n        path = os.path.join(directory, filename)\n        try:\n            with Image.open(path) as img:\n                print(f\"{filename} is valid.\")\n        except IOError:\n            print(f\"Error opening {filename}; it may be corrupted or in an incorrect format.\")\n\n# Verify images before processing\nverify_images('images')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Display images**","metadata":{}},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport pandas as pd\n\n# Directory containing the extracted images\nimages_path = \"images\"\n\n# Function to display images\ndef plot_images(images_folder):\n    # Check if the directory exists\n    if not os.path.exists(images_folder):\n        print(f\"The directory {images_folder} does not exist.\")\n        return\n\n    # List all image files\n    image_files = [f for f in os.listdir(images_folder) if f.endswith(('.png', '.jpg', '.jpeg', '.ppm'))]\n    total_files = len(image_files)\n\n    if total_files == 0:\n        print(\"No images found in the directory.\")\n        return\n\n    print(f\"Found {total_files} images.\")\n    \n    # Plot each image\n    fig, axs = plt.subplots(1, total_files, figsize=(15, 5))\n    for ax, image_file in zip(axs, image_files):\n        image_path = os.path.join(images_folder, image_file)\n        image = Image.open(image_path)\n        ax.imshow(image)\n        ax.axis('off')\n        ax.set_title(image_file)\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Clear files**","metadata":{}},{"cell_type":"code","source":"def clear_file_if_not_empty(file_path):\n    try:\n        # Check if the file exists\n        with open(file_path, 'r+') as file:\n            contents = file.read()\n            # Check if the file is not empty\n            if contents:\n                # Move the cursor to the beginning of the file\n                file.seek(0)\n                # Clear the file\n                file.truncate()\n                print(\"File was not empty and has been cleared.\")\n            else:\n                print(\"File is already empty.\")\n    except FileNotFoundError:\n        print(f\"No file found at {file_path}.\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n# Specify the path to your file\nfile_path = 'captions_output.txt'\n\n# Call the function to check and clear the file\nclear_file_if_not_empty(file_path)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Processing with MILVLG","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom PIL import Image\ntorch.set_default_device(\"cuda\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntorch.cuda.empty_cache()\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"MILVLG/imp-v1-3b\",\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(\"MILVLG/imp-v1-3b\", trust_remote_code=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_and_caption_image(image_path):\n    try:\n        # Ensure image is loaded correctly\n        image = Image.open(image_path).convert('RGB')\n        text = (\n            \"Please analyze the image thoroughly. The image may contain various forms of data representation such as charts, graphs, tables, etc. \"\n            \"For charts and graphs, identify and describe the type, axes, labels, data points, trends, and any significant peaks, troughs, or patterns. \"\n            \"Highlight any anomalies or outliers and discuss their possible implications. Extract and report all key numerical values. \"\n            \"For other types of images, describe all visible elements and their relationships in detail. Provide a clear and precise summary of the key features,\"\n            \"interpret the data where applicable, and make note of any notable observations or ambiguities.\" \n            \"It is crucial to extract all key numerical values if present in the image.\"\n\n        )\n        input_ids = tokenizer(text, return_tensors='pt').input_ids\n        image_tensor = model.image_preprocess(image)\n\n        \n\n        # Generate the answer\n        with torch.no_grad():\n            output_ids = model.generate(\n                input_ids,\n                max_new_tokens=100,\n                images=image_tensor,\n                use_cache=True\n            )[0]\n        \n        return [tokenizer.decode(output_ids[input_ids.shape[1]:], skip_special_tokens=True).strip()]\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {e}\")\n        return []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport json\ndef process_images_in_folder(base_folder_path):\n    structured_data = {}\n\n    for pdf_name in os.listdir(base_folder_path):\n        pdf_folder_path = os.path.join(base_folder_path, pdf_name, 'images')\n        if os.path.isdir(pdf_folder_path):\n            image_files = [f for f in os.listdir(pdf_folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n            image_captions = {}\n            print(f\"Processing pdf : {pdf_name} \\n\")\n            for filename in image_files:\n                image_path = os.path.join(pdf_folder_path, filename)\n                \n                captions = process_and_caption_image(image_path)\n                image_captions[filename] = captions\n\n            structured_data[pdf_name] = image_captions\n\n    return structured_data\n\nbase_folder = \"extracted_jsons\"  # Path to your base folder containing PDF directories\nquestion = \"Describe the image in detail and extract key features from it\"\nstructured_image_data = process_images_in_folder(base_folder)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Optionally, save structured image data to a JSON file for later use\nwith open('structured_image_data.json', 'w') as outfile:\n    json.dump(structured_image_data, outfile, indent=4)\n\nprint(f\"Image captions have been saved to structured_image_data.json\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"structured_image_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Retriever Langchain","metadata":{}},{"cell_type":"code","source":"import uuid\nimport json\n\nfrom langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\nfrom langchain.retrievers.multi_vector import MultiVectorRetriever\nfrom langchain.schema.document import Document\nfrom langchain.storage import InMemoryStore\nfrom langchain.vectorstores import Chroma\n\nembeddings_model = AzureOpenAIEmbeddings(\n        azure_deployment=\"text-embedding3\",\n        api_version=\"2024-02-01\"\n    )\n# Load the JSON file containing summaries\ndef load_data(file_path):\n    with open(file_path, 'r') as file:\n        return json.load(file)\n\n# Initialize the retriever\nid_key = \"doc_id\"\nretriever = MultiVectorRetriever(\n    vectorstore=Chroma(collection_name=\"summaries\", embedding_function=embeddings_model),\n    docstore=InMemoryStore(),\n    id_key=id_key,\n)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"retriever.vectorstore.clear()\nretriever.docstore.clear()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef store_summaries(original_data, summary_data):\n    for pdf_name, contents in original_data.items():\n        print(f\"Processing PDF: {pdf_name}\")\n        \n        # Extract original text and table contents\n        text_contents = [c['content'] for c in contents if c['type'] == 'text']\n        table_contents = [c['content'] for c in contents if c['type'] == 'table']\n#         print(text_contents[1])\n          \n\n        # Store and print text summaries along with their corresponding original texts\n        if 'text_summaries' in summary_data[pdf_name]:\n            text_ids = [str(uuid.uuid4()) for _ in summary_data[pdf_name]['text_summaries']]\n            print(\"text Ids:\",text_ids)\n            for i, summary in enumerate(summary_data[pdf_name]['text_summaries']):\n                \n                summary_texts = Document(page_content=summary, metadata={id_key: text_ids[i]})\n                retriever.vectorstore.add_documents([summary_texts])\n                retriever.docstore.mset([(text_ids[i], text_contents[i])])\n        \n        # Store and print table summaries along with their corresponding original tables\n        if 'table_summaries' in summary_data[pdf_name]:\n            table_ids = [str(uuid.uuid4()) for _ in summary_data[pdf_name]['table_summaries']]\n            for i, summary in enumerate(summary_data[pdf_name]['table_summaries']):                \n                summary_tables = Document(page_content=summary, metadata={id_key: table_ids[i]})\n                retriever.vectorstore.add_documents([summary_tables])\n                retriever.docstore.mset([(table_ids[i], table_contents[i])])\n\n# Proceed with data loading and summarizing\noriginal_data = load_data('processed_tableandtext.json')\nsummary_data = load_data('summarized_data.json')\nstore_summaries(original_data, summary_data)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport os\n\nimport base64\n\ndef image_to_base64(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode('utf-8')\n\n\ndef load_and_process_images(pdf_name, image_data, base_dir):\n    \"\"\"Load image files, convert them to base64, and collect summaries.\"\"\"\n    image_paths = [os.path.join(base_dir, pdf_name, 'images', image_name) for image_name in image_data.keys()]\n    image_summaries = [summary[0] for summary in image_data.values()]\n    image_contents = [image_to_base64(path) for path in image_paths]\n\n    # Print each image path, a snippet of its base64 content, and its summary for verification\n    for path, content, summary in zip(image_paths, image_contents, image_summaries):\n        print(f\"Image Path: {path}\")\n        print(f\"Base64 Snippet: {content[:60]}...\")  # Print only the first 60 characters of the base64 string\n        print(f\"Summary: {summary}\")\n        print(\"\\n-------------------\\n\")\n\n    return image_paths, image_summaries, image_contents\n\ndef store_image_data(image_paths, image_summaries, image_contents):\n    doc_ids = [str(uuid.uuid4()) for _ in image_paths]\n    summary_images = [\n        Document(page_content=summary, metadata={id_key: doc_id})\n        for doc_id, summary in zip(doc_ids, image_summaries)\n    ]\n    # Add summaries to the vector store\n    retriever.vectorstore.add_documents(summary_images)\n    # Store original images in the document store\n    retriever.docstore.mset(list(zip(doc_ids, image_contents)))\n\n# Example usage\nbase_dir = 'extracted_jsons'\nstructured_image_file = 'structured_image_data.json'\nimage_data_full = json.load(open(structured_image_file))\n\n# Process and store each PDF's images\nfor pdf_name, images in image_data_full.items():\n    image_paths, image_summaries, image_contents = load_and_process_images(pdf_name, images, base_dir)\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import base64\nfrom PIL import Image\nfrom IPython.display import HTML, display\nimport io\nfrom langchain.schema import Document, HumanMessage\n\ndef plt_img_base64(img_base64):\n    \"\"\"Display an image from a base64 encoded string.\"\"\"\n    display(HTML(f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'))\n\ndef is_image_data(b64data):\n    \"\"\"Check if the base64 data is an image by looking at the start of the data.\"\"\"\n    image_signatures = {\n        b\"\\xFF\\xD8\\xFF\": \"jpg\",\n        b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\": \"png\",\n        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n    }\n    try:\n        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n        for sig, format in image_signatures.items():\n            if header.startswith(sig):\n                return True\n        return False\n    except Exception:\n        return False\n\ndef split_image_text_types(docs):\n    \"\"\"Split base64-encoded images and texts.\"\"\"\n    b64_images = []\n    texts = []\n    for doc in docs:\n        # Check if the document is of type Document and extract page_content if so\n        if isinstance(doc, Document):\n            doc = doc.page_content\n\n        if is_image_data(doc):\n            b64_images.append(doc)\n        else:\n            texts.append(doc)\n    return {\"images\": b64_images, \"texts\": texts}\n\ndef img_prompt_func(data_dict):\n    \"\"\"Generate a text prompt incorporating image descriptions or placeholders.\"\"\"\n    messages = []\n\n    # Process images: Since GPT-3 cannot directly handle images, use descriptions or placeholders\n    if \"images\" in data_dict[\"context\"]:\n        for image in data_dict[\"context\"][\"images\"]:\n            description = f\"[Image: A detailed description or a URL pointing to the image data.]\"  # Placeholder text\n            messages.append(description)\n\n    # Process texts: Handle complex structures\n    if \"texts\" in data_dict[\"context\"]:\n        formatted_texts = []\n        for text_block in data_dict[\"context\"][\"texts\"]:\n            if isinstance(text_block, list):\n                for text in text_block:\n                    # Check if the item is a dictionary and format it\n                    if isinstance(text, dict):\n                        text_description = ' | '.join(f\"{key}: {value}\" for key, value in text.items() if value is not None)\n                        formatted_texts.append(text_description)\n                    elif isinstance(text, str):\n                        # Directly append the string if it's not a dictionary\n                        formatted_texts.append(text)\n            elif isinstance(text_block, str):\n                # Handle the case where text_block is directly a string\n                formatted_texts.append(text_block)\n\n        formatted_text_string = \"\\n\".join(formatted_texts)\n        messages.append(formatted_text_string)\n\n    # Combine all messages into a single string with the question\n    prompt = (\n        f\"You are a research analyst. You should provide precise answers to each question\\n\"\n        f\"Question: {data_dict['question']}\\n\\n\"\n        \"Details:\\n\"\n        + \"\\n\".join(messages)\n    )\n    return prompt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n\n# RAG pipeline\nchain = (\n    {\n        \"context\": retriever | RunnableLambda(split_image_text_types),\n        \"question\": RunnablePassthrough(),\n    }\n    | RunnableLambda(img_prompt_func)\n    | model\n    | StrOutputParser()\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query='Who are the authors of \"GNN_DEEPRL\"?'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chain.invoke(query)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"docs = retriever.get_relevant_documents(query)\nlen(docs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"docs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Retriever llama index","metadata":{}},{"cell_type":"code","source":"import json\nfrom tqdm import tqdm  # \nfrom llama_index.core import SimpleDirectoryReader, VectorStoreIndex\nfrom llama_index.core.node_parser import SimpleNodeParser\nfrom llama_index.vector_stores.chroma import ChromaVectorStore\n\n# Load the JSON data from files\nwith open('processed_tableandtext.json', 'r') as file:\n    table_and_text_data = json.load(file)\n\nwith open('summarized_data.json', 'r') as file:\n    summarized_data = json.load(file)\n\nwith open('structured_image_data.json', 'r') as file:\n    structured_image_data = json.load(file)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T00:59:30.683450Z","iopub.execute_input":"2024-07-12T00:59:30.684071Z","iopub.status.idle":"2024-07-12T00:59:30.703057Z","shell.execute_reply.started":"2024-07-12T00:59:30.684043Z","shell.execute_reply":"2024-07-12T00:59:30.702135Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def normalize_title(title):\n    return title.lower().strip().replace('.pdf', '')\n\n# Normalize the keys in the data dictionaries\nnormalized_table_and_text_data = {normalize_title(key): value for key, value in table_and_text_data.items()}\nnormalized_summarized_data = {normalize_title(key): value for key, value in summarized_data.items()}\nnormalized_structured_image_data = {normalize_title(key): value for key, value in structured_image_data.items()}","metadata":{"execution":{"iopub.status.busy":"2024-07-12T00:59:30.867889Z","iopub.execute_input":"2024-07-12T00:59:30.868479Z","iopub.status.idle":"2024-07-12T00:59:30.874280Z","shell.execute_reply.started":"2024-07-12T00:59:30.868450Z","shell.execute_reply":"2024-07-12T00:59:30.873287Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"source_docs = SimpleDirectoryReader('test_data').load_data()\n\n# Create a node parser with desired settings\nbaseline_parser = SimpleNodeParser.from_defaults(\n    chunk_overlap=200,\n    chunk_size=1024\n)\n\n# Extract nodes from the documents with progress monitoring\nprint(\"Parsing documents into nodes...\")\nbaseline_nodes = []\nfor doc in tqdm(source_docs, desc=\"Parsing documents\"):\n    nodes = baseline_parser.get_nodes_from_documents([doc])\n    for node in nodes:\n        # Initialize extra_info if it doesn't exist\n        if not hasattr(node, 'extra_info'):\n            node.extra_info = {'document_title': doc.metadata.get('title', 'Unknown')}\n    baseline_nodes.extend(nodes)\nprint(\"Parsing complete.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:14:25.065579Z","iopub.execute_input":"2024-07-11T18:14:25.066320Z","iopub.status.idle":"2024-07-11T18:14:47.101342Z","shell.execute_reply.started":"2024-07-11T18:14:25.066285Z","shell.execute_reply":"2024-07-11T18:14:47.100381Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pypdf/_utils.py:233: RuntimeWarning: coroutine 'Executor.wrap_callable_with_index.<locals>.wrapped_callable_async' was never awaited\n  m = regex.search(name + tok)\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\n","output_type":"stream"},{"name":"stdout","text":"Parsing documents into nodes...\n","output_type":"stream"},{"name":"stderr","text":"Parsing documents: 100%|██████████| 179/179 [00:00<00:00, 229.00it/s]","output_type":"stream"},{"name":"stdout","text":"Parsing complete.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"from tqdm import tqdm  # Import tqdm for progress monitoring\n\ndef normalize_title(title):\n    return title.lower().strip().replace('.pdf', '')\n\ndef extend_nodes_with_summaries(nodes, normalized_table_and_text_data, normalized_summarized_data, normalized_structured_image_data):\n    extended_nodes = []\n    for node in tqdm(nodes, desc=\"Extending nodes\"):\n        # Normalize title for comparison\n        title = normalize_title(node.metadata['file_name'])  # Assuming the title is in metadata\n        print(f\"Processing node for title: {title}\")\n\n        # Handling table content\n        if title in normalized_table_and_text_data:\n            table_content = [item['content'] for item in normalized_table_and_text_data[title] if item['type'] == 'table']\n            node.metadata['table_content'] = table_content\n            print(f\"Added table content for title: {title}\")\n        else:\n            print(f\"Title {title} not found in normalized_table_and_text_data\")\n\n        # Handling summarized data for tables and texts\n        if title in normalized_summarized_data:\n            table_summaries = normalized_summarized_data[title].get('table_summaries', [])\n            text_summaries = normalized_summarized_data[title].get('text_summaries', [])\n            node.metadata['table_summaries'] = table_summaries\n            node.metadata['text_summaries'] = text_summaries\n            if table_summaries:\n                print(f\"Added table summaries for title: {title}\")\n            if text_summaries:\n                print(f\"Added text summaries for title: {title}\")\n        else:\n            print(f\"Title {title} not found in normalized_summarized_data\")\n\n        # Handling summarized data for images\n        if title in normalized_structured_image_data:\n            image_summaries = normalized_structured_image_data[title]\n            node.metadata['image_summaries'] = image_summaries\n            print(f\"Added image summaries for title: {title}\")\n        else:\n            print(f\"Title {title} not found in normalized_structured_image_data\")\n\n        extended_nodes.append(node)\n    \n    return extended_nodes\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:14:51.275216Z","iopub.execute_input":"2024-07-11T18:14:51.275850Z","iopub.status.idle":"2024-07-11T18:14:51.286463Z","shell.execute_reply.started":"2024-07-11T18:14:51.275813Z","shell.execute_reply":"2024-07-11T18:14:51.285445Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"extended_baseline_nodes = extend_nodes_with_summaries(baseline_nodes, normalized_table_and_text_data, normalized_summarized_data, normalized_structured_image_data)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:14:53.635159Z","iopub.execute_input":"2024-07-11T18:14:53.635840Z","iopub.status.idle":"2024-07-11T18:14:53.657597Z","shell.execute_reply.started":"2024-07-11T18:14:53.635806Z","shell.execute_reply":"2024-07-11T18:14:53.656688Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"Extending nodes: 100%|██████████| 299/299 [00:00<00:00, 21105.28it/s]","output_type":"stream"},{"name":"stdout","text":"Processing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: deepfake\nAdded table content for title: deepfake\nAdded table summaries for title: deepfake\nAdded text summaries for title: deepfake\nAdded image summaries for title: deepfake\nProcessing node for title: fmri_vision_reconstruction\nAdded table content for title: fmri_vision_reconstruction\nAdded table summaries for title: fmri_vision_reconstruction\nAdded text summaries for title: fmri_vision_reconstruction\nAdded image summaries for title: fmri_vision_reconstruction\nProcessing node for title: fmri_vision_reconstruction\nAdded table content for title: fmri_vision_reconstruction\nAdded table summaries for title: fmri_vision_reconstruction\nAdded text summaries for title: fmri_vision_reconstruction\nAdded image summaries for title: fmri_vision_reconstruction\nProcessing node for title: fmri_vision_reconstruction\nAdded table content for title: fmri_vision_reconstruction\nAdded table summaries for title: fmri_vision_reconstruction\nAdded text summaries for title: fmri_vision_reconstruction\nAdded image summaries for title: fmri_vision_reconstruction\nProcessing node for title: fmri_vision_reconstruction\nAdded table content for title: fmri_vision_reconstruction\nAdded table summaries for title: fmri_vision_reconstruction\nAdded text summaries for title: fmri_vision_reconstruction\nAdded image summaries for title: fmri_vision_reconstruction\nProcessing node for title: fmri_vision_reconstruction\nAdded table content for title: fmri_vision_reconstruction\nAdded table summaries for title: fmri_vision_reconstruction\nAdded text summaries for title: fmri_vision_reconstruction\nAdded image summaries for title: fmri_vision_reconstruction\nProcessing node for title: fmri_vision_reconstruction\nAdded table content for title: fmri_vision_reconstruction\nAdded table summaries for title: fmri_vision_reconstruction\nAdded text summaries for title: fmri_vision_reconstruction\nAdded image summaries for title: fmri_vision_reconstruction\nProcessing node for title: fmri_vision_reconstruction\nAdded table content for title: fmri_vision_reconstruction\nAdded table summaries for title: fmri_vision_reconstruction\nAdded text summaries for title: fmri_vision_reconstruction\nAdded image summaries for title: fmri_vision_reconstruction\nProcessing node for title: fmri_vision_reconstruction\nAdded table content for title: fmri_vision_reconstruction\nAdded table summaries for title: fmri_vision_reconstruction\nAdded text summaries for title: fmri_vision_reconstruction\nAdded image summaries for title: fmri_vision_reconstruction\nProcessing node for title: fmri_vision_reconstruction\nAdded table content for title: fmri_vision_reconstruction\nAdded table summaries for title: fmri_vision_reconstruction\nAdded text summaries for title: fmri_vision_reconstruction\nAdded image summaries for title: fmri_vision_reconstruction\nProcessing node for title: fmri_vision_reconstruction\nAdded table content for title: fmri_vision_reconstruction\nAdded table summaries for title: fmri_vision_reconstruction\nAdded text summaries for title: fmri_vision_reconstruction\nAdded image summaries for title: fmri_vision_reconstruction\nProcessing node for title: fmri_vision_reconstruction\nAdded table content for title: fmri_vision_reconstruction\nAdded table summaries for title: fmri_vision_reconstruction\nAdded text summaries for title: fmri_vision_reconstruction\nAdded image summaries for title: fmri_vision_reconstruction\nProcessing node for title: fmri_vision_reconstruction\nAdded table content for title: fmri_vision_reconstruction\nAdded table summaries for title: fmri_vision_reconstruction\nAdded text summaries for title: fmri_vision_reconstruction\nAdded image summaries for title: fmri_vision_reconstruction\nProcessing node for title: fmri_vision_reconstruction\nAdded table content for title: fmri_vision_reconstruction\nAdded table summaries for title: fmri_vision_reconstruction\nAdded text summaries for title: fmri_vision_reconstruction\nAdded image summaries for title: fmri_vision_reconstruction\nProcessing node for title: fmri_vision_reconstruction\nAdded table content for title: fmri_vision_reconstruction\nAdded table summaries for title: fmri_vision_reconstruction\nAdded text summaries for title: fmri_vision_reconstruction\nAdded image summaries for title: fmri_vision_reconstruction\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: llama\nAdded table content for title: llama\nAdded table summaries for title: llama\nAdded text summaries for title: llama\nAdded image summaries for title: llama\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: qwen\nAdded table content for title: qwen\nAdded table summaries for title: qwen\nAdded text summaries for title: qwen\nAdded image summaries for title: qwen\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: vision_instruction\nAdded table content for title: vision_instruction\nAdded table summaries for title: vision_instruction\nAdded text summaries for title: vision_instruction\nAdded image summaries for title: vision_instruction\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\nProcessing node for title: wisper\nAdded table content for title: wisper\nAdded table summaries for title: wisper\nAdded text summaries for title: wisper\nAdded image summaries for title: wisper\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"for node in extended_baseline_nodes[:3]:  # Check the first 3 nodes for demonstration\n    print(f\"Title: {node.metadata['file_name']}\")\n    print(f\"Table Content: {node.metadata['table_summaries']}\")\n    print(f\"Table Summaries: {node.metadata['table_summaries']}\")\n    print(f\"Text Summaries: {node.metadata['text_summaries']}\")\n    print(f\"Image Summaries: {node.metadata['image_summaries']}\")\n    print(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from llama_index.core import Settings\nSettings.llm=llm\nSettings.embed_model=embedding_model","metadata":{"execution":{"iopub.status.busy":"2024-07-12T03:02:48.263000Z","iopub.execute_input":"2024-07-12T03:02:48.263713Z","iopub.status.idle":"2024-07-12T03:02:48.268336Z","shell.execute_reply.started":"2024-07-12T03:02:48.263679Z","shell.execute_reply":"2024-07-12T03:02:48.267344Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"import time\nimport chromadb\nfrom llama_index.core import StorageContext\n\n# Retry mechanism for handling rate limits\n\n\ndef create_and_persist_index():\n    # Create the Chroma client and collection using a persistent client\n    print(\"Initializing Chroma persistent client and collection...\")\n    db = chromadb.PersistentClient(path=\"./chroma_db\")  # Define the storage path\n    chroma_collection = db.get_or_create_collection(\"baseline_indexNew\")\n    \n    print(f\"Collection '{chroma_collection.name}' ready for use.\")\n\n    # Create the Chroma vector store\n    print(\"Initializing ChromaVectorStore...\")\n    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n    print(\"ChromaVectorStore initialized with collection_name 'baseline_indexNew'\")\n\n    # Create the index\n    print(\"Creating VectorStoreIndex...\")\n    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n    baseline_index = VectorStoreIndex.from_documents(\n        documents=source_docs,\n        storage_context=storage_context,\n        embed_model=embedding_model\n    )\n    print(\"VectorStoreIndex created\")\n\n    # Persist the index\n    print(\"Persisting VectorStoreIndex...\")\n    baseline_index.storage_context.persist()\n    print(\"VectorStoreIndex persisted successfully.\")\n    return baseline_index\n\nbaseline_index = retry_with_backoff(create_and_persist_index)\nprint(\"Index creation and persistence complete.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:24:52.426440Z","iopub.execute_input":"2024-07-11T18:24:52.427128Z","iopub.status.idle":"2024-07-11T18:26:29.521795Z","shell.execute_reply.started":"2024-07-11T18:24:52.427094Z","shell.execute_reply":"2024-07-11T18:26:29.520772Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Creating and persisting index with retries...\nInitializing Chroma persistent client and collection...\nCollection 'baseline_indexNew' ready for use.\nInitializing ChromaVectorStore...\nChromaVectorStore initialized with collection_name 'baseline_indexNew'\nCreating VectorStoreIndex...\nVectorStoreIndex created\nPersisting VectorStoreIndex...\nVectorStoreIndex persisted successfully.\nIndex creation and persistence complete.\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Converting index to query engine...\")\nbaseline_query_engine = baseline_index.as_query_engine(similarity_top_k=3)\nprint(\"Query engine ready.\")\n\n# Function to retrieve data based on a query\ndef retrieve_data(query):\n    results = baseline_query_engine.query(query)\n    return results\n\n# Example query\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:31:22.103923Z","iopub.execute_input":"2024-07-11T18:31:22.104680Z","iopub.status.idle":"2024-07-11T18:31:22.110174Z","shell.execute_reply.started":"2024-07-11T18:31:22.104647Z","shell.execute_reply":"2024-07-11T18:31:22.109337Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Converting index to query engine...\nQuery engine ready.\n","output_type":"stream"}]},{"cell_type":"code","source":"query = 'what is multimodal condition in talking face generation?'\nretrieved_data = retrieve_data(query)\nprint(retrieved_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_output(results):\n    for result in results:\n        node = result.node\n        print(\"Document ID:\", node.id_)\n        print(\"File Name:\", node.metadata['file_name'])\n        print(\"File Type:\", node.metadata['file_type'])\n        print(\"Creation Date:\", node.metadata['creation_date'])\n        print(\"Page Label:\", node.metadata['page_label'])\n        print(\"File Path:\", node.metadata['file_path'])\n        print()\n        print(\"Text Extract:\")\n        print(node.text[:300])  # Displaying first 300 characters for brevity\n        print(\"-\" * 80)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=baseline_query_engine.retrieve(query)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Custom Retriever","metadata":{}},{"cell_type":"code","source":"import time\nfrom tqdm import tqdm\nimport chromadb\nfrom llama_index.core import Document, VectorStoreIndex, StorageContext\nfrom llama_index.core.node_parser import SimpleNodeParser\nfrom llama_index.core.schema import Node, NodeRelationship, RelatedNodeInfo\nfrom llama_index.vector_stores.chroma import ChromaVectorStore\nimport time\nimport chromadb\nimport multiprocessing\nfrom tqdm import tqdm\nimport logging\nfrom functools import partial\ndef normalize_title(title):\n    return title.lower().strip().replace('.pdf', '')\n\ndef process_document(doc, normalized_table_and_text_data, normalized_summarized_data, normalized_structured_image_data):\n    all_nodes = []\n    main_node = Node(\n        text=doc.text,\n        metadata={\n            \"file_name\": doc.metadata.get(\"file_name\"),\n            \"doc_id\": doc.id_,\n            \"is_document\": True\n        }\n    )\n    all_nodes.append(main_node)\n\n    title = normalize_title(doc.metadata.get(\"file_name\", \"\"))\n    print(\"PROCESSING FILE:\",title)\n\n    # Create nodes for table content\n    if title in normalized_table_and_text_data:\n        for item in normalized_table_and_text_data[title]:\n            if item['type'] == 'table':\n                table_node = Node(\n                    text=f\"Table Content: {item['content']}\",\n                    metadata={\n                        \"doc_id\": doc.id_,\n                        \"type\": \"table_content\"\n                    }\n                )\n                all_nodes.append(table_node)\n                main_node.relationships[NodeRelationship.CHILD] = main_node.relationships.get(NodeRelationship.CHILD, []) + [\n                    RelatedNodeInfo(node_id=table_node.id_)\n                ]\n\n    # Create nodes for table summaries\n    if title in normalized_summarized_data:\n        for summary in normalized_summarized_data[title].get('table_summaries', []):\n            table_summary_node = Node(\n                text=f\"Table Summary: {summary}\",\n                metadata={\n                    \"doc_id\": doc.id_,\n                    \"type\": \"table_summary\"\n                }\n            )\n            all_nodes.append(table_summary_node)\n            main_node.relationships[NodeRelationship.CHILD] = main_node.relationships.get(NodeRelationship.CHILD, []) + [\n                RelatedNodeInfo(node_id=table_summary_node.id_)\n            ]\n\n    # Create nodes for text summaries\n    if title in normalized_summarized_data:\n        for summary in normalized_summarized_data[title].get('text_summaries', []):\n            text_summary_node = Node(\n                text=f\"Text Summary: {summary}\",\n                metadata={\n                    \"doc_id\": doc.id_,\n                    \"type\": \"text_summary\"\n                }\n            )\n            all_nodes.append(text_summary_node)\n            main_node.relationships[NodeRelationship.CHILD] = main_node.relationships.get(NodeRelationship.CHILD, []) + [\n                RelatedNodeInfo(node_id=text_summary_node.id_)\n            ]\n\n    # Create nodes for image summaries\n    if title in normalized_structured_image_data:\n        for summary in normalized_structured_image_data[title]:\n            image_summary_node = Node(\n                text=f\"Image Summary: {summary}\",\n                metadata={\n                    \"doc_id\": doc.id_,\n                    \"type\": \"image_summary\"\n                }\n            )\n            all_nodes.append(image_summary_node)\n            main_node.relationships[NodeRelationship.CHILD] = main_node.relationships.get(NodeRelationship.CHILD, []) + [\n                RelatedNodeInfo(node_id=image_summary_node.id_)\n            ]\n\n    return all_nodes","metadata":{"execution":{"iopub.status.busy":"2024-07-12T02:00:05.152931Z","iopub.execute_input":"2024-07-12T02:00:05.153324Z","iopub.status.idle":"2024-07-12T02:00:05.170141Z","shell.execute_reply.started":"2024-07-12T02:00:05.153292Z","shell.execute_reply":"2024-07-12T02:00:05.169201Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def create_and_persist_index_for_document(doc, vector_store, retry_delay=60, max_retries=5):\n    for attempt in range(max_retries):\n        try:\n            nodes = process_document(doc, normalized_table_and_text_data, normalized_summarized_data, normalized_structured_image_data)\n            storage_context = StorageContext.from_defaults(vector_store=vector_store)\n            index = VectorStoreIndex(nodes, storage_context=storage_context)\n            return index\n        except Exception as e:\n            if \"rate limit\" in str(e).lower():\n                if attempt < max_retries - 1:\n                    logging.warning(f\"Rate limit error. Retrying in {retry_delay} seconds... (Attempt {attempt + 1}/{max_retries})\")\n                    time.sleep(retry_delay)\n                else:\n                    logging.error(f\"Max retries exceeded for document: {doc.metadata.get('file_name')}\")\n                    raise\n            else:\n                logging.error(f\"Unexpected error: {e}\")\n                raise","metadata":{"execution":{"iopub.status.busy":"2024-07-12T02:00:06.283230Z","iopub.execute_input":"2024-07-12T02:00:06.284072Z","iopub.status.idle":"2024-07-12T02:00:06.292278Z","shell.execute_reply.started":"2024-07-12T02:00:06.284037Z","shell.execute_reply":"2024-07-12T02:00:06.291058Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def add_document_to_index(doc, index, retry_delay=60, max_retries=5):\n    for attempt in range(max_retries):\n        try:\n            nodes = process_document(doc, normalized_table_and_text_data, normalized_summarized_data, normalized_structured_image_data)\n            index.insert_nodes(nodes)\n            return True\n        except Exception as e:\n            if \"rate limit\" in str(e).lower():\n                if attempt < max_retries - 1:\n                    logging.warning(f\"Rate limit error. Retrying in {retry_delay} seconds... (Attempt {attempt + 1}/{max_retries})\")\n                    time.sleep(retry_delay)\n                else:\n                    logging.error(f\"Max retries exceeded for document: {doc.metadata.get('file_name')}\")\n                    return False\n            else:\n                logging.error(f\"Unexpected error: {e}\")\n                return False\n\ndef create_full_index(source_docs):\n    logging.info(\"Creating Chroma client and collection\")\n    chroma_client = chromadb.EphemeralClient()\n    chroma_collection = chroma_client.get_or_create_collection(\"advanced_rag_index\")\n    \n    logging.info(\"Initializing vector store and storage context\")\n    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n\n    logging.info(\"Creating empty index\")\n    index = VectorStoreIndex([], storage_context=storage_context.property_graph_store,embed_model=embedding_model)\n    \n    for doc in tqdm(source_docs, desc=\"Processing documents\"):\n        success = add_document_to_index(doc, index)\n        if success:\n            logging.info(f\"Successfully processed document: {doc.metadata.get('file_name')}\")\n        else:\n            logging.error(f\"Failed to process document: {doc.metadata.get('file_name')}\")\n        time.sleep(5)  # Add a delay between documents to avoid rate limiting\n\n    logging.info(\"Persisting final index\")\n    index.storage_context.persist()\n    logging.info(\"Full index creation and persistence complete\")\n\n    return index\n","metadata":{"execution":{"iopub.status.busy":"2024-07-12T02:05:49.162797Z","iopub.execute_input":"2024-07-12T02:05:49.163165Z","iopub.status.idle":"2024-07-12T02:05:49.174266Z","shell.execute_reply.started":"2024-07-12T02:05:49.163137Z","shell.execute_reply":"2024-07-12T02:05:49.173339Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"index = create_full_index(source_docs)\nif index:\n    print(\"Full index created successfully\")\nelse:\n    print(\"Failed to create full index\")","metadata":{"execution":{"iopub.status.busy":"2024-07-12T02:05:51.280916Z","iopub.execute_input":"2024-07-12T02:05:51.281292Z","iopub.status.idle":"2024-07-12T03:02:00.893088Z","shell.execute_reply.started":"2024-07-12T02:05:51.281261Z","shell.execute_reply":"2024-07-12T03:02:00.892125Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"Processing documents:   0%|          | 0/179 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   1%|          | 1/179 [00:17<52:54, 17.83s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   1%|          | 2/179 [00:36<53:32, 18.15s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   2%|▏         | 3/179 [01:20<1:27:38, 29.88s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   2%|▏         | 4/179 [01:38<1:14:09, 25.43s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   3%|▎         | 5/179 [01:57<1:06:33, 22.95s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   3%|▎         | 6/179 [02:15<1:01:33, 21.35s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   4%|▍         | 7/179 [02:33<58:25, 20.38s/it]  ","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   4%|▍         | 8/179 [02:51<56:01, 19.66s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   5%|▌         | 9/179 [03:10<54:30, 19.24s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   6%|▌         | 10/179 [03:28<53:19, 18.93s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   6%|▌         | 11/179 [03:46<52:25, 18.72s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   7%|▋         | 12/179 [04:05<51:56, 18.66s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   7%|▋         | 13/179 [04:23<51:20, 18.56s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   8%|▊         | 14/179 [04:41<50:46, 18.47s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   8%|▊         | 15/179 [04:59<50:12, 18.37s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   9%|▉         | 16/179 [05:18<49:40, 18.29s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:   9%|▉         | 17/179 [05:36<49:29, 18.33s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  10%|█         | 18/179 [05:54<49:18, 18.38s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  11%|█         | 19/179 [06:13<48:47, 18.30s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  11%|█         | 20/179 [06:31<48:44, 18.40s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  12%|█▏        | 21/179 [06:50<48:25, 18.39s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  12%|█▏        | 22/179 [07:08<48:13, 18.43s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  13%|█▎        | 23/179 [07:27<47:52, 18.41s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  13%|█▎        | 24/179 [07:45<47:32, 18.40s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  14%|█▍        | 25/179 [08:03<47:19, 18.44s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  15%|█▍        | 26/179 [08:22<47:01, 18.44s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  15%|█▌        | 27/179 [08:40<46:38, 18.41s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: deepfake\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  16%|█▌        | 28/179 [08:58<46:13, 18.36s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: fmri_vision_reconstruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  16%|█▌        | 29/179 [09:08<39:02, 15.62s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: fmri_vision_reconstruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  17%|█▋        | 30/179 [09:17<33:51, 13.64s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: fmri_vision_reconstruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  17%|█▋        | 31/179 [09:26<30:21, 12.31s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: fmri_vision_reconstruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  18%|█▊        | 32/179 [09:35<27:51, 11.37s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: fmri_vision_reconstruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  18%|█▊        | 33/179 [09:44<26:05, 10.72s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: fmri_vision_reconstruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  19%|█▉        | 34/179 [09:53<24:46, 10.25s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: fmri_vision_reconstruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  20%|█▉        | 35/179 [10:02<23:45,  9.90s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: fmri_vision_reconstruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  20%|██        | 36/179 [10:12<23:03,  9.68s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: fmri_vision_reconstruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  21%|██        | 37/179 [10:21<22:25,  9.48s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: fmri_vision_reconstruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  21%|██        | 38/179 [10:30<22:04,  9.40s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: fmri_vision_reconstruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  22%|██▏       | 39/179 [10:39<21:50,  9.36s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: fmri_vision_reconstruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  22%|██▏       | 40/179 [10:48<21:32,  9.30s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  23%|██▎       | 41/179 [11:03<25:06, 10.92s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  23%|██▎       | 42/179 [11:18<27:41, 12.13s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  24%|██▍       | 43/179 [11:33<29:42, 13.11s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  25%|██▍       | 44/179 [11:49<30:55, 13.74s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  25%|██▌       | 45/179 [12:04<31:45, 14.22s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  26%|██▌       | 46/179 [12:19<32:07, 14.49s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  26%|██▋       | 47/179 [12:34<32:18, 14.69s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  27%|██▋       | 48/179 [12:49<32:22, 14.83s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  27%|██▋       | 49/179 [13:04<32:15, 14.89s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  28%|██▊       | 50/179 [13:19<32:05, 14.93s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  28%|██▊       | 51/179 [13:35<32:04, 15.04s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  29%|██▉       | 52/179 [13:50<31:47, 15.02s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  30%|██▉       | 53/179 [14:05<31:35, 15.04s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  30%|███       | 54/179 [14:20<31:25, 15.09s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  31%|███       | 55/179 [14:35<31:13, 15.11s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  31%|███▏      | 56/179 [14:50<31:05, 15.16s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  32%|███▏      | 57/179 [15:05<30:45, 15.12s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  32%|███▏      | 58/179 [15:20<30:26, 15.10s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  33%|███▎      | 59/179 [15:36<30:21, 15.18s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  34%|███▎      | 60/179 [15:51<30:07, 15.19s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  34%|███▍      | 61/179 [16:06<29:47, 15.15s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  35%|███▍      | 62/179 [16:21<29:27, 15.10s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  35%|███▌      | 63/179 [16:36<29:10, 15.09s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  36%|███▌      | 64/179 [16:51<28:54, 15.08s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  36%|███▋      | 65/179 [17:06<28:34, 15.04s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  37%|███▋      | 66/179 [17:21<28:25, 15.09s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: llama\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  37%|███▋      | 67/179 [17:37<28:13, 15.12s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  38%|███▊      | 68/179 [18:00<32:19, 17.47s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  39%|███▊      | 69/179 [18:22<35:02, 19.11s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  39%|███▉      | 70/179 [18:46<36:58, 20.36s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  40%|███▉      | 71/179 [19:09<37:59, 21.11s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  40%|████      | 72/179 [19:31<38:34, 21.63s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  41%|████      | 73/179 [19:58<41:00, 23.22s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  41%|████▏     | 74/179 [20:23<41:14, 23.56s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  42%|████▏     | 75/179 [20:46<40:38, 23.45s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  42%|████▏     | 76/179 [21:09<40:13, 23.43s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  43%|████▎     | 77/179 [21:33<39:46, 23.40s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  44%|████▎     | 78/179 [21:56<39:21, 23.38s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  44%|████▍     | 79/179 [22:19<38:51, 23.32s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  45%|████▍     | 80/179 [22:42<38:19, 23.22s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  45%|████▌     | 81/179 [23:05<37:59, 23.26s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  46%|████▌     | 82/179 [23:28<37:25, 23.15s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  46%|████▋     | 83/179 [23:51<36:56, 23.08s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  47%|████▋     | 84/179 [24:14<36:34, 23.10s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  47%|████▋     | 85/179 [24:37<36:07, 23.06s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  48%|████▊     | 86/179 [25:01<35:50, 23.13s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  49%|████▊     | 87/179 [25:24<35:24, 23.09s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  49%|████▉     | 88/179 [25:47<34:55, 23.03s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  50%|████▉     | 89/179 [26:10<34:38, 23.10s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  50%|█████     | 90/179 [26:33<34:21, 23.17s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  51%|█████     | 91/179 [26:56<33:58, 23.17s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  51%|█████▏    | 92/179 [27:20<33:42, 23.24s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  52%|█████▏    | 93/179 [27:43<33:18, 23.23s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  53%|█████▎    | 94/179 [28:06<32:49, 23.17s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  53%|█████▎    | 95/179 [28:29<32:27, 23.19s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  54%|█████▎    | 96/179 [28:53<32:09, 23.24s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  54%|█████▍    | 97/179 [29:16<31:43, 23.22s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  55%|█████▍    | 98/179 [29:39<31:17, 23.18s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  55%|█████▌    | 99/179 [30:02<30:57, 23.22s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  56%|█████▌    | 100/179 [30:29<32:08, 24.41s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  56%|█████▋    | 101/179 [30:52<31:14, 24.03s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  57%|█████▋    | 102/179 [31:15<30:24, 23.70s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  58%|█████▊    | 103/179 [31:39<30:04, 23.75s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  58%|█████▊    | 104/179 [32:02<29:25, 23.55s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  59%|█████▊    | 105/179 [32:26<28:56, 23.46s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  59%|█████▉    | 106/179 [32:51<29:15, 24.05s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  60%|█████▉    | 107/179 [33:14<28:30, 23.76s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  60%|██████    | 108/179 [33:37<27:55, 23.60s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  61%|██████    | 109/179 [34:01<27:42, 23.74s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  61%|██████▏   | 110/179 [34:25<27:09, 23.61s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  62%|██████▏   | 111/179 [34:48<26:38, 23.51s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  63%|██████▎   | 112/179 [35:11<26:09, 23.43s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  63%|██████▎   | 113/179 [35:35<25:44, 23.40s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  64%|██████▎   | 114/179 [35:58<25:15, 23.31s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  64%|██████▍   | 115/179 [36:21<24:48, 23.27s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  65%|██████▍   | 116/179 [36:44<24:25, 23.26s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  65%|██████▌   | 117/179 [37:07<24:01, 23.24s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  66%|██████▌   | 118/179 [37:31<23:50, 23.45s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  66%|██████▋   | 119/179 [37:54<23:16, 23.28s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  67%|██████▋   | 120/179 [38:17<22:46, 23.16s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  68%|██████▊   | 121/179 [38:40<22:13, 23.00s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  68%|██████▊   | 122/179 [39:03<22:04, 23.23s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  69%|██████▊   | 123/179 [39:26<21:35, 23.14s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  69%|██████▉   | 124/179 [39:49<21:13, 23.15s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  70%|██████▉   | 125/179 [40:13<20:51, 23.18s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: qwen\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  70%|███████   | 126/179 [40:36<20:27, 23.16s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  71%|███████   | 127/179 [40:48<17:15, 19.91s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  72%|███████▏  | 128/179 [41:00<14:59, 17.63s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  72%|███████▏  | 129/179 [41:13<13:18, 15.97s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  73%|███████▎  | 130/179 [41:25<12:06, 14.82s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  73%|███████▎  | 131/179 [41:37<11:14, 14.06s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  74%|███████▎  | 132/179 [41:49<10:37, 13.56s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  74%|███████▍  | 133/179 [42:02<10:05, 13.16s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  75%|███████▍  | 134/179 [42:14<09:39, 12.89s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  75%|███████▌  | 135/179 [42:26<09:20, 12.74s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  76%|███████▌  | 136/179 [42:39<09:01, 12.60s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  77%|███████▋  | 137/179 [42:51<08:45, 12.50s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  77%|███████▋  | 138/179 [43:03<08:30, 12.46s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  78%|███████▊  | 139/179 [43:16<08:23, 12.60s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  78%|███████▊  | 140/179 [43:28<08:07, 12.49s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  79%|███████▉  | 141/179 [43:40<07:50, 12.38s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  79%|███████▉  | 142/179 [43:52<07:34, 12.27s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  80%|███████▉  | 143/179 [44:05<07:22, 12.29s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  80%|████████  | 144/179 [44:17<07:07, 12.22s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  81%|████████  | 145/179 [44:29<06:54, 12.19s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  82%|████████▏ | 146/179 [44:41<06:40, 12.14s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  82%|████████▏ | 147/179 [44:53<06:28, 12.13s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  83%|████████▎ | 148/179 [45:05<06:15, 12.11s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  83%|████████▎ | 149/179 [45:17<06:04, 12.15s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  84%|████████▍ | 150/179 [45:29<05:51, 12.11s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: vision_instruction\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  84%|████████▍ | 151/179 [45:42<05:40, 12.17s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  85%|████████▍ | 152/179 [45:50<04:54, 10.90s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  85%|████████▌ | 153/179 [45:58<04:21, 10.05s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  86%|████████▌ | 154/179 [46:06<03:57,  9.50s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  87%|████████▋ | 155/179 [46:14<03:38,  9.10s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  87%|████████▋ | 156/179 [46:22<03:23,  8.83s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  88%|████████▊ | 157/179 [46:31<03:10,  8.65s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  88%|████████▊ | 158/179 [46:39<02:58,  8.50s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  89%|████████▉ | 159/179 [46:51<03:09,  9.49s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  89%|████████▉ | 160/179 [46:58<02:51,  9.03s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  90%|████████▉ | 161/179 [47:06<02:36,  8.72s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  91%|█████████ | 162/179 [47:14<02:24,  8.50s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  91%|█████████ | 163/179 [47:23<02:14,  8.40s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  92%|█████████▏| 164/179 [47:31<02:05,  8.34s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  92%|█████████▏| 165/179 [47:39<01:56,  8.30s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  93%|█████████▎| 166/179 [47:52<02:05,  9.66s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  93%|█████████▎| 167/179 [48:00<01:50,  9.17s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  94%|█████████▍| 168/179 [48:08<01:37,  8.82s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  94%|█████████▍| 169/179 [48:16<01:25,  8.58s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  95%|█████████▍| 170/179 [48:24<01:16,  8.46s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  96%|█████████▌| 171/179 [48:32<01:06,  8.37s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  96%|█████████▌| 172/179 [48:40<00:57,  8.25s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  97%|█████████▋| 173/179 [48:49<00:49,  8.27s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  97%|█████████▋| 174/179 [48:59<00:45,  9.03s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  98%|█████████▊| 175/179 [49:08<00:35,  8.79s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  98%|█████████▊| 176/179 [49:16<00:25,  8.63s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  99%|█████████▉| 177/179 [49:24<00:16,  8.49s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents:  99%|█████████▉| 178/179 [49:32<00:08,  8.41s/it]","output_type":"stream"},{"name":"stdout","text":"PROCESSING FILE: wisper\n","output_type":"stream"},{"name":"stderr","text":"Processing documents: 100%|██████████| 179/179 [49:40<00:00, 16.65s/it]\n","output_type":"stream"},{"name":"stdout","text":"Full index created successfully\n","output_type":"stream"}]},{"cell_type":"code","source":"print(index)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T03:02:00.894821Z","iopub.execute_input":"2024-07-12T03:02:00.895129Z","iopub.status.idle":"2024-07-12T03:02:00.899743Z","shell.execute_reply.started":"2024-07-12T03:02:00.895103Z","shell.execute_reply":"2024-07-12T03:02:00.898899Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"<llama_index.core.indices.vector_store.base.VectorStoreIndex object at 0x7cf7d3e59e70>\n","output_type":"stream"}]},{"cell_type":"code","source":"qe = index.as_query_engine(similarity_top_k=3)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T03:57:14.663196Z","iopub.execute_input":"2024-07-12T03:57:14.663902Z","iopub.status.idle":"2024-07-12T03:57:14.669001Z","shell.execute_reply.started":"2024-07-12T03:57:14.663871Z","shell.execute_reply":"2024-07-12T03:57:14.668016Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"y=qe.query(\"what is multimodal condition in talking face generation?\")","metadata":{"execution":{"iopub.status.busy":"2024-07-12T04:02:10.153025Z","iopub.execute_input":"2024-07-12T04:02:10.153798Z","iopub.status.idle":"2024-07-12T04:02:16.366001Z","shell.execute_reply.started":"2024-07-12T04:02:10.153766Z","shell.execute_reply":"2024-07-12T04:02:16.365029Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"execution":{"iopub.status.busy":"2024-07-12T04:02:29.083870Z","iopub.execute_input":"2024-07-12T04:02:29.084466Z","iopub.status.idle":"2024-07-12T04:02:29.091750Z","shell.execute_reply.started":"2024-07-12T04:02:29.084434Z","shell.execute_reply":"2024-07-12T04:02:29.090737Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"Response(response='Multimodal condition in talking face generation involves introducing additional modal information, such as text, image, and audio-emotional modalities, to guide facial pose and expression in generated videos. This approach aims to complement emotional content in textual information and enhance the vividness of the generated videos.', source_nodes=[NodeWithScore(node=TextNode(id_='e191431b-e323-4795-b7cd-c66750890fff', embedding=None, metadata={'file_name': 'Deepfake.pdf', 'doc_id': '10121698-98b4-4107-8cb3-7b0b7e8285c7', 'is_document': True}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.CHILD: '5'>: [RelatedNodeInfo(node_id='b8b8a401-7a3b-4c13-8083-c573b1b3ee65', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='c1927463-7e56-44bb-87f1-fa43298afd26', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='2ad9aefb-a183-45a3-bbf4-1f4fe8e4ecf8', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='bcc173b1-8102-4a1d-87e9-9ea403fd585b', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='bf670220-3983-4054-911d-559f99f5d640', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='ea26ab5d-ce6f-4a07-82ee-4fcefecbcb8c', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='14ef9c0b-1448-4e80-83d3-01970a857a5e', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='9fb22a24-203c-4d50-84e4-7b0c497a90ad', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='6e62adf0-6cf8-4fc7-b89d-ab7bf5d3b683', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='e995131e-5cdf-44cf-a2d1-39d579a8bf9e', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='d5b10257-a494-4ede-b462-2d276b59a74b', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='fd523a64-8c6d-4927-9985-63d8123fdb9f', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='c277c751-5f44-4c25-968a-c106d700ca18', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='aba1f1ec-649f-44ed-bbd8-c52fd4082764', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='39bacbb7-415e-43f5-aa39-2c3dd8fc8c83', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='85ebf18d-297a-4fe7-bbab-e953a406165b', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='15ee6e4d-227f-4e1d-abef-d405c64a06b4', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='9099cbef-4446-4ebf-9ccd-687f90188b84', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='1257b0d9-3364-4274-b69f-63f62c02080e', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='112caa9e-8b69-4ea9-a2a2-884c5f416629', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='4f7a3f0e-d3e0-4c99-b92c-250a61c14af8', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='2433c0f0-a70b-4a85-ae46-719e2c3bdcab', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='db6090bd-7910-42a4-89be-bb7fd86e7988', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='f7a0d7f0-3ff1-420b-a6c4-e96a9fa148c0', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='94886b23-2295-4489-b64a-1d061d47d72e', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='5f1b5419-effa-4032-b3cf-76ff412a1c9f', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='e333caf9-f995-4c24-935a-908feb138634', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='b831520f-c8fc-4eb2-94f1-cd0a3d725e3f', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='3ff0a048-9b3a-4cf2-831b-73478261ebf6', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='928b923e-eb27-41cf-af0a-106dfccfa742', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='72cd35d4-042f-4cd2-a078-59bd572ad6a0', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='8abcbdbe-452f-407c-8782-30afb469e0c7', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='a712fb63-034f-448a-a6e1-a3da19d01430', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='8e572d6c-7ba9-4fc5-8a6b-bc31fd7a0c22', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='825d0710-7162-4195-965c-d4061ec6773a', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='728a27e0-ddcd-4295-86b6-1055402d1dde', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='fcbbdc19-dc19-4630-9647-80001f5e40ee', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='7c3c4b2a-4e04-497a-8fc6-bd8943201505', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='6961f746-d65e-4e10-aff6-d50830031f58', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='bf74705f-fbef-4b6d-a06c-5b801aa1a878', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='05ef08af-99fa-4c97-a8a1-c98c1b5e41dd', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='d2c04758-fc21-473e-8057-3272608dcc9d', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='4a612660-fe8b-4083-acbe-e055713d5306', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='ada32c38-a7bd-40bd-864a-eaed99cc59c6', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='f5bc99af-3dd1-47bc-a59d-efe1a224ec74', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='cbbefac7-6a9e-49aa-b73a-45da8e6968c1', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='789bab2c-20a1-46e4-b259-4cd77bbc813d', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='0affa310-af6f-47df-ae67-7927d0a9b5f4', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='707911d3-c6b9-4655-a4c4-bd867f1452a5', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='94a72f5b-5b23-4821-b18c-d410a2e4e13c', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='d05220ae-d869-4509-bf52-e5795d056c16', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='39ecedf1-1735-4274-b89e-09c4a3efefd6', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='be785dbd-f6b0-4d9a-82c3-690e6b9bfc65', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='4044aab4-a9df-4ddb-9024-7133c8133da8', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='29f5913a-7a05-4d8d-9a99-01eb26a43e95', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='7d80d7ff-ff8e-4d79-a282-11bfc2ca0923', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='94f18b4d-670a-4c08-9326-fa0c9d3a0f91', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='c00bf39c-df11-49a4-b0dc-17a18865a8a0', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='9e7f3c4d-65e1-4f51-84cb-7db348e01c19', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='42f930cc-46b4-458e-bfb2-24e586f5cfd4', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='8357a684-9aaa-40ec-b5d3-18a66581f971', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='1f2e47e5-36c7-4d1b-a382-a521aa122d23', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='da605ba1-a04c-4095-8883-e4b8b03fed24', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='5aa2f93d-0a8c-4c32-bb6e-b5c89254388b', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='1a3f60eb-9add-4b4b-b341-43bc54844dd6', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='bbc04526-83ed-437f-b382-feced38b18f2', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='08cb407e-f4cd-4758-90d9-ece88618fb2e', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='a98c9702-8f93-49a9-b98a-11facc764391', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='faf61417-cfe6-4fdb-ba80-a46a605723a8', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='40e41644-f6b8-466e-90ca-cc96402f750f', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='1e42a8d7-0c23-4e0e-9110-b3643e1d1248', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='b0314e57-f148-4079-a0ad-2095d844760c', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='af665a31-4a8f-4658-9047-bdf995ea5a82', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='459ceeda-6ae3-4fd8-80f2-17b309916b98', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='fd5b31e7-61e1-47d1-b1b0-c9b666adc16e', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='e6fc624b-10c5-49da-aeca-92c6396c04a6', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='76f61557-a731-424f-a912-ebf1cbe0623f', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='46422b87-38d6-4a1a-b136-86336b87fd07', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='5512c293-2cfc-4767-855a-fadb879b3108', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='48beaa88-3246-44e4-9131-36175f86b59c', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='880a66eb-4a99-42d9-ae5b-c80c58c79075', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='4f0f9e99-21ef-4bf9-9cd5-88308cb15b12', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='214bfaaa-dc1a-4bdb-a363-937ceaac08b9', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='6e967c1f-c445-484f-b922-0a1d3f3bfce1', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='c1ee190b-4bf5-47bf-b425-e179e534c867', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='c5db5fb5-99b4-4327-b292-2ff2f2498059', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='6b6686f5-b860-4a63-aec2-05a924107bd9', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='17e2a07d-b9d8-4b19-9b08-ace60c57b008', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='108be156-d991-453f-a4a0-2dcc079a9a10', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='fd92ce14-365d-4439-a906-2495ca5cce2a', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='0e963688-80c8-4b34-b537-5579881b040b', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='792e1307-93f1-448c-8eaf-d6304f7c87f8', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='848f4cc7-acb4-4503-a975-8bdbcec91702', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='a72af28b-eda0-4a38-b975-253fe6657992', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='2a9c917b-5951-4298-830c-d1a09c79ee91', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='62d59b21-737b-4206-870c-5043a4783435', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='5de77258-1044-4b73-9a43-618ea1ec2039', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='98e4b437-d69a-4c76-bb3b-5811d1b49573', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='75e0dd2f-b2a9-4431-afed-915003ae2db9', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='7d5aa803-47b3-4ce8-97f0-9f495c168998', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='c5adf6a2-c17a-448b-a8a7-32e248755e0f', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='45d3c27f-3cdb-4966-b6fb-1098f7eb0c0f', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='ab4480f7-c87a-4bed-8c3c-7e0f1a915bef', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='45f49f81-6a29-4f7a-8062-aa502c7d9657', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='9d68ac0d-e49e-4a6b-9efa-0070a72a75a0', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='51c51a36-6b41-4ff3-9262-d44e328404e8', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='54c5f667-7344-4367-8ba8-1450ab592984', node_type=None, metadata={}, hash=None), RelatedNodeInfo(node_id='4afd3648-1eb6-4dc1-b0f4-9773b6003ae3', node_type=None, metadata={}, hash=None)]}, text='10 Gan Pei, Jiangning Zhang, et al.\\nmator addresses the regression of deformations related\\nto 3D poses and expressions. The eye gaze estimator\\ncontrols eye movement in videos, providing finer details.\\nMetaPortrait [324] achieves accurate distortion field\\nprediction through dense facial keypoint matching and\\naccelerates model training based on meta-learning prin-\\nciples, delivering excellent results on limited datasets.\\n•Feature Decoupling. The latent feature decoupling\\nand driving methods [16 –18,155,300] aims to disentangle\\nfacial features in the latent space of the driving video, re-\\nplacing or mapping the corresponding latent information\\nto achieve high-fidelity facial reproduction under specific\\nconditions. HyperReenact [17] uses attribute decoupling,\\nemploying a hyper-network to refine source identity fea-\\ntures and modify facial poses. StyleMask [18] separates\\nfacial pose and expression from the identity information\\nof the source image by learning masks and blending\\ncorresponding channels in the pre-trained style space S\\nof StyleGAN2. HiDe-NeRF [155] employs a deformable\\nneural radiance field to represent a 3D scene, with a\\nlightweight deformation module explicitly decoupling\\nfacial pose and expression attributes.\\n•Self-supervised Learning. Self-supervised learning\\nemploys supervisory signals inferred from the intrinsic\\nstructure of the data, reducing the reliance on exter-\\nnal data labels [207,253,321,328]. Oorloff et al. [207]\\nemploys self-supervised methods to train an encoder,\\ndisentangling identity and facial attribute information\\nof portrait images within the pre-defined latent space\\nitself of a pre-trained StyleGAN2. Zhang et al. [328]\\nutilizes 3DMM to provide geometric guidance, employs\\npre-computed optical flow to guide motion field esti-\\nmation, and relies on pre-computed occlusion maps to\\nguide the perception and repair of occluded areas.\\n3.1.3 Talking Face Generation\\nIn this section, we review current methods from three\\nperspectives:audio/textdriven,multimodalconditioned,\\ndiffusion-based, and 3D-model Technologies. We also\\nsummarize them in Table 4.\\n•Audio/Text Driven. Methods aim to map and guide\\nlip and facial movements in generated videos by under-\\nstanding the semantic information from the driving\\nsource [230,259,332]. Early methods [32,58] perform\\npoorly in terms of generalization and training complex-\\nity. After training, the models struggled to generalize to\\nnew individuals, requiring extensive conversational data\\nfor training new characters. Researchers [33,217] pro-\\npose their solutions from various perspectives. However,\\nMost of these methods prioritize generating lip move-\\nments aligned with semantic information, overlooking\\nessential aspects like identity and style, such as headpose changes and movement control, which are crucial\\nin natural videos. To address this, MakeItTalk [357]\\ndecouples input audio information by predicting facial\\nlandmarks based on audio and obtaining semantic de-\\ntails on facial expressions and poses from audio sig-\\nnals. SadTalker [337] extracts 3D motion coefficients\\nfor constructing a 3DMM from audio and uses this to\\nmodulate a new 3D perceptual facial rendering for gen-\\nerating head poses in talking videos. Additionally, some\\nmethods [63,145,262,279] propose their improvement\\nmethods, and these will not be detailed one by one. In\\naddition, the emotional expression varies for different\\ntextsduringaconversation,andvividemotionsareanes-\\nsential part of real talking face videos [64,233]. Recently,\\nsome methods [82,250,322] extend their previous ap-\\nproaches by incorporating matching between the driving\\ninformation and corresponding emotions. EMMN [250]\\nestablishes an organic relationship between emotions\\nand lip movements by extracting emotion embeddings\\nfrom the audio signal, synthesizing overall facial expres-\\nsionsintalkingfacesratherthanfocusingsolelyonaudio\\nfor facial expression synthesis. AMIGO [322] employs\\na sequence-to-sequence cross-modal emotion landmark\\ngeneration network to generate vivid landmarks aided\\nby audio information, ensuring that lips and emotions\\nin the output image sequence are synchronized with the\\ninput audio. However, existing methods still lack effec-\\ntive control over the intensity of emotions. In addition,\\nTalkCLIP [178] introduces style parameters, expanding\\nthe style categories for text-guided talking video genera-\\ntion. Zhong et al. [348] propose a two-stage framework,\\nincorporating appearance priors during the generation\\nprocess to enhance the model’s ability to preserve at-\\ntributes of the target face. DR2 [326] explores practical\\nstrategies for reducing the training workload.\\n•Multimodal Conditioned. To generate more real-\\nistic talking videos, some methods [159,261,284,351]\\nintroduce additional modal information on top of audio-\\ndriven methods to guide facial pose and expression.\\nGC-AVT [159] generates realistic talking videos by inde-\\npendently controlling head pose, audio information, and\\nfacial expressions. This approach introduces an expres-\\nsion source video, providing emotional information dur-\\ning the speech and the pose source video. However, the\\nvideo quality falls below expectations, and it struggles\\nto handle complex background changes. Xu et al. [284]\\nintegrate text, image, and audio-emotional modalities\\ninto a unified space to complement emotional content\\nin textual information. Multimodal approaches have sig-\\nnificantly enhanced the vividness of generated videos,\\nbut there is still room for exploration of organically\\ncombining information driven by different sources and\\nmodalities.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6025535344308235), NodeWithScore(node=TextNode(id_='5aa2f93d-0a8c-4c32-bb6e-b5c89254388b', embedding=None, metadata={'doc_id': '10121698-98b4-4107-8cb3-7b0b7e8285c7', 'type': 'text_summary'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Text Summary: In this section, the current methods for talking face generation are reviewed from three perspectives: audio/text driven, multimodal conditioned, and diffusion-based, as well as 3D-model Technologies. The methods aim to map and guide lip and facial movements in generated videos by understanding semantic information from the driving source. Some methods prioritize generating lip movements aligned with semantic information, while others incorporate matching between the driving information and corresponding emotions. Multimodal approaches have significantly enhanced the vividness of generated videos, but there is still room for exploration of organically combining information driven by different sources and modalities.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.5918122777481373), NodeWithScore(node=TextNode(id_='fa6c2ebd-9cfb-4812-b229-cecd380a4e36', embedding=None, metadata={'doc_id': '2f5335ad-5219-43a1-bb15-5281541842dc', 'type': 'text_summary'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Text Summary: In this section, the current methods for talking face generation are reviewed from three perspectives: audio/text driven, multimodal conditioned, and diffusion-based, as well as 3D-model Technologies. The methods aim to map and guide lip and facial movements in generated videos by understanding semantic information from the driving source. Some methods prioritize generating lip movements aligned with semantic information, while others incorporate matching between the driving information and corresponding emotions. Multimodal approaches have significantly enhanced the vividness of generated videos, but there is still room for exploration of organically combining information driven by different sources and modalities.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.591019749333408)], metadata={'e191431b-e323-4795-b7cd-c66750890fff': {'file_name': 'Deepfake.pdf', 'doc_id': '10121698-98b4-4107-8cb3-7b0b7e8285c7', 'is_document': True}, '5aa2f93d-0a8c-4c32-bb6e-b5c89254388b': {'doc_id': '10121698-98b4-4107-8cb3-7b0b7e8285c7', 'type': 'text_summary'}, 'fa6c2ebd-9cfb-4812-b229-cecd380a4e36': {'doc_id': '2f5335ad-5219-43a1-bb15-5281541842dc', 'type': 'text_summary'}})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Ragas","metadata":{}},{"cell_type":"code","source":"from ragas.testset.generator import TestsetGenerator\nimport random\n\n# Initialize a TestsetGenerator using its default settings.\n# TestsetGenerator is used for generating test datasets, typically for model evaluation or testing.\n# The 'from_default' method sets up the generator with default configurations.\ntestsetgenerator = TestsetGenerator.from_llama_index(\n    generator_llm=llm,\n    critic_llm=llm,\n    embeddings=embedding_model,\n)\n\n# Specify the sample size for the source documents.\n# This determines how many documents will be randomly selected from the source documents.\nsample_size = 6\n\n# Define the number of questions to be included in the test set.\n# This will set how many test cases or questions the test set will contain.\nnum_questions = 15\n\n# Generate a test dataset from a random sample of source documents.\n# 'random.sample' is used to randomly select a subset of documents from the source.\n# The test set is then generated based on these documents.\n# Parameters:\n#   random.sample(source_docs, sample_size): A randomly selected subset of source documents.\n#   test_size: The number of questions or test cases to generate in the test set.\ntestset = testsetgenerator.generate_with_llamaindex_docs(\n    random.sample(source_docs, sample_size),  # Randomly selected documents\n    test_size=num_questions,               # Number of questions in the test set \n)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:02:21.022914Z","iopub.execute_input":"2024-07-11T18:02:21.023264Z","iopub.status.idle":"2024-07-11T18:09:30.014929Z","shell.execute_reply.started":"2024-07-11T18:02:21.023236Z","shell.execute_reply":"2024-07-11T18:09:30.013959Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"embedding nodes:   0%|          | 0/14 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2574dab99ba4e7d8cc06fadc435b8ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60b51db203af45d6a1871553dfd1806b"}},"metadata":{}}]},{"cell_type":"code","source":"import re\n\ntest_df = testset.to_pandas()\n# Define the regex pattern to match any character that is NOT a letter, a number, '.', ',', or '?'\npattern = r\"[^a-zA-Z0-9.,? ]\"\n\n# Define a function to replace special characters in a string\ndef remove_special_chars(s):\n    return re.sub(pattern, '', str(s))\n\n# Apply the function to each cell in the DataFrame\ntest_df = test_df.applymap(remove_special_chars)\n\n\ntest_questions = test_df['question'].values.tolist()\ntest_answers = [[item] for item in test_df['ground_truth'].values.tolist()]\n\ntest_df","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:09:58.039281Z","iopub.execute_input":"2024-07-11T18:09:58.039690Z","iopub.status.idle":"2024-07-11T18:09:58.157553Z","shell.execute_reply.started":"2024-07-11T18:09:58.039658Z","shell.execute_reply":"2024-07-11T18:09:58.156324Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_532/2846673957.py:12: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  test_df = test_df.applymap(remove_special_chars)\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                             question  \\\n0   How does the modified code correctly calculate...   \n1   How does the modified code correctly handle th...   \n2   What are the average scores of the proprietary...   \n3   How does the text normalization process impact...   \n4   What is the approach used to train the chat mo...   \n5   How can the punchline be revealed upon button ...   \n6   How does the use of multilingual and multitask...   \n7   What changes were made in the modified code to...   \n8   Whats the purpose of the queue in the code and...   \n9   What are the average scores of the proprietary...   \n10  Whats the purpose of the queue in the code and...   \n11  What is the return value and calculation proce...   \n\n                                             contexts  \\\n0   Qwen14BChat RLHF nnnnnndefmaxDepthself, root  ...   \n1   Qwen14BChat RLHF nnnnnndefmaxDepthself, root  ...   \n2   Table 13 Results on MMLU . All are tested with...   \n3   Robust Speech Recognition via LargeScale Weak ...   \n4   QWEN TECHNICAL REPORTnJinze Bai, Shuai Bai, Yu...   \n5   DOCTYPE htmlnhtmlnheadntitleMy Joke Websitetit...   \n6   Robust Speech Recognition via LargeScale Weak ...   \n7   Qwen14BChat RLHF nnnnnndefmaxDepthself, root  ...   \n8   def maxDepth  s e l f , r o o t  TreeNode   i ...   \n9   Table 13 Results on MMLU . All are tested with...   \n10  def maxDepth  s e l f , r o o t  TreeNode   i ...   \n11  def maxDepth  s e l f , r o o t  TreeNode   i ...   \n\n                                         ground_truth evolution_type  \\\n0   The modified code correctly calculates the max...         simple   \n1   In this code, when each node is extracted, we ...         simple   \n2   The answer to given question is not present in...         simple   \n3   The text normalization process impacts the per...         simple   \n4   The approach used to train the chat models in ...         simple   \n5   The answer to given question is not present in...         simple   \n6   Multilingual and multitask models benefit more...         simple   \n7   In the modified code, when each node is extrac...         simple   \n8   Your code is correct, the answer is correct. I...      reasoning   \n9   The answer to given question is not present in...      reasoning   \n10  Your code is correct, the answer is correct. I...      reasoning   \n11  Your code is correct, the answer is correct. I...      reasoning   \n\n                                             metadata episode_done  \n0   pagelabel 56, filename Qwen.pdf, filepath kagg...         True  \n1   pagelabel 56, filename Qwen.pdf, filepath kagg...         True  \n2   pagelabel 37, filename Qwen.pdf, filepath kagg...         True  \n3   pagelabel 12, filename Wisper.pdf, filepath ka...         True  \n4   pagelabel 1, filename Qwen.pdf, filepath kaggl...         True  \n5   pagelabel 16, filename VisionInstruction.pdf, ...         True  \n6   pagelabel 12, filename Wisper.pdf, filepath ka...         True  \n7   pagelabel 56, filename Qwen.pdf, filepath kagg...         True  \n8   pagelabel 55, filename Qwen.pdf, filepath kagg...         True  \n9   pagelabel 37, filename Qwen.pdf, filepath kagg...         True  \n10  pagelabel 55, filename Qwen.pdf, filepath kagg...         True  \n11  pagelabel 55, filename Qwen.pdf, filepath kagg...         True  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>contexts</th>\n      <th>ground_truth</th>\n      <th>evolution_type</th>\n      <th>metadata</th>\n      <th>episode_done</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>How does the modified code correctly calculate...</td>\n      <td>Qwen14BChat RLHF nnnnnndefmaxDepthself, root  ...</td>\n      <td>The modified code correctly calculates the max...</td>\n      <td>simple</td>\n      <td>pagelabel 56, filename Qwen.pdf, filepath kagg...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>How does the modified code correctly handle th...</td>\n      <td>Qwen14BChat RLHF nnnnnndefmaxDepthself, root  ...</td>\n      <td>In this code, when each node is extracted, we ...</td>\n      <td>simple</td>\n      <td>pagelabel 56, filename Qwen.pdf, filepath kagg...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>What are the average scores of the proprietary...</td>\n      <td>Table 13 Results on MMLU . All are tested with...</td>\n      <td>The answer to given question is not present in...</td>\n      <td>simple</td>\n      <td>pagelabel 37, filename Qwen.pdf, filepath kagg...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>How does the text normalization process impact...</td>\n      <td>Robust Speech Recognition via LargeScale Weak ...</td>\n      <td>The text normalization process impacts the per...</td>\n      <td>simple</td>\n      <td>pagelabel 12, filename Wisper.pdf, filepath ka...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>What is the approach used to train the chat mo...</td>\n      <td>QWEN TECHNICAL REPORTnJinze Bai, Shuai Bai, Yu...</td>\n      <td>The approach used to train the chat models in ...</td>\n      <td>simple</td>\n      <td>pagelabel 1, filename Qwen.pdf, filepath kaggl...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>How can the punchline be revealed upon button ...</td>\n      <td>DOCTYPE htmlnhtmlnheadntitleMy Joke Websitetit...</td>\n      <td>The answer to given question is not present in...</td>\n      <td>simple</td>\n      <td>pagelabel 16, filename VisionInstruction.pdf, ...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>How does the use of multilingual and multitask...</td>\n      <td>Robust Speech Recognition via LargeScale Weak ...</td>\n      <td>Multilingual and multitask models benefit more...</td>\n      <td>simple</td>\n      <td>pagelabel 12, filename Wisper.pdf, filepath ka...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>What changes were made in the modified code to...</td>\n      <td>Qwen14BChat RLHF nnnnnndefmaxDepthself, root  ...</td>\n      <td>In the modified code, when each node is extrac...</td>\n      <td>simple</td>\n      <td>pagelabel 56, filename Qwen.pdf, filepath kagg...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Whats the purpose of the queue in the code and...</td>\n      <td>def maxDepth  s e l f , r o o t  TreeNode   i ...</td>\n      <td>Your code is correct, the answer is correct. I...</td>\n      <td>reasoning</td>\n      <td>pagelabel 55, filename Qwen.pdf, filepath kagg...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>What are the average scores of the proprietary...</td>\n      <td>Table 13 Results on MMLU . All are tested with...</td>\n      <td>The answer to given question is not present in...</td>\n      <td>reasoning</td>\n      <td>pagelabel 37, filename Qwen.pdf, filepath kagg...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Whats the purpose of the queue in the code and...</td>\n      <td>def maxDepth  s e l f , r o o t  TreeNode   i ...</td>\n      <td>Your code is correct, the answer is correct. I...</td>\n      <td>reasoning</td>\n      <td>pagelabel 55, filename Qwen.pdf, filepath kagg...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>What is the return value and calculation proce...</td>\n      <td>def maxDepth  s e l f , r o o t  TreeNode   i ...</td>\n      <td>Your code is correct, the answer is correct. I...</td>\n      <td>reasoning</td>\n      <td>pagelabel 55, filename Qwen.pdf, filepath kagg...</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_df.to_csv(\"test_dataset.csv\", index=False, encoding='utf-8')","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:10:47.085213Z","iopub.execute_input":"2024-07-11T18:10:47.085582Z","iopub.status.idle":"2024-07-11T18:10:47.171839Z","shell.execute_reply.started":"2024-07-11T18:10:47.085548Z","shell.execute_reply":"2024-07-11T18:10:47.171053Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import asyncio\nfrom ragas.metrics import (\n    faithfulness,\n    answer_relevancy,\n    context_precision,\n    context_recall,\n    answer_similarity,\n    answer_correctness\n)\nfrom tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\nfrom ragas.integrations.llama_index import evaluate\nimport pandas as pd\nimport time\nimport httpx \n\n# List of evaluation metrics functions to be used.\nmetrics = [\n    faithfulness,           # Evaluates faithfulness of the response to the source material.\n    answer_relevancy,       # Assesses relevance of the response to the query.\n    context_precision,      # Measures precision of the context in the response.\n    context_recall,         # Measures recall of the context in the response.\n    answer_correctness,     # Checks correctness of the answer.\n    answer_similarity,      # Evaluates similarity of the answer to a reference answer.\n]\n\n# A list to collect individual result DataFrames.\nresults_list = []","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:31:31.143403Z","iopub.execute_input":"2024-07-11T18:31:31.144138Z","iopub.status.idle":"2024-07-11T18:31:31.150627Z","shell.execute_reply.started":"2024-07-11T18:31:31.144105Z","shell.execute_reply":"2024-07-11T18:31:31.149656Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"@retry(stop=stop_after_attempt(5), wait=wait_exponential(min=1, max=60), retry=retry_if_exception_type(httpx.HTTPStatusError))\ndef safe_evaluate(query_engine, metrics, dataset, llm, embeddings):\n    return evaluate(query_engine=query_engine, metrics=metrics, dataset=dataset, llm=llm, embeddings=embeddings,raise_exceptions=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:31:41.633507Z","iopub.execute_input":"2024-07-11T18:31:41.634367Z","iopub.status.idle":"2024-07-11T18:31:41.639757Z","shell.execute_reply.started":"2024-07-11T18:31:41.634334Z","shell.execute_reply":"2024-07-11T18:31:41.638760Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def evaluate_and_append(query_engine, technique):\n    # Evaluate the query engine.\n    result = safe_evaluate(query_engine=query_engine, metrics=metrics, dataset=test_df, llm=llm, embeddings=embedding_model)\n    # Add a 'technique' column to the result DataFrame.\n    result['technique'] = technique\n\n    # Add the result DataFrame to the results list.\n    results_list.append(result)\n\n    # Sleep to handle rate limits.\n    # time.sleep(60)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:31:44.624712Z","iopub.execute_input":"2024-07-11T18:31:44.625312Z","iopub.status.idle":"2024-07-11T18:31:44.630579Z","shell.execute_reply.started":"2024-07-11T18:31:44.625282Z","shell.execute_reply":"2024-07-11T18:31:44.629599Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"evaluate_and_append(baseline_query_engine, 'chunks_with_overlap')","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:31:45.364388Z","iopub.execute_input":"2024-07-11T18:31:45.365243Z","iopub.status.idle":"2024-07-11T18:34:02.974788Z","shell.execute_reply.started":"2024-07-11T18:31:45.365210Z","shell.execute_reply":"2024-07-11T18:34:02.974041Z"},"trusted":true},"execution_count":41,"outputs":[{"output_type":"display_data","data":{"text/plain":"Running Query Engine:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3d25c826e9f4c58bd611439c2ba07f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/72 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"169e842385254fda8072f178e52c8e8d"}},"metadata":{}}]},{"cell_type":"code","source":"# Convert each Result object's items to a dictionary and collect them in a list\ndict_list = [dict(result.items()) for result in results_list]\n\n# Convert the list of dictionaries to a DataFrame\nresults_df = pd.DataFrame(dict_list)\n\nresults_df","metadata":{"execution":{"iopub.status.busy":"2024-07-11T18:34:02.976032Z","iopub.execute_input":"2024-07-11T18:34:02.976305Z","iopub.status.idle":"2024-07-11T18:34:02.990396Z","shell.execute_reply.started":"2024-07-11T18:34:02.976279Z","shell.execute_reply":"2024-07-11T18:34:02.989488Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"   faithfulness  answer_relevancy  context_precision  context_recall  \\\n0      0.949074          0.821416               0.75        0.741667   \n\n   answer_correctness  answer_similarity            technique  \n0            0.559331           0.625103  chunks_with_overlap  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>faithfulness</th>\n      <th>answer_relevancy</th>\n      <th>context_precision</th>\n      <th>context_recall</th>\n      <th>answer_correctness</th>\n      <th>answer_similarity</th>\n      <th>technique</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.949074</td>\n      <td>0.821416</td>\n      <td>0.75</td>\n      <td>0.741667</td>\n      <td>0.559331</td>\n      <td>0.625103</td>\n      <td>chunks_with_overlap</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}